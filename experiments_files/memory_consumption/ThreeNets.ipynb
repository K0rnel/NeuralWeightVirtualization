{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ThreeNets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EXQhfEdZsGai",
        "outputId": "fcf6566b-b5b1-472d-b1da-600653def8b9"
      },
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow-gpu==1.13.1\n",
        "\n",
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.16.4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.0:\n",
            "  Successfully uninstalled tensorflow-2.4.0\n",
            "Collecting tensorflow-gpu==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |████████████████████████████████| 345.2MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.19.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.12.4)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.32.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 55.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.36.2)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (51.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.3.3)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.4.0)\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, keras-applications, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1\n",
            "Uninstalling numpy-1.19.4:\n",
            "  Successfully uninstalled numpy-1.19.4\n",
            "Collecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 213kB/s \n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "Successfully installed numpy-1.16.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNd3Kue8tF_E",
        "outputId": "30a14825-e5d0-4e2c-ef2a-25cc952fbda1"
      },
      "source": [
        "!git clone https://github.com/K0rnel/NeuralWeightVirtualization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'NeuralWeightVirtualization' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yYg0EcStH5f",
        "outputId": "a34ef8c9-ffa3-428d-a57c-4bae97e12bbb"
      },
      "source": [
        "%cd NeuralWeightVirtualization"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NeuralWeightVirtualization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe8QYrAhtJhG",
        "outputId": "73be1293-efd1-480c-8619-d1e47831f907"
      },
      "source": [
        "!sh download_dataset.sh"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/9] Downloading CIFAR10 dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   4163      0 --:--:-- --:--:-- --:--:--  4163\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  234M    0  234M    0     0   123M      0 --:--:--  0:00:01 --:--:--  158M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    429      0 --:--:-- --:--:-- --:--:--   429\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100  781k  100  781k    0     0   651k      0  0:00:01  0:00:01 --:--:--  651k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2503      0 --:--:-- --:--:-- --:--:--  2487\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1171M    0 1171M    0     0   108M      0 --:--:--  0:00:10 --:--:-- 34.1M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    522      0 --:--:-- --:--:-- --:--:--   521\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 3906k    0 3906k    0     0  3752k      0 --:--:--  0:00:01 --:--:-- 3752k\n",
            "\n",
            "[2/9] Downloading Google Speech Command V2 dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    714      0 --:--:-- --:--:-- --:--:--   713\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 66.5M    0 66.5M    0     0  65.9M      0 --:--:--  0:00:01 --:--:-- 65.9M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    416      0 --:--:-- --:--:-- --:--:--   416\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 3009k    0 3009k    0     0  2487k      0 --:--:--  0:00:01 --:--:-- 2487k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2813      0 --:--:-- --:--:-- --:--:--  2813\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  513M    0  513M    0     0   111M      0 --:--:--  0:00:04 --:--:--  130M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    352      0 --:--:--  0:00:01 --:--:--   352\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 22.6M    0 22.6M    0     0  15.3M      0 --:--:--  0:00:01 --:--:-- 15.3M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    207      0 --:--:--  0:00:01 --:--:--   207\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "100 60.3M    0 60.3M    0     0  24.8M      0 --:--:--  0:00:02 --:--:--  360M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    515      0 --:--:-- --:--:-- --:--:--   514\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 2729k    0 2729k    0     0  2699k      0 --:--:--  0:00:01 --:--:-- 2699k\n",
            "\n",
            "[3/9] Downloading GTSRB dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    123      0 --:--:--  0:00:03 --:--:--   123\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
            "100 98.6M    0 98.6M    0     0  25.5M      0 --:--:--  0:00:03 --:--:-- 25.5M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    445      0 --:--:-- --:--:-- --:--:--   445\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 4243k    0 4243k    0     0  3683k      0 --:--:--  0:00:01 --:--:-- 3683k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   4975      0 --:--:-- --:--:-- --:--:--  4975\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  306M    0  306M    0     0   124M      0 --:--:--  0:00:02 --:--:--  146M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    378      0 --:--:--  0:00:01 --:--:--   378\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 12.8M    0 12.8M    0     0  9948k      0 --:--:--  0:00:01 --:--:-- 9948k\n",
            "\n",
            "[4/9] Downloading SVHN dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   4250      0 --:--:-- --:--:-- --:--:--  4250\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  305M    0  305M    0     0   141M      0 --:--:--  0:00:02 --:--:--  204M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    349      0 --:--:--  0:00:01 --:--:--   349\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 1017k  100 1017k    0     0   746k      0  0:00:01  0:00:01 --:--:--  746k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2598      0 --:--:-- --:--:-- --:--:--  2598\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  772M    0  772M    0     0   135M      0 --:--:--  0:00:05 --:--:--  148M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    573      0 --:--:-- --:--:-- --:--:--   573\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 2575k    0 2575k    0     0  2778k      0 --:--:-- --:--:-- --:--:-- 2778k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    130      0 --:--:--  0:00:03 --:--:--   130\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
            "100 85.8M    0 85.8M    0     0  23.2M      0 --:--:--  0:00:03 --:--:--  300M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   1022      0 --:--:-- --:--:-- --:--:--  1020\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  286k  100  286k    0     0   421k      0 --:--:-- --:--:-- --:--:--  421k\n",
            "\n",
            "[5/9] Downloading US8K dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   3090      0 --:--:-- --:--:-- --:--:--  3090\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  202M    0  202M    0     0  97.2M      0 --:--:--  0:00:02 --:--:--  135M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    718      0 --:--:-- --:--:-- --:--:--   717\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  422k  100  422k    0     0   558k      0 --:--:-- --:--:-- --:--:--  558k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2794      0 --:--:-- --:--:-- --:--:--  2794\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1826M    0 1826M    0     0  46.9M      0 --:--:--  0:00:38 --:--:-- 34.6M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   1829      0 --:--:-- --:--:-- --:--:--  1829\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 3801k    0 3801k    0     0  8718k      0 --:--:-- --:--:-- --:--:-- 8718k\n",
            "\n",
            "[6/9] Downloading FMNIST dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    168      0 --:--:--  0:00:02 --:--:--   168\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "100 59.8M    0 59.8M    0     0  21.1M      0 --:--:--  0:00:02 --:--:-- 21.1M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    885      0 --:--:-- --:--:-- --:--:--   883\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  390k  100  390k    0     0   588k      0 --:--:-- --:--:-- --:--:--  588k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   4744      0 --:--:-- --:--:-- --:--:--  4744\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  358M    0  358M    0     0   114M      0 --:--:--  0:00:03 --:--:--  133M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    342      0 --:--:--  0:00:01 --:--:--   342\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 2343k    0 2343k    0     0  1594k      0 --:--:--  0:00:01 --:--:-- 1594k\n",
            "\n",
            "[7/9] Downloading HHAR dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    336      0 --:--:--  0:00:01 --:--:--   336\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 21.8M    0 21.8M    0     0  14.6M      0 --:--:--  0:00:01 --:--:-- 14.6M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   1133      0 --:--:-- --:--:-- --:--:--  1133\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 57392  100 57392    0     0   103k      0 --:--:-- --:--:-- --:--:--  103k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2443      0 --:--:-- --:--:-- --:--:--  2428\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 2180M    0 2180M    0     0  48.1M      0 --:--:--  0:00:45 --:--:-- 33.0M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    478      0 --:--:-- --:--:-- --:--:--   478\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 5582k    0 5582k    0     0  5097k      0 --:--:--  0:00:01 --:--:-- 5097k\n",
            "\n",
            "[8/9] Downloading ESC10 dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    379      0 --:--:--  0:00:01 --:--:--   379\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 26.3M    0 26.3M    0     0  19.0M      0 --:--:--  0:00:01 --:--:-- 19.0M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2414      0 --:--:-- --:--:-- --:--:--  2414\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 28208  100 28208    0     0  85478      0 --:--:-- --:--:-- --:--:-- 85478\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   5164      0 --:--:-- --:--:-- --:--:--  5164\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  237M    0  237M    0     0  92.6M      0 --:--:--  0:00:02 --:--:--  105M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   1210      0 --:--:-- --:--:-- --:--:--  1207\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  246k  100  246k    0     0   463k      0 --:--:-- --:--:-- --:--:--  463k\n",
            "\n",
            "[9/9] Downloading OBS dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    147      0 --:--:--  0:00:02 --:--:--   147\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "100 78.4M    0 78.4M    0     0  24.7M      0 --:--:--  0:00:03 --:--:-- 24.7M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   1378      0 --:--:-- --:--:-- --:--:--  1378\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 91488  100 91488    0     0   179k      0 --:--:-- --:--:-- --:--:--  179k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2756      0 --:--:-- --:--:-- --:--:--  2756\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  156M    0  156M    0     0   125M      0 --:--:--  0:00:01 --:--:--  290M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   1046      0 --:--:-- --:--:-- --:--:--  1046\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  178k  100  178k    0     0   314k      0 --:--:-- --:--:-- --:--:--  314k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    192      0 --:--:--  0:00:02 --:--:--   192\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "100 78.4M    0 78.4M    0     0  29.9M      0 --:--:--  0:00:02 --:--:--  295M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   1961      0 --:--:-- --:--:-- --:--:--  1961\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 91488  100 91488    0     0   215k      0 --:--:-- --:--:-- --:--:--  215k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuzFzCYNx3-6",
        "outputId": "ecb7b1fb-9377-4d85-fbf5-111746f00df8"
      },
      "source": [
        "!python weight_virtualization.py -mode=a -network_path=mnist | tee -a mnist_matching_three_nets.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init new weight pages\n",
            "add_vnn\n",
            "mnist/mnist_network_weight.npy\n",
            "compute_fisher\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "do_compute_fisher\n",
            "sample num:    0, data_idx:  3005\n",
            "sample num:    1, data_idx: 24653\n",
            "sample num:    2, data_idx: 38672\n",
            "sample num:    3, data_idx: 27796\n",
            "sample num:    4, data_idx: 43903\n",
            "sample num:    5, data_idx: 14224\n",
            "sample num:    6, data_idx: 51991\n",
            "sample num:    7, data_idx: 52568\n",
            "sample num:    8, data_idx: 30593\n",
            "sample num:    9, data_idx: 53720\n",
            "sample num:   10, data_idx:  8505\n",
            "sample num:   11, data_idx: 33276\n",
            "sample num:   12, data_idx: 26219\n",
            "sample num:   13, data_idx: 32202\n",
            "sample num:   14, data_idx: 32739\n",
            "sample num:   15, data_idx:  3188\n",
            "sample num:   16, data_idx: 11889\n",
            "sample num:   17, data_idx:  4019\n",
            "sample num:   18, data_idx: 48153\n",
            "sample num:   19, data_idx: 16038\n",
            "sample num:   20, data_idx: 38594\n",
            "sample num:   21, data_idx: 48088\n",
            "sample num:   22, data_idx: 19390\n",
            "sample num:   23, data_idx: 40427\n",
            "sample num:   24, data_idx: 49605\n",
            "sample num:   25, data_idx: 43840\n",
            "sample num:   26, data_idx: 23263\n",
            "sample num:   27, data_idx: 20163\n",
            "sample num:   28, data_idx:  7170\n",
            "sample num:   29, data_idx: 20703\n",
            "sample num:   30, data_idx: 30248\n",
            "sample num:   31, data_idx: 33527\n",
            "sample num:   32, data_idx:  7789\n",
            "sample num:   33, data_idx: 11332\n",
            "sample num:   34, data_idx:  2706\n",
            "sample num:   35, data_idx: 39740\n",
            "sample num:   36, data_idx: 48949\n",
            "sample num:   37, data_idx: 17294\n",
            "sample num:   38, data_idx: 11602\n",
            "sample num:   39, data_idx: 41228\n",
            "sample num:   40, data_idx: 46722\n",
            "sample num:   41, data_idx: 31281\n",
            "sample num:   42, data_idx:  1633\n",
            "sample num:   43, data_idx: 28910\n",
            "sample num:   44, data_idx: 16119\n",
            "sample num:   45, data_idx:  2861\n",
            "sample num:   46, data_idx: 14858\n",
            "sample num:   47, data_idx: 26036\n",
            "sample num:   48, data_idx:  8626\n",
            "sample num:   49, data_idx: 14119\n",
            "sample num:   50, data_idx: 34793\n",
            "sample num:   51, data_idx: 23811\n",
            "sample num:   52, data_idx: 46988\n",
            "sample num:   53, data_idx: 52371\n",
            "sample num:   54, data_idx: 20480\n",
            "sample num:   55, data_idx: 52163\n",
            "sample num:   56, data_idx:  3820\n",
            "sample num:   57, data_idx: 38850\n",
            "sample num:   58, data_idx:  9159\n",
            "sample num:   59, data_idx: 52932\n",
            "sample num:   60, data_idx: 16409\n",
            "sample num:   61, data_idx:  4979\n",
            "sample num:   62, data_idx: 40979\n",
            "sample num:   63, data_idx: 46545\n",
            "sample num:   64, data_idx: 51156\n",
            "sample num:   65, data_idx:   511\n",
            "sample num:   66, data_idx: 51060\n",
            "sample num:   67, data_idx: 10119\n",
            "sample num:   68, data_idx: 29802\n",
            "sample num:   69, data_idx: 41988\n",
            "sample num:   70, data_idx: 34296\n",
            "sample num:   71, data_idx: 43257\n",
            "sample num:   72, data_idx: 16602\n",
            "sample num:   73, data_idx: 47455\n",
            "sample num:   74, data_idx: 26029\n",
            "sample num:   75, data_idx: 40799\n",
            "sample num:   76, data_idx: 21208\n",
            "sample num:   77, data_idx: 44999\n",
            "sample num:   78, data_idx: 44107\n",
            "sample num:   79, data_idx: 36054\n",
            "sample num:   80, data_idx: 51408\n",
            "sample num:   81, data_idx:  2808\n",
            "sample num:   82, data_idx: 45854\n",
            "sample num:   83, data_idx:  9754\n",
            "sample num:   84, data_idx:  3256\n",
            "sample num:   85, data_idx: 46080\n",
            "sample num:   86, data_idx: 41521\n",
            "sample num:   87, data_idx: 13848\n",
            "sample num:   88, data_idx: 53040\n",
            "sample num:   89, data_idx: 42930\n",
            "sample num:   90, data_idx: 18055\n",
            "sample num:   91, data_idx: 24374\n",
            "sample num:   92, data_idx: 36313\n",
            "sample num:   93, data_idx: 42234\n",
            "sample num:   94, data_idx: 44661\n",
            "sample num:   95, data_idx: 48687\n",
            "sample num:   96, data_idx: 16612\n",
            "sample num:   97, data_idx: 38388\n",
            "sample num:   98, data_idx:  4643\n",
            "sample num:   99, data_idx: 34705\n",
            "mnist/mnist_network_fisher.npy\n",
            "[calculate_cost]\n",
            "toal_cost: 0.0\n",
            "458 pages allocated for 45706 weights\n",
            "total_network_cost: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD2Bl2AdzVvH",
        "outputId": "a05ade60-b0c2-4f80-c4be-0235480e83dd"
      },
      "source": [
        "!python weight_virtualization.py -mode=a -network_path=gsc | tee -a gsc_matching_three_nets.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "add_vnn\n",
            "gsc/gsc_network_weight.npy\n",
            "compute_fisher\n",
            "do_compute_fisher\n",
            "sample num:    0, data_idx: 62512\n",
            "sample num:    1, data_idx: 46370\n",
            "sample num:    2, data_idx: 84057\n",
            "sample num:    3, data_idx: 15240\n",
            "sample num:    4, data_idx: 22718\n",
            "sample num:    5, data_idx: 41530\n",
            "sample num:    6, data_idx:  3107\n",
            "sample num:    7, data_idx: 45550\n",
            "sample num:    8, data_idx: 16813\n",
            "sample num:    9, data_idx: 52130\n",
            "sample num:   10, data_idx: 84294\n",
            "sample num:   11, data_idx: 79692\n",
            "sample num:   12, data_idx: 38018\n",
            "sample num:   13, data_idx: 39334\n",
            "sample num:   14, data_idx: 71138\n",
            "sample num:   15, data_idx: 40081\n",
            "sample num:   16, data_idx:  7251\n",
            "sample num:   17, data_idx: 74638\n",
            "sample num:   18, data_idx: 40525\n",
            "sample num:   19, data_idx: 42330\n",
            "sample num:   20, data_idx: 71066\n",
            "sample num:   21, data_idx: 79527\n",
            "sample num:   22, data_idx: 31183\n",
            "sample num:   23, data_idx:  4307\n",
            "sample num:   24, data_idx: 29273\n",
            "sample num:   25, data_idx: 68131\n",
            "sample num:   26, data_idx: 60026\n",
            "sample num:   27, data_idx: 77211\n",
            "sample num:   28, data_idx: 49436\n",
            "sample num:   29, data_idx: 46946\n",
            "sample num:   30, data_idx:  8053\n",
            "sample num:   31, data_idx: 81323\n",
            "sample num:   32, data_idx: 12616\n",
            "sample num:   33, data_idx: 71983\n",
            "sample num:   34, data_idx: 59567\n",
            "sample num:   35, data_idx:   834\n",
            "sample num:   36, data_idx: 60284\n",
            "sample num:   37, data_idx: 41765\n",
            "sample num:   38, data_idx: 41730\n",
            "sample num:   39, data_idx: 17724\n",
            "sample num:   40, data_idx:  4527\n",
            "sample num:   41, data_idx: 16848\n",
            "sample num:   42, data_idx: 72040\n",
            "sample num:   43, data_idx:  6269\n",
            "sample num:   44, data_idx: 17289\n",
            "sample num:   45, data_idx: 36075\n",
            "sample num:   46, data_idx: 76624\n",
            "sample num:   47, data_idx: 78958\n",
            "sample num:   48, data_idx:  3718\n",
            "sample num:   49, data_idx: 48022\n",
            "sample num:   50, data_idx: 75954\n",
            "sample num:   51, data_idx: 15521\n",
            "sample num:   52, data_idx: 35048\n",
            "sample num:   53, data_idx: 48955\n",
            "sample num:   54, data_idx:  3501\n",
            "sample num:   55, data_idx: 59041\n",
            "sample num:   56, data_idx: 64454\n",
            "sample num:   57, data_idx:  5255\n",
            "sample num:   58, data_idx: 73706\n",
            "sample num:   59, data_idx:  7197\n",
            "sample num:   60, data_idx: 19824\n",
            "sample num:   61, data_idx: 41502\n",
            "sample num:   62, data_idx: 55835\n",
            "sample num:   63, data_idx: 35528\n",
            "sample num:   64, data_idx: 38986\n",
            "sample num:   65, data_idx:  7083\n",
            "sample num:   66, data_idx: 15549\n",
            "sample num:   67, data_idx: 44415\n",
            "sample num:   68, data_idx: 15883\n",
            "sample num:   69, data_idx: 28579\n",
            "sample num:   70, data_idx: 31677\n",
            "sample num:   71, data_idx: 35480\n",
            "sample num:   72, data_idx:  9959\n",
            "sample num:   73, data_idx: 32867\n",
            "sample num:   74, data_idx: 77124\n",
            "sample num:   75, data_idx: 79498\n",
            "sample num:   76, data_idx: 64365\n",
            "sample num:   77, data_idx: 72785\n",
            "sample num:   78, data_idx: 60586\n",
            "sample num:   79, data_idx: 63516\n",
            "sample num:   80, data_idx: 22101\n",
            "sample num:   81, data_idx: 77612\n",
            "sample num:   82, data_idx: 52597\n",
            "sample num:   83, data_idx: 17894\n",
            "sample num:   84, data_idx: 63334\n",
            "sample num:   85, data_idx: 40889\n",
            "sample num:   86, data_idx: 32032\n",
            "sample num:   87, data_idx:  5956\n",
            "sample num:   88, data_idx: 39406\n",
            "sample num:   89, data_idx: 64693\n",
            "sample num:   90, data_idx: 27563\n",
            "sample num:   91, data_idx: 39506\n",
            "sample num:   92, data_idx: 29037\n",
            "sample num:   93, data_idx: 59522\n",
            "sample num:   94, data_idx: 18052\n",
            "sample num:   95, data_idx: 13141\n",
            "sample num:   96, data_idx: 52147\n",
            "sample num:   97, data_idx: 26651\n",
            "sample num:   98, data_idx:  5342\n",
            "sample num:   99, data_idx: 52444\n",
            "gsc/gsc_network_fisher.npy\n",
            "[match_page_by_cost]\n",
            "occupation: 0\n",
            "len(page_list): 263\n",
            "len(network_page_list): 656\n",
            "cost: 0.0\n",
            "\n",
            "occupation: 1\n",
            "len(page_list): 458\n",
            "len(network_page_list): 393\n",
            "cost: 0.0048742327\n",
            "\n",
            "assing_page 273.360 ms\n",
            "[calculate_cost]\n",
            "toal_cost: 0.004874234806550248\n",
            "656 pages allocated for 65531 weights\n",
            "total_network_cost: 0.010059744119644165\n",
            "       0-th page\n",
            "     262-th page\n",
            "       0-th page\n",
            "     392-th page\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzG8FANj2WTC",
        "outputId": "98728c9e-7b89-460c-cb2a-636aef72320d"
      },
      "source": [
        "!python weight_virtualization.py -mode=a -network_path=gtsrb | tee -a gtsrb_matching_three_nets.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "add_vnn\n",
            "gtsrb/gtsrb_network_weight.npy\n",
            "compute_fisher\n",
            "do_compute_fisher\n",
            "sample num:    0, data_idx:  2208\n",
            "sample num:    1, data_idx: 20686\n",
            "sample num:    2, data_idx: 35565\n",
            "sample num:    3, data_idx: 28000\n",
            "sample num:    4, data_idx: 18716\n",
            "sample num:    5, data_idx: 11090\n",
            "sample num:    6, data_idx:  8141\n",
            "sample num:    7, data_idx: 12204\n",
            "sample num:    8, data_idx:  5032\n",
            "sample num:    9, data_idx: 38508\n",
            "sample num:   10, data_idx: 26875\n",
            "sample num:   11, data_idx: 13591\n",
            "sample num:   12, data_idx: 29610\n",
            "sample num:   13, data_idx:  9704\n",
            "sample num:   14, data_idx: 23563\n",
            "sample num:   15, data_idx: 11501\n",
            "sample num:   16, data_idx: 22925\n",
            "sample num:   17, data_idx:  4434\n",
            "sample num:   18, data_idx: 19496\n",
            "sample num:   19, data_idx: 11663\n",
            "sample num:   20, data_idx: 21814\n",
            "sample num:   21, data_idx: 27894\n",
            "sample num:   22, data_idx:  4206\n",
            "sample num:   23, data_idx: 24307\n",
            "sample num:   24, data_idx:  1784\n",
            "sample num:   25, data_idx: 30657\n",
            "sample num:   26, data_idx: 25742\n",
            "sample num:   27, data_idx: 12986\n",
            "sample num:   28, data_idx: 36580\n",
            "sample num:   29, data_idx: 18865\n",
            "sample num:   30, data_idx: 32112\n",
            "sample num:   31, data_idx: 16736\n",
            "sample num:   32, data_idx:   937\n",
            "sample num:   33, data_idx: 30086\n",
            "sample num:   34, data_idx: 13651\n",
            "sample num:   35, data_idx: 34847\n",
            "sample num:   36, data_idx: 38052\n",
            "sample num:   37, data_idx:  4395\n",
            "sample num:   38, data_idx: 31068\n",
            "sample num:   39, data_idx: 31160\n",
            "sample num:   40, data_idx: 26897\n",
            "sample num:   41, data_idx: 26477\n",
            "sample num:   42, data_idx: 33663\n",
            "sample num:   43, data_idx: 25033\n",
            "sample num:   44, data_idx: 21987\n",
            "sample num:   45, data_idx: 17481\n",
            "sample num:   46, data_idx: 16806\n",
            "sample num:   47, data_idx: 16233\n",
            "sample num:   48, data_idx: 14388\n",
            "sample num:   49, data_idx: 18129\n",
            "sample num:   50, data_idx: 31679\n",
            "sample num:   51, data_idx: 13449\n",
            "sample num:   52, data_idx: 31415\n",
            "sample num:   53, data_idx:  3587\n",
            "sample num:   54, data_idx: 24879\n",
            "sample num:   55, data_idx: 30034\n",
            "sample num:   56, data_idx:  5208\n",
            "sample num:   57, data_idx: 10629\n",
            "sample num:   58, data_idx: 38121\n",
            "sample num:   59, data_idx: 34691\n",
            "sample num:   60, data_idx: 19974\n",
            "sample num:   61, data_idx: 32728\n",
            "sample num:   62, data_idx: 14109\n",
            "sample num:   63, data_idx: 11736\n",
            "sample num:   64, data_idx: 38919\n",
            "sample num:   65, data_idx: 31478\n",
            "sample num:   66, data_idx: 26634\n",
            "sample num:   67, data_idx:  6008\n",
            "sample num:   68, data_idx: 27030\n",
            "sample num:   69, data_idx:  6861\n",
            "sample num:   70, data_idx:  1113\n",
            "sample num:   71, data_idx: 21931\n",
            "sample num:   72, data_idx: 25978\n",
            "sample num:   73, data_idx: 33405\n",
            "sample num:   74, data_idx:   516\n",
            "sample num:   75, data_idx: 19341\n",
            "sample num:   76, data_idx: 19036\n",
            "sample num:   77, data_idx: 33261\n",
            "sample num:   78, data_idx: 18705\n",
            "sample num:   79, data_idx:  8665\n",
            "sample num:   80, data_idx: 20545\n",
            "sample num:   81, data_idx: 16855\n",
            "sample num:   82, data_idx: 11406\n",
            "sample num:   83, data_idx: 19120\n",
            "sample num:   84, data_idx: 29970\n",
            "sample num:   85, data_idx: 20806\n",
            "sample num:   86, data_idx: 23608\n",
            "sample num:   87, data_idx: 38993\n",
            "sample num:   88, data_idx: 25233\n",
            "sample num:   89, data_idx:  6730\n",
            "sample num:   90, data_idx: 37764\n",
            "sample num:   91, data_idx: 21691\n",
            "sample num:   92, data_idx: 28977\n",
            "sample num:   93, data_idx: 32528\n",
            "sample num:   94, data_idx:  6034\n",
            "sample num:   95, data_idx: 36304\n",
            "sample num:   96, data_idx: 14229\n",
            "sample num:   97, data_idx: 23818\n",
            "sample num:   98, data_idx: 15420\n",
            "sample num:   99, data_idx:  6575\n",
            "gtsrb/gtsrb_network_fisher.npy\n",
            "[match_page_by_cost]\n",
            "occupation: 0\n",
            "len(page_list): 0\n",
            "len(network_page_list): 665\n",
            "cost: 0\n",
            "\n",
            "occupation: 1\n",
            "len(page_list): 328\n",
            "len(network_page_list): 665\n",
            "cost: 1.3101691\n",
            "\n",
            "occupation: 2\n",
            "len(page_list): 393\n",
            "len(network_page_list): 337\n",
            "cost: 0.004256283\n",
            "\n",
            "assing_page 138.218 ms\n",
            "[calculate_cost]\n",
            "toal_cost: 1.3144653116842164\n",
            "665 pages allocated for 66475 weights\n",
            "total_network_cost: 5.479358911514282\n",
            "       0-th page\n",
            "     327-th page\n",
            "       0-th page\n",
            "     336-th page\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnN79J9qNdck"
      },
      "source": [
        "import time\n",
        "beginning = time.time()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeFoQ-r53VDw",
        "outputId": "ffadd663-5dc2-44a3-b82e-8a40a7d3785c"
      },
      "source": [
        "!bash ./joint_optimization.sh"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-th joint optimization\n",
            "get_matching_loss\n",
            "v_train\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "step 0, training accuracy: 0.970000 original loss: 4.865963 matching loss: 0.001769\n",
            "step 0, Validation accuracy: 0.973500\n",
            "step 100, training accuracy: 0.960000 original loss: 4.766099 matching loss: 0.001021\n",
            "step 100, Validation accuracy: 0.978900\n",
            "get new weight for 0.9789\n",
            "step 200, training accuracy: 0.990000 original loss: 4.646124 matching loss: 0.001110\n",
            "step 200, Validation accuracy: 0.979800\n",
            "get new weight for 0.9798\n",
            "step 300, training accuracy: 0.960000 original loss: 4.739178 matching loss: 0.001137\n",
            "step 300, Validation accuracy: 0.980200\n",
            "get new weight for 0.9802\n",
            "step 400, training accuracy: 0.990000 original loss: 4.628866 matching loss: 0.001196\n",
            "step 400, Validation accuracy: 0.981100\n",
            "get new weight for 0.9811\n",
            "step 500, training accuracy: 0.980000 original loss: 4.666474 matching loss: 0.001225\n",
            "step 500, Validation accuracy: 0.979000\n",
            "step 600, training accuracy: 0.970000 original loss: 4.696264 matching loss: 0.001269\n",
            "step 600, Validation accuracy: 0.978900\n",
            "step 700, training accuracy: 0.970000 original loss: 4.680432 matching loss: 0.001313\n",
            "step 700, Validation accuracy: 0.979200\n",
            "step 800, training accuracy: 0.980000 original loss: 4.651668 matching loss: 0.001359\n",
            "step 800, Validation accuracy: 0.980100\n",
            "step 900, training accuracy: 0.980000 original loss: 4.657212 matching loss: 0.001396\n",
            "step 900, Validation accuracy: 0.981800\n",
            "get new weight for 0.9818\n",
            "step 1000, training accuracy: 1.000000 original loss: 4.642266 matching loss: 0.001432\n",
            "step 1000, Validation accuracy: 0.980700\n",
            "step 1100, training accuracy: 1.000000 original loss: 4.643449 matching loss: 0.001476\n",
            "step 1100, Validation accuracy: 0.981000\n",
            "step 1200, training accuracy: 1.000000 original loss: 4.645471 matching loss: 0.001529\n",
            "step 1200, Validation accuracy: 0.980300\n",
            "step 1300, training accuracy: 0.990000 original loss: 4.643557 matching loss: 0.001606\n",
            "step 1300, Validation accuracy: 0.981700\n",
            "step 1400, training accuracy: 0.980000 original loss: 4.669388 matching loss: 0.001661\n",
            "step 1400, Validation accuracy: 0.980800\n",
            "step 1500, training accuracy: 0.990000 original loss: 4.641891 matching loss: 0.001700\n",
            "step 1500, Validation accuracy: 0.982200\n",
            "get new weight for 0.9822\n",
            "step 1600, training accuracy: 1.000000 original loss: 4.624533 matching loss: 0.001753\n",
            "step 1600, Validation accuracy: 0.980900\n",
            "step 1700, training accuracy: 0.990000 original loss: 4.685210 matching loss: 0.001808\n",
            "step 1700, Validation accuracy: 0.979700\n",
            "step 1800, training accuracy: 0.960000 original loss: 4.714972 matching loss: 0.001842\n",
            "step 1800, Validation accuracy: 0.980600\n",
            "step 1900, training accuracy: 1.000000 original loss: 4.622663 matching loss: 0.001926\n",
            "step 1900, Validation accuracy: 0.980400\n",
            "step 1999, training accuracy: 0.980000 original loss: 4.682531 matching loss: 0.001948\n",
            "step 1999, Validation accuracy: 0.977200\n",
            "mnist/mnist_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.780000 original loss: 5.489896 matching loss: 0.000284\n",
            "step 0, Validation accuracy: 0.710404\n",
            "step 100, training accuracy: 0.680000 original loss: 5.725124 matching loss: 0.002411\n",
            "step 100, Validation accuracy: 0.719764\n",
            "get new weight for 0.71976376\n",
            "step 200, training accuracy: 0.770000 original loss: 5.521921 matching loss: 0.003015\n",
            "step 200, Validation accuracy: 0.728487\n",
            "get new weight for 0.7284871\n",
            "step 300, training accuracy: 0.740000 original loss: 5.624941 matching loss: 0.003394\n",
            "step 300, Validation accuracy: 0.727215\n",
            "step 400, training accuracy: 0.620000 original loss: 5.634255 matching loss: 0.003953\n",
            "step 400, Validation accuracy: 0.722581\n",
            "step 500, training accuracy: 0.740000 original loss: 5.528656 matching loss: 0.004074\n",
            "step 500, Validation accuracy: 0.729668\n",
            "get new weight for 0.7296683\n",
            "step 600, training accuracy: 0.740000 original loss: 5.579065 matching loss: 0.004378\n",
            "step 600, Validation accuracy: 0.732576\n",
            "get new weight for 0.7325761\n",
            "step 700, training accuracy: 0.750000 original loss: 5.493476 matching loss: 0.004628\n",
            "step 700, Validation accuracy: 0.730123\n",
            "step 800, training accuracy: 0.780000 original loss: 5.534302 matching loss: 0.004839\n",
            "step 800, Validation accuracy: 0.733848\n",
            "get new weight for 0.7338483\n",
            "step 900, training accuracy: 0.840000 original loss: 5.326745 matching loss: 0.005197\n",
            "step 900, Validation accuracy: 0.730668\n",
            "step 1000, training accuracy: 0.800000 original loss: 5.432518 matching loss: 0.005640\n",
            "step 1000, Validation accuracy: 0.733212\n",
            "step 1100, training accuracy: 0.740000 original loss: 5.538760 matching loss: 0.005811\n",
            "step 1100, Validation accuracy: 0.733121\n",
            "step 1200, training accuracy: 0.760000 original loss: 5.497209 matching loss: 0.006051\n",
            "step 1200, Validation accuracy: 0.734393\n",
            "get new weight for 0.7343935\n",
            "step 1300, training accuracy: 0.740000 original loss: 5.707261 matching loss: 0.006214\n",
            "step 1300, Validation accuracy: 0.741027\n",
            "get new weight for 0.7410268\n",
            "step 1400, training accuracy: 0.760000 original loss: 5.527780 matching loss: 0.006479\n",
            "step 1400, Validation accuracy: 0.737392\n",
            "step 1500, training accuracy: 0.770000 original loss: 5.513696 matching loss: 0.006662\n",
            "step 1500, Validation accuracy: 0.736847\n",
            "step 1600, training accuracy: 0.780000 original loss: 5.400256 matching loss: 0.006701\n",
            "step 1600, Validation accuracy: 0.744480\n",
            "get new weight for 0.7444798\n",
            "step 1700, training accuracy: 0.830000 original loss: 5.456299 matching loss: 0.006961\n",
            "step 1700, Validation accuracy: 0.739391\n",
            "step 1800, training accuracy: 0.840000 original loss: 5.284925 matching loss: 0.007116\n",
            "step 1800, Validation accuracy: 0.742844\n",
            "step 1900, training accuracy: 0.800000 original loss: 5.532341 matching loss: 0.007491\n",
            "step 1900, Validation accuracy: 0.739209\n",
            "step 1999, training accuracy: 0.770000 original loss: 5.496006 matching loss: 0.007749\n",
            "step 1999, Validation accuracy: 0.741027\n",
            "gsc/gsc_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.990000 original loss: 4.863671 matching loss: 0.000241\n",
            "step 0, Validation accuracy: 0.926682\n",
            "step 100, training accuracy: 0.980000 original loss: 4.772933 matching loss: 0.002506\n",
            "step 100, Validation accuracy: 0.934838\n",
            "get new weight for 0.9348377\n",
            "step 200, training accuracy: 0.990000 original loss: 4.706858 matching loss: 0.002893\n",
            "step 200, Validation accuracy: 0.939588\n",
            "get new weight for 0.9395883\n",
            "step 300, training accuracy: 0.990000 original loss: 4.713250 matching loss: 0.003321\n",
            "step 300, Validation accuracy: 0.941489\n",
            "get new weight for 0.9414885\n",
            "step 400, training accuracy: 0.990000 original loss: 4.719548 matching loss: 0.003440\n",
            "step 400, Validation accuracy: 0.935550\n",
            "step 500, training accuracy: 0.990000 original loss: 4.724765 matching loss: 0.003946\n",
            "step 500, Validation accuracy: 0.942439\n",
            "get new weight for 0.94243866\n",
            "step 600, training accuracy: 1.000000 original loss: 4.688295 matching loss: 0.003959\n",
            "step 600, Validation accuracy: 0.942122\n",
            "step 700, training accuracy: 0.980000 original loss: 4.763489 matching loss: 0.004362\n",
            "step 700, Validation accuracy: 0.942201\n",
            "step 800, training accuracy: 0.980000 original loss: 4.721521 matching loss: 0.004408\n",
            "step 800, Validation accuracy: 0.940697\n",
            "step 900, training accuracy: 1.000000 original loss: 4.719671 matching loss: 0.004478\n",
            "step 900, Validation accuracy: 0.945527\n",
            "get new weight for 0.94552654\n",
            "step 1000, training accuracy: 0.980000 original loss: 4.689508 matching loss: 0.004931\n",
            "step 1000, Validation accuracy: 0.945447\n",
            "step 1100, training accuracy: 0.980000 original loss: 4.783482 matching loss: 0.004816\n",
            "step 1100, Validation accuracy: 0.941330\n",
            "step 1200, training accuracy: 0.980000 original loss: 4.722590 matching loss: 0.005103\n",
            "step 1200, Validation accuracy: 0.941251\n",
            "step 1300, training accuracy: 0.990000 original loss: 4.676875 matching loss: 0.005313\n",
            "step 1300, Validation accuracy: 0.945447\n",
            "step 1400, training accuracy: 1.000000 original loss: 4.706590 matching loss: 0.005359\n",
            "step 1400, Validation accuracy: 0.943468\n",
            "step 1500, training accuracy: 1.000000 original loss: 4.665964 matching loss: 0.005469\n",
            "step 1500, Validation accuracy: 0.943389\n",
            "step 1600, training accuracy: 1.000000 original loss: 4.667422 matching loss: 0.005642\n",
            "step 1600, Validation accuracy: 0.944656\n",
            "step 1700, training accuracy: 1.000000 original loss: 4.682448 matching loss: 0.005863\n",
            "step 1700, Validation accuracy: 0.941409\n",
            "step 1800, training accuracy: 1.000000 original loss: 4.696645 matching loss: 0.005969\n",
            "step 1800, Validation accuracy: 0.943943\n",
            "step 1900, training accuracy: 1.000000 original loss: 4.684995 matching loss: 0.006168\n",
            "step 1900, Validation accuracy: 0.945685\n",
            "get new weight for 0.94568485\n",
            "step 1999, training accuracy: 0.990000 original loss: 4.713486 matching loss: 0.006248\n",
            "step 1999, Validation accuracy: 0.945922\n",
            "get new weight for 0.94592243\n",
            "gtsrb/gtsrb_weight.npy\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.978800\n",
            "GSC performance\n",
            "Inference accuracy: 0.720036\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.945922\n",
            "2-th joint optimization\n",
            "get_matching_loss\n",
            "v_train\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "step 0, training accuracy: 0.970000 original loss: 4.765578 matching loss: 0.001453\n",
            "step 0, Validation accuracy: 0.978800\n",
            "step 100, training accuracy: 0.960000 original loss: 4.713526 matching loss: 0.000848\n",
            "step 100, Validation accuracy: 0.981100\n",
            "get new weight for 0.9811\n",
            "step 200, training accuracy: 0.990000 original loss: 4.653446 matching loss: 0.000892\n",
            "step 200, Validation accuracy: 0.980800\n",
            "step 300, training accuracy: 1.000000 original loss: 4.637277 matching loss: 0.000939\n",
            "step 300, Validation accuracy: 0.981900\n",
            "get new weight for 0.9819\n",
            "step 400, training accuracy: 0.990000 original loss: 4.673826 matching loss: 0.000981\n",
            "step 400, Validation accuracy: 0.982000\n",
            "get new weight for 0.982\n",
            "step 500, training accuracy: 0.980000 original loss: 4.659026 matching loss: 0.001016\n",
            "step 500, Validation accuracy: 0.979100\n",
            "step 600, training accuracy: 0.980000 original loss: 4.674582 matching loss: 0.001048\n",
            "step 600, Validation accuracy: 0.980600\n",
            "step 700, training accuracy: 0.970000 original loss: 4.668212 matching loss: 0.001102\n",
            "step 700, Validation accuracy: 0.978000\n",
            "step 800, training accuracy: 1.000000 original loss: 4.628409 matching loss: 0.001137\n",
            "step 800, Validation accuracy: 0.982900\n",
            "get new weight for 0.9829\n",
            "step 900, training accuracy: 1.000000 original loss: 4.637072 matching loss: 0.001173\n",
            "step 900, Validation accuracy: 0.981700\n",
            "step 1000, training accuracy: 0.990000 original loss: 4.634896 matching loss: 0.001207\n",
            "step 1000, Validation accuracy: 0.980500\n",
            "step 1100, training accuracy: 0.990000 original loss: 4.669617 matching loss: 0.001251\n",
            "step 1100, Validation accuracy: 0.981300\n",
            "step 1200, training accuracy: 0.990000 original loss: 4.662778 matching loss: 0.001281\n",
            "step 1200, Validation accuracy: 0.981400\n",
            "step 1300, training accuracy: 0.990000 original loss: 4.644644 matching loss: 0.001320\n",
            "step 1300, Validation accuracy: 0.981300\n",
            "step 1400, training accuracy: 0.980000 original loss: 4.672682 matching loss: 0.001372\n",
            "step 1400, Validation accuracy: 0.981300\n",
            "step 1500, training accuracy: 0.960000 original loss: 4.686422 matching loss: 0.001421\n",
            "step 1500, Validation accuracy: 0.977700\n",
            "step 1600, training accuracy: 0.970000 original loss: 4.678461 matching loss: 0.001457\n",
            "step 1600, Validation accuracy: 0.981300\n",
            "step 1700, training accuracy: 1.000000 original loss: 4.629725 matching loss: 0.001523\n",
            "step 1700, Validation accuracy: 0.983300\n",
            "get new weight for 0.9833\n",
            "step 1800, training accuracy: 0.990000 original loss: 4.656708 matching loss: 0.001582\n",
            "step 1800, Validation accuracy: 0.980200\n",
            "step 1900, training accuracy: 1.000000 original loss: 4.625382 matching loss: 0.001611\n",
            "step 1900, Validation accuracy: 0.982200\n",
            "step 1999, training accuracy: 0.990000 original loss: 4.648218 matching loss: 0.001646\n",
            "step 1999, Validation accuracy: 0.981500\n",
            "mnist/mnist_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.700000 original loss: 5.767438 matching loss: 0.000279\n",
            "step 0, Validation accuracy: 0.718128\n",
            "step 100, training accuracy: 0.750000 original loss: 5.469110 matching loss: 0.002525\n",
            "step 100, Validation accuracy: 0.736029\n",
            "get new weight for 0.7360291\n",
            "step 200, training accuracy: 0.790000 original loss: 5.412967 matching loss: 0.003056\n",
            "step 200, Validation accuracy: 0.734848\n",
            "step 300, training accuracy: 0.790000 original loss: 5.458372 matching loss: 0.003443\n",
            "step 300, Validation accuracy: 0.736393\n",
            "get new weight for 0.73639256\n",
            "step 400, training accuracy: 0.820000 original loss: 5.392362 matching loss: 0.003783\n",
            "step 400, Validation accuracy: 0.734666\n",
            "step 500, training accuracy: 0.720000 original loss: 5.664542 matching loss: 0.003957\n",
            "step 500, Validation accuracy: 0.742026\n",
            "get new weight for 0.7420263\n",
            "step 600, training accuracy: 0.810000 original loss: 5.484927 matching loss: 0.004068\n",
            "step 600, Validation accuracy: 0.739664\n",
            "step 700, training accuracy: 0.770000 original loss: 5.498127 matching loss: 0.004310\n",
            "step 700, Validation accuracy: 0.738664\n",
            "step 800, training accuracy: 0.790000 original loss: 5.359040 matching loss: 0.004582\n",
            "step 800, Validation accuracy: 0.741299\n",
            "step 900, training accuracy: 0.690000 original loss: 5.766479 matching loss: 0.004826\n",
            "step 900, Validation accuracy: 0.740572\n",
            "step 1000, training accuracy: 0.750000 original loss: 5.540024 matching loss: 0.005081\n",
            "step 1000, Validation accuracy: 0.744116\n",
            "get new weight for 0.7441163\n",
            "step 1100, training accuracy: 0.840000 original loss: 5.378771 matching loss: 0.005320\n",
            "step 1100, Validation accuracy: 0.742753\n",
            "step 1200, training accuracy: 0.710000 original loss: 5.633227 matching loss: 0.005524\n",
            "step 1200, Validation accuracy: 0.738210\n",
            "step 1300, training accuracy: 0.760000 original loss: 5.531943 matching loss: 0.005693\n",
            "step 1300, Validation accuracy: 0.743571\n",
            "step 1400, training accuracy: 0.790000 original loss: 5.353248 matching loss: 0.005890\n",
            "step 1400, Validation accuracy: 0.742844\n",
            "step 1500, training accuracy: 0.810000 original loss: 5.465642 matching loss: 0.006016\n",
            "step 1500, Validation accuracy: 0.741118\n",
            "step 1600, training accuracy: 0.770000 original loss: 5.512200 matching loss: 0.006132\n",
            "step 1600, Validation accuracy: 0.744662\n",
            "get new weight for 0.7446615\n",
            "step 1700, training accuracy: 0.760000 original loss: 5.514351 matching loss: 0.006367\n",
            "step 1700, Validation accuracy: 0.741572\n",
            "step 1800, training accuracy: 0.770000 original loss: 5.468137 matching loss: 0.006541\n",
            "step 1800, Validation accuracy: 0.743571\n",
            "step 1900, training accuracy: 0.720000 original loss: 5.557419 matching loss: 0.006679\n",
            "step 1900, Validation accuracy: 0.742117\n",
            "step 1999, training accuracy: 0.760000 original loss: 5.490469 matching loss: 0.006930\n",
            "step 1999, Validation accuracy: 0.746933\n",
            "get new weight for 0.7469332\n",
            "gsc/gsc_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.970000 original loss: 4.863351 matching loss: 0.000284\n",
            "step 0, Validation accuracy: 0.935550\n",
            "step 100, training accuracy: 0.980000 original loss: 4.758232 matching loss: 0.002660\n",
            "step 100, Validation accuracy: 0.937846\n",
            "get new weight for 0.9378464\n",
            "step 200, training accuracy: 0.990000 original loss: 4.703611 matching loss: 0.003040\n",
            "step 200, Validation accuracy: 0.941884\n",
            "get new weight for 0.9418844\n",
            "step 300, training accuracy: 0.990000 original loss: 4.702359 matching loss: 0.003059\n",
            "step 300, Validation accuracy: 0.945210\n",
            "get new weight for 0.9452098\n",
            "step 400, training accuracy: 1.000000 original loss: 4.676590 matching loss: 0.003161\n",
            "step 400, Validation accuracy: 0.944181\n",
            "step 500, training accuracy: 1.000000 original loss: 4.675938 matching loss: 0.003406\n",
            "step 500, Validation accuracy: 0.945289\n",
            "get new weight for 0.945289\n",
            "step 600, training accuracy: 1.000000 original loss: 4.685375 matching loss: 0.003713\n",
            "step 600, Validation accuracy: 0.943943\n",
            "step 700, training accuracy: 0.990000 original loss: 4.662680 matching loss: 0.003696\n",
            "step 700, Validation accuracy: 0.945051\n",
            "step 800, training accuracy: 0.980000 original loss: 4.726129 matching loss: 0.004492\n",
            "step 800, Validation accuracy: 0.943943\n",
            "step 900, training accuracy: 0.990000 original loss: 4.717399 matching loss: 0.004183\n",
            "step 900, Validation accuracy: 0.943468\n",
            "step 1000, training accuracy: 1.000000 original loss: 4.682396 matching loss: 0.004300\n",
            "step 1000, Validation accuracy: 0.946160\n",
            "get new weight for 0.94615996\n",
            "step 1100, training accuracy: 1.000000 original loss: 4.697618 matching loss: 0.004442\n",
            "step 1100, Validation accuracy: 0.944814\n",
            "step 1200, training accuracy: 0.990000 original loss: 4.686393 matching loss: 0.004519\n",
            "step 1200, Validation accuracy: 0.945606\n",
            "step 1300, training accuracy: 0.980000 original loss: 4.706469 matching loss: 0.004493\n",
            "step 1300, Validation accuracy: 0.941805\n",
            "step 1400, training accuracy: 1.000000 original loss: 4.688255 matching loss: 0.004707\n",
            "step 1400, Validation accuracy: 0.945527\n",
            "step 1500, training accuracy: 1.000000 original loss: 4.663496 matching loss: 0.005154\n",
            "step 1500, Validation accuracy: 0.944735\n",
            "step 1600, training accuracy: 1.000000 original loss: 4.674679 matching loss: 0.004896\n",
            "step 1600, Validation accuracy: 0.948456\n",
            "get new weight for 0.94845605\n",
            "step 1700, training accuracy: 1.000000 original loss: 4.656986 matching loss: 0.005160\n",
            "step 1700, Validation accuracy: 0.947981\n",
            "step 1800, training accuracy: 0.990000 original loss: 4.674701 matching loss: 0.005130\n",
            "step 1800, Validation accuracy: 0.948219\n",
            "step 1900, training accuracy: 1.000000 original loss: 4.650656 matching loss: 0.005418\n",
            "step 1900, Validation accuracy: 0.948694\n",
            "get new weight for 0.9486936\n",
            "step 1999, training accuracy: 0.990000 original loss: 4.683969 matching loss: 0.005494\n",
            "step 1999, Validation accuracy: 0.947427\n",
            "gtsrb/gtsrb_weight.npy\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.979000\n",
            "GSC performance\n",
            "Inference accuracy: 0.733212\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.948694\n",
            "3-th joint optimization\n",
            "get_matching_loss\n",
            "v_train\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "step 0, training accuracy: 0.990000 original loss: 4.751141 matching loss: 0.001251\n",
            "step 0, Validation accuracy: 0.979000\n",
            "step 100, training accuracy: 0.990000 original loss: 4.657795 matching loss: 0.000753\n",
            "step 100, Validation accuracy: 0.981000\n",
            "get new weight for 0.981\n",
            "step 200, training accuracy: 1.000000 original loss: 4.619754 matching loss: 0.000801\n",
            "step 200, Validation accuracy: 0.980200\n",
            "step 300, training accuracy: 0.990000 original loss: 4.641828 matching loss: 0.000844\n",
            "step 300, Validation accuracy: 0.983100\n",
            "get new weight for 0.9831\n",
            "step 400, training accuracy: 0.970000 original loss: 4.691998 matching loss: 0.000887\n",
            "step 400, Validation accuracy: 0.980500\n",
            "step 500, training accuracy: 0.970000 original loss: 4.680176 matching loss: 0.000901\n",
            "step 500, Validation accuracy: 0.982100\n",
            "step 600, training accuracy: 0.980000 original loss: 4.660704 matching loss: 0.000918\n",
            "step 600, Validation accuracy: 0.982900\n",
            "step 700, training accuracy: 0.980000 original loss: 4.672854 matching loss: 0.000945\n",
            "step 700, Validation accuracy: 0.979000\n",
            "step 800, training accuracy: 1.000000 original loss: 4.633465 matching loss: 0.000999\n",
            "step 800, Validation accuracy: 0.982000\n",
            "step 900, training accuracy: 1.000000 original loss: 4.637228 matching loss: 0.001027\n",
            "step 900, Validation accuracy: 0.981400\n",
            "step 1000, training accuracy: 0.970000 original loss: 4.688251 matching loss: 0.001084\n",
            "step 1000, Validation accuracy: 0.981500\n",
            "step 1100, training accuracy: 0.970000 original loss: 4.666344 matching loss: 0.001109\n",
            "step 1100, Validation accuracy: 0.981500\n",
            "step 1200, training accuracy: 0.990000 original loss: 4.640295 matching loss: 0.001153\n",
            "step 1200, Validation accuracy: 0.982200\n",
            "step 1300, training accuracy: 0.980000 original loss: 4.683245 matching loss: 0.001198\n",
            "step 1300, Validation accuracy: 0.982100\n",
            "step 1400, training accuracy: 1.000000 original loss: 4.636116 matching loss: 0.001233\n",
            "step 1400, Validation accuracy: 0.982800\n",
            "step 1500, training accuracy: 0.980000 original loss: 4.645590 matching loss: 0.001258\n",
            "step 1500, Validation accuracy: 0.982000\n",
            "step 1600, training accuracy: 1.000000 original loss: 4.624941 matching loss: 0.001281\n",
            "step 1600, Validation accuracy: 0.982700\n",
            "step 1700, training accuracy: 1.000000 original loss: 4.614588 matching loss: 0.001311\n",
            "step 1700, Validation accuracy: 0.981100\n",
            "step 1800, training accuracy: 1.000000 original loss: 4.627167 matching loss: 0.001361\n",
            "step 1800, Validation accuracy: 0.980300\n",
            "step 1900, training accuracy: 1.000000 original loss: 4.635392 matching loss: 0.001392\n",
            "step 1900, Validation accuracy: 0.981000\n",
            "step 1999, training accuracy: 1.000000 original loss: 4.635500 matching loss: 0.001462\n",
            "step 1999, Validation accuracy: 0.980700\n",
            "mnist/mnist_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.760000 original loss: 5.498608 matching loss: 0.000052\n",
            "step 0, Validation accuracy: 0.731667\n",
            "step 100, training accuracy: 0.700000 original loss: 5.675123 matching loss: 0.002403\n",
            "step 100, Validation accuracy: 0.744025\n",
            "get new weight for 0.74402547\n",
            "step 200, training accuracy: 0.770000 original loss: 5.502031 matching loss: 0.002792\n",
            "step 200, Validation accuracy: 0.750295\n",
            "get new weight for 0.75029534\n",
            "step 300, training accuracy: 0.700000 original loss: 5.727556 matching loss: 0.003208\n",
            "step 300, Validation accuracy: 0.743662\n",
            "step 400, training accuracy: 0.830000 original loss: 5.380404 matching loss: 0.003537\n",
            "step 400, Validation accuracy: 0.745752\n",
            "step 500, training accuracy: 0.810000 original loss: 5.378290 matching loss: 0.003726\n",
            "step 500, Validation accuracy: 0.749387\n",
            "step 600, training accuracy: 0.810000 original loss: 5.332443 matching loss: 0.003802\n",
            "step 600, Validation accuracy: 0.750841\n",
            "get new weight for 0.75084054\n",
            "step 700, training accuracy: 0.740000 original loss: 5.451639 matching loss: 0.004068\n",
            "step 700, Validation accuracy: 0.749205\n",
            "step 800, training accuracy: 0.740000 original loss: 5.509796 matching loss: 0.004265\n",
            "step 800, Validation accuracy: 0.752567\n",
            "get new weight for 0.752567\n",
            "step 900, training accuracy: 0.770000 original loss: 5.444672 matching loss: 0.004549\n",
            "step 900, Validation accuracy: 0.746479\n",
            "step 1000, training accuracy: 0.760000 original loss: 5.478872 matching loss: 0.004767\n",
            "step 1000, Validation accuracy: 0.749659\n",
            "step 1100, training accuracy: 0.730000 original loss: 5.498577 matching loss: 0.004815\n",
            "step 1100, Validation accuracy: 0.750295\n",
            "step 1200, training accuracy: 0.790000 original loss: 5.450972 matching loss: 0.004929\n",
            "step 1200, Validation accuracy: 0.753021\n",
            "get new weight for 0.75302136\n",
            "step 1300, training accuracy: 0.750000 original loss: 5.554426 matching loss: 0.005187\n",
            "step 1300, Validation accuracy: 0.751022\n",
            "step 1400, training accuracy: 0.820000 original loss: 5.326213 matching loss: 0.005269\n",
            "step 1400, Validation accuracy: 0.751477\n",
            "step 1500, training accuracy: 0.750000 original loss: 5.447823 matching loss: 0.005485\n",
            "step 1500, Validation accuracy: 0.744389\n",
            "step 1600, training accuracy: 0.790000 original loss: 5.434050 matching loss: 0.005682\n",
            "step 1600, Validation accuracy: 0.753567\n",
            "get new weight for 0.75356656\n",
            "step 1700, training accuracy: 0.810000 original loss: 5.404268 matching loss: 0.005770\n",
            "step 1700, Validation accuracy: 0.751749\n",
            "step 1800, training accuracy: 0.720000 original loss: 5.497003 matching loss: 0.005906\n",
            "step 1800, Validation accuracy: 0.755929\n",
            "get new weight for 0.7559291\n",
            "step 1900, training accuracy: 0.810000 original loss: 5.373507 matching loss: 0.006147\n",
            "step 1900, Validation accuracy: 0.752840\n",
            "step 1999, training accuracy: 0.830000 original loss: 5.310773 matching loss: 0.006223\n",
            "step 1999, Validation accuracy: 0.750750\n",
            "gsc/gsc_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.990000 original loss: 4.786044 matching loss: 0.000211\n",
            "step 0, Validation accuracy: 0.938717\n",
            "step 100, training accuracy: 1.000000 original loss: 4.694586 matching loss: 0.002332\n",
            "step 100, Validation accuracy: 0.944181\n",
            "get new weight for 0.94418055\n",
            "step 200, training accuracy: 0.990000 original loss: 4.730240 matching loss: 0.002616\n",
            "step 200, Validation accuracy: 0.943230\n",
            "step 300, training accuracy: 1.000000 original loss: 4.653631 matching loss: 0.002594\n",
            "step 300, Validation accuracy: 0.944497\n",
            "get new weight for 0.9444972\n",
            "step 400, training accuracy: 1.000000 original loss: 4.669597 matching loss: 0.002644\n",
            "step 400, Validation accuracy: 0.946318\n",
            "get new weight for 0.94631827\n",
            "step 500, training accuracy: 1.000000 original loss: 4.667923 matching loss: 0.002755\n",
            "step 500, Validation accuracy: 0.947743\n",
            "get new weight for 0.9477435\n",
            "step 600, training accuracy: 0.990000 original loss: 4.705011 matching loss: 0.003099\n",
            "step 600, Validation accuracy: 0.943705\n",
            "step 700, training accuracy: 0.990000 original loss: 4.675387 matching loss: 0.003265\n",
            "step 700, Validation accuracy: 0.942359\n",
            "step 800, training accuracy: 1.000000 original loss: 4.666185 matching loss: 0.003295\n",
            "step 800, Validation accuracy: 0.947506\n",
            "step 900, training accuracy: 1.000000 original loss: 4.661683 matching loss: 0.003464\n",
            "step 900, Validation accuracy: 0.944735\n",
            "step 1000, training accuracy: 0.990000 original loss: 4.693785 matching loss: 0.003393\n",
            "step 1000, Validation accuracy: 0.948298\n",
            "get new weight for 0.9482977\n",
            "step 1100, training accuracy: 0.990000 original loss: 4.707381 matching loss: 0.003476\n",
            "step 1100, Validation accuracy: 0.947743\n",
            "step 1200, training accuracy: 1.000000 original loss: 4.658304 matching loss: 0.003616\n",
            "step 1200, Validation accuracy: 0.943705\n",
            "step 1300, training accuracy: 1.000000 original loss: 4.665209 matching loss: 0.003742\n",
            "step 1300, Validation accuracy: 0.942676\n",
            "step 1400, training accuracy: 1.000000 original loss: 4.652251 matching loss: 0.003772\n",
            "step 1400, Validation accuracy: 0.950673\n",
            "get new weight for 0.950673\n",
            "step 1500, training accuracy: 1.000000 original loss: 4.669351 matching loss: 0.003967\n",
            "step 1500, Validation accuracy: 0.949723\n",
            "step 1600, training accuracy: 1.000000 original loss: 4.655420 matching loss: 0.004083\n",
            "step 1600, Validation accuracy: 0.947348\n",
            "step 1700, training accuracy: 1.000000 original loss: 4.659381 matching loss: 0.004233\n",
            "step 1700, Validation accuracy: 0.944497\n",
            "step 1800, training accuracy: 1.000000 original loss: 4.646303 matching loss: 0.004226\n",
            "step 1800, Validation accuracy: 0.947110\n",
            "step 1900, training accuracy: 1.000000 original loss: 4.644239 matching loss: 0.004298\n",
            "step 1900, Validation accuracy: 0.951069\n",
            "get new weight for 0.9510689\n",
            "step 1999, training accuracy: 1.000000 original loss: 4.639729 matching loss: 0.004286\n",
            "step 1999, Validation accuracy: 0.952494\n",
            "get new weight for 0.9524941\n",
            "gtsrb/gtsrb_weight.npy\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.981500\n",
            "GSC performance\n",
            "Inference accuracy: 0.747024\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.952494\n",
            "4-th joint optimization\n",
            "get_matching_loss\n",
            "v_train\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "step 0, training accuracy: 0.990000 original loss: 4.664704 matching loss: 0.001043\n",
            "step 0, Validation accuracy: 0.981500\n",
            "step 100, training accuracy: 0.980000 original loss: 4.678229 matching loss: 0.000656\n",
            "step 100, Validation accuracy: 0.980600\n",
            "step 200, training accuracy: 1.000000 original loss: 4.627763 matching loss: 0.000712\n",
            "step 200, Validation accuracy: 0.980800\n",
            "step 300, training accuracy: 0.980000 original loss: 4.649239 matching loss: 0.000778\n",
            "step 300, Validation accuracy: 0.982000\n",
            "get new weight for 0.982\n",
            "step 400, training accuracy: 0.980000 original loss: 4.662162 matching loss: 0.000830\n",
            "step 400, Validation accuracy: 0.980200\n",
            "step 500, training accuracy: 0.990000 original loss: 4.637336 matching loss: 0.000876\n",
            "step 500, Validation accuracy: 0.981800\n",
            "step 600, training accuracy: 0.990000 original loss: 4.642797 matching loss: 0.000912\n",
            "step 600, Validation accuracy: 0.980000\n",
            "step 700, training accuracy: 1.000000 original loss: 4.622142 matching loss: 0.000986\n",
            "step 700, Validation accuracy: 0.980500\n",
            "step 800, training accuracy: 0.990000 original loss: 4.645477 matching loss: 0.001021\n",
            "step 800, Validation accuracy: 0.983000\n",
            "get new weight for 0.983\n",
            "step 900, training accuracy: 0.980000 original loss: 4.696507 matching loss: 0.001076\n",
            "step 900, Validation accuracy: 0.982600\n",
            "step 1000, training accuracy: 0.990000 original loss: 4.646614 matching loss: 0.001132\n",
            "step 1000, Validation accuracy: 0.979900\n",
            "step 1100, training accuracy: 1.000000 original loss: 4.628605 matching loss: 0.001177\n",
            "step 1100, Validation accuracy: 0.982700\n",
            "step 1200, training accuracy: 0.980000 original loss: 4.671046 matching loss: 0.001223\n",
            "step 1200, Validation accuracy: 0.980600\n",
            "step 1300, training accuracy: 0.990000 original loss: 4.642287 matching loss: 0.001278\n",
            "step 1300, Validation accuracy: 0.981500\n",
            "step 1400, training accuracy: 1.000000 original loss: 4.632984 matching loss: 0.001300\n",
            "step 1400, Validation accuracy: 0.981200\n",
            "step 1500, training accuracy: 0.980000 original loss: 4.672543 matching loss: 0.001345\n",
            "step 1500, Validation accuracy: 0.982400\n",
            "step 1600, training accuracy: 0.990000 original loss: 4.695053 matching loss: 0.001392\n",
            "step 1600, Validation accuracy: 0.981700\n",
            "step 1700, training accuracy: 1.000000 original loss: 4.639431 matching loss: 0.001451\n",
            "step 1700, Validation accuracy: 0.983600\n",
            "get new weight for 0.9836\n",
            "step 1800, training accuracy: 0.990000 original loss: 4.630979 matching loss: 0.001486\n",
            "step 1800, Validation accuracy: 0.982300\n",
            "step 1900, training accuracy: 0.990000 original loss: 4.640591 matching loss: 0.001515\n",
            "step 1900, Validation accuracy: 0.982500\n",
            "step 1999, training accuracy: 1.000000 original loss: 4.629982 matching loss: 0.001593\n",
            "step 1999, Validation accuracy: 0.982000\n",
            "mnist/mnist_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.830000 original loss: 5.362066 matching loss: 0.000306\n",
            "step 0, Validation accuracy: 0.745298\n",
            "step 100, training accuracy: 0.900000 original loss: 5.207291 matching loss: 0.002422\n",
            "step 100, Validation accuracy: 0.750750\n",
            "get new weight for 0.75074965\n",
            "step 200, training accuracy: 0.870000 original loss: 5.304866 matching loss: 0.002978\n",
            "step 200, Validation accuracy: 0.750659\n",
            "step 300, training accuracy: 0.770000 original loss: 5.523200 matching loss: 0.003283\n",
            "step 300, Validation accuracy: 0.754294\n",
            "get new weight for 0.7542935\n",
            "step 400, training accuracy: 0.760000 original loss: 5.361016 matching loss: 0.003592\n",
            "step 400, Validation accuracy: 0.751658\n",
            "step 500, training accuracy: 0.800000 original loss: 5.451954 matching loss: 0.003678\n",
            "step 500, Validation accuracy: 0.757383\n",
            "get new weight for 0.757383\n",
            "step 600, training accuracy: 0.830000 original loss: 5.467713 matching loss: 0.003977\n",
            "step 600, Validation accuracy: 0.756111\n",
            "step 700, training accuracy: 0.750000 original loss: 5.467721 matching loss: 0.004201\n",
            "step 700, Validation accuracy: 0.755929\n",
            "step 800, training accuracy: 0.860000 original loss: 5.290040 matching loss: 0.004297\n",
            "step 800, Validation accuracy: 0.750477\n",
            "step 900, training accuracy: 0.780000 original loss: 5.396217 matching loss: 0.004558\n",
            "step 900, Validation accuracy: 0.750477\n",
            "step 1000, training accuracy: 0.750000 original loss: 5.545979 matching loss: 0.004575\n",
            "step 1000, Validation accuracy: 0.756565\n",
            "step 1100, training accuracy: 0.770000 original loss: 5.415591 matching loss: 0.004651\n",
            "step 1100, Validation accuracy: 0.759019\n",
            "get new weight for 0.7590186\n",
            "step 1200, training accuracy: 0.830000 original loss: 5.413908 matching loss: 0.004880\n",
            "step 1200, Validation accuracy: 0.757474\n",
            "step 1300, training accuracy: 0.860000 original loss: 5.247413 matching loss: 0.005161\n",
            "step 1300, Validation accuracy: 0.756020\n",
            "step 1400, training accuracy: 0.820000 original loss: 5.274265 matching loss: 0.005233\n",
            "step 1400, Validation accuracy: 0.755838\n",
            "step 1500, training accuracy: 0.830000 original loss: 5.240236 matching loss: 0.005443\n",
            "step 1500, Validation accuracy: 0.755657\n",
            "step 1600, training accuracy: 0.810000 original loss: 5.294521 matching loss: 0.005459\n",
            "step 1600, Validation accuracy: 0.757292\n",
            "step 1700, training accuracy: 0.780000 original loss: 5.444278 matching loss: 0.005538\n",
            "step 1700, Validation accuracy: 0.752567\n",
            "step 1800, training accuracy: 0.860000 original loss: 5.267606 matching loss: 0.005816\n",
            "step 1800, Validation accuracy: 0.755384\n",
            "step 1900, training accuracy: 0.720000 original loss: 5.545615 matching loss: 0.005847\n",
            "step 1900, Validation accuracy: 0.761381\n",
            "get new weight for 0.7613812\n",
            "step 1999, training accuracy: 0.690000 original loss: 5.501166 matching loss: 0.005933\n",
            "step 1999, Validation accuracy: 0.762017\n",
            "get new weight for 0.76201725\n",
            "gsc/gsc_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 1.000000 original loss: 4.819865 matching loss: 0.000242\n",
            "step 0, Validation accuracy: 0.936896\n",
            "step 100, training accuracy: 0.990000 original loss: 4.679351 matching loss: 0.002281\n",
            "step 100, Validation accuracy: 0.944735\n",
            "get new weight for 0.94473475\n",
            "step 200, training accuracy: 1.000000 original loss: 4.680757 matching loss: 0.002316\n",
            "step 200, Validation accuracy: 0.949010\n",
            "get new weight for 0.9490103\n",
            "step 300, training accuracy: 0.990000 original loss: 4.679230 matching loss: 0.002590\n",
            "step 300, Validation accuracy: 0.944814\n",
            "step 400, training accuracy: 1.000000 original loss: 4.665802 matching loss: 0.002661\n",
            "step 400, Validation accuracy: 0.946318\n",
            "step 500, training accuracy: 1.000000 original loss: 4.662208 matching loss: 0.002845\n",
            "step 500, Validation accuracy: 0.947902\n",
            "step 600, training accuracy: 1.000000 original loss: 4.680324 matching loss: 0.003148\n",
            "step 600, Validation accuracy: 0.948456\n",
            "step 700, training accuracy: 1.000000 original loss: 4.657913 matching loss: 0.003179\n",
            "step 700, Validation accuracy: 0.944260\n",
            "step 800, training accuracy: 0.990000 original loss: 4.675095 matching loss: 0.003159\n",
            "step 800, Validation accuracy: 0.948773\n",
            "step 900, training accuracy: 1.000000 original loss: 4.654327 matching loss: 0.003313\n",
            "step 900, Validation accuracy: 0.944814\n",
            "step 1000, training accuracy: 1.000000 original loss: 4.679404 matching loss: 0.003457\n",
            "step 1000, Validation accuracy: 0.948852\n",
            "step 1100, training accuracy: 1.000000 original loss: 4.649325 matching loss: 0.003296\n",
            "step 1100, Validation accuracy: 0.950198\n",
            "get new weight for 0.95019794\n",
            "step 1200, training accuracy: 1.000000 original loss: 4.648234 matching loss: 0.003775\n",
            "step 1200, Validation accuracy: 0.947823\n",
            "step 1300, training accuracy: 0.980000 original loss: 4.675687 matching loss: 0.003534\n",
            "step 1300, Validation accuracy: 0.951781\n",
            "get new weight for 0.95178145\n",
            "step 1400, training accuracy: 1.000000 original loss: 4.642422 matching loss: 0.003668\n",
            "step 1400, Validation accuracy: 0.949960\n",
            "step 1500, training accuracy: 1.000000 original loss: 4.645523 matching loss: 0.003708\n",
            "step 1500, Validation accuracy: 0.952336\n",
            "get new weight for 0.9523357\n",
            "step 1600, training accuracy: 1.000000 original loss: 4.671872 matching loss: 0.003961\n",
            "step 1600, Validation accuracy: 0.949485\n",
            "step 1700, training accuracy: 0.980000 original loss: 4.716474 matching loss: 0.003937\n",
            "step 1700, Validation accuracy: 0.952494\n",
            "get new weight for 0.9524941\n",
            "step 1800, training accuracy: 1.000000 original loss: 4.670702 matching loss: 0.004049\n",
            "step 1800, Validation accuracy: 0.953998\n",
            "get new weight for 0.9539984\n",
            "step 1900, training accuracy: 1.000000 original loss: 4.650944 matching loss: 0.004207\n",
            "step 1900, Validation accuracy: 0.952494\n",
            "step 1999, training accuracy: 1.000000 original loss: 4.661656 matching loss: 0.004115\n",
            "step 1999, Validation accuracy: 0.954949\n",
            "get new weight for 0.95494854\n",
            "gtsrb/gtsrb_weight.npy\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.980700\n",
            "GSC performance\n",
            "Inference accuracy: 0.745752\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.954949\n",
            "5-th joint optimization\n",
            "get_matching_loss\n",
            "v_train\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "step 0, training accuracy: 0.990000 original loss: 4.747910 matching loss: 0.001004\n",
            "step 0, Validation accuracy: 0.980700\n",
            "step 100, training accuracy: 0.990000 original loss: 4.648762 matching loss: 0.000611\n",
            "step 100, Validation accuracy: 0.982500\n",
            "get new weight for 0.9825\n",
            "step 200, training accuracy: 1.000000 original loss: 4.629889 matching loss: 0.000669\n",
            "step 200, Validation accuracy: 0.980900\n",
            "step 300, training accuracy: 0.990000 original loss: 4.667049 matching loss: 0.000717\n",
            "step 300, Validation accuracy: 0.981200\n",
            "step 400, training accuracy: 1.000000 original loss: 4.627352 matching loss: 0.000740\n",
            "step 400, Validation accuracy: 0.981300\n",
            "step 500, training accuracy: 1.000000 original loss: 4.646242 matching loss: 0.000788\n",
            "step 500, Validation accuracy: 0.983000\n",
            "get new weight for 0.983\n",
            "step 600, training accuracy: 0.990000 original loss: 4.652535 matching loss: 0.000824\n",
            "step 600, Validation accuracy: 0.981800\n",
            "step 700, training accuracy: 0.980000 original loss: 4.641505 matching loss: 0.000842\n",
            "step 700, Validation accuracy: 0.981200\n",
            "step 800, training accuracy: 1.000000 original loss: 4.626681 matching loss: 0.000871\n",
            "step 800, Validation accuracy: 0.983100\n",
            "get new weight for 0.9831\n",
            "step 900, training accuracy: 0.980000 original loss: 4.661203 matching loss: 0.000936\n",
            "step 900, Validation accuracy: 0.979700\n",
            "step 1000, training accuracy: 1.000000 original loss: 4.618778 matching loss: 0.000951\n",
            "step 1000, Validation accuracy: 0.982400\n",
            "step 1100, training accuracy: 1.000000 original loss: 4.633451 matching loss: 0.000987\n",
            "step 1100, Validation accuracy: 0.981100\n",
            "step 1200, training accuracy: 1.000000 original loss: 4.624827 matching loss: 0.001033\n",
            "step 1200, Validation accuracy: 0.981300\n",
            "step 1300, training accuracy: 0.980000 original loss: 4.675385 matching loss: 0.001063\n",
            "step 1300, Validation accuracy: 0.982300\n",
            "step 1400, training accuracy: 0.990000 original loss: 4.637496 matching loss: 0.001112\n",
            "step 1400, Validation accuracy: 0.980800\n",
            "step 1500, training accuracy: 1.000000 original loss: 4.622063 matching loss: 0.001153\n",
            "step 1500, Validation accuracy: 0.981500\n",
            "step 1600, training accuracy: 1.000000 original loss: 4.624792 matching loss: 0.001181\n",
            "step 1600, Validation accuracy: 0.983700\n",
            "get new weight for 0.9837\n",
            "step 1700, training accuracy: 1.000000 original loss: 4.628759 matching loss: 0.001218\n",
            "step 1700, Validation accuracy: 0.982500\n",
            "step 1800, training accuracy: 1.000000 original loss: 4.617472 matching loss: 0.001248\n",
            "step 1800, Validation accuracy: 0.982200\n",
            "step 1900, training accuracy: 1.000000 original loss: 4.623034 matching loss: 0.001299\n",
            "step 1900, Validation accuracy: 0.982700\n",
            "step 1999, training accuracy: 1.000000 original loss: 4.621316 matching loss: 0.001337\n",
            "step 1999, Validation accuracy: 0.983200\n",
            "mnist/mnist_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.740000 original loss: 5.780193 matching loss: 0.000221\n",
            "step 0, Validation accuracy: 0.743480\n",
            "step 100, training accuracy: 0.870000 original loss: 5.216653 matching loss: 0.002376\n",
            "step 100, Validation accuracy: 0.755020\n",
            "get new weight for 0.75502044\n",
            "step 200, training accuracy: 0.840000 original loss: 5.227337 matching loss: 0.003041\n",
            "step 200, Validation accuracy: 0.758019\n",
            "get new weight for 0.7580191\n",
            "step 300, training accuracy: 0.800000 original loss: 5.351404 matching loss: 0.003306\n",
            "step 300, Validation accuracy: 0.759109\n",
            "get new weight for 0.7591095\n",
            "step 400, training accuracy: 0.790000 original loss: 5.431792 matching loss: 0.003527\n",
            "step 400, Validation accuracy: 0.751840\n",
            "step 500, training accuracy: 0.850000 original loss: 5.315628 matching loss: 0.003611\n",
            "step 500, Validation accuracy: 0.754203\n",
            "step 600, training accuracy: 0.800000 original loss: 5.368417 matching loss: 0.004034\n",
            "step 600, Validation accuracy: 0.761290\n",
            "get new weight for 0.7612903\n",
            "step 700, training accuracy: 0.710000 original loss: 5.662770 matching loss: 0.004113\n",
            "step 700, Validation accuracy: 0.758655\n",
            "step 800, training accuracy: 0.820000 original loss: 5.363247 matching loss: 0.004197\n",
            "step 800, Validation accuracy: 0.759109\n",
            "step 900, training accuracy: 0.790000 original loss: 5.421443 matching loss: 0.004263\n",
            "step 900, Validation accuracy: 0.756929\n",
            "step 1000, training accuracy: 0.770000 original loss: 5.509564 matching loss: 0.004427\n",
            "step 1000, Validation accuracy: 0.756929\n",
            "step 1100, training accuracy: 0.810000 original loss: 5.355037 matching loss: 0.004630\n",
            "step 1100, Validation accuracy: 0.761290\n",
            "step 1200, training accuracy: 0.770000 original loss: 5.433651 matching loss: 0.004687\n",
            "step 1200, Validation accuracy: 0.757292\n",
            "step 1300, training accuracy: 0.850000 original loss: 5.174402 matching loss: 0.004673\n",
            "step 1300, Validation accuracy: 0.761109\n",
            "step 1400, training accuracy: 0.780000 original loss: 5.398584 matching loss: 0.004942\n",
            "step 1400, Validation accuracy: 0.758564\n",
            "step 1500, training accuracy: 0.790000 original loss: 5.415902 matching loss: 0.005106\n",
            "step 1500, Validation accuracy: 0.762381\n",
            "get new weight for 0.7623807\n",
            "step 1600, training accuracy: 0.820000 original loss: 5.353862 matching loss: 0.005414\n",
            "step 1600, Validation accuracy: 0.757837\n",
            "step 1700, training accuracy: 0.840000 original loss: 5.253329 matching loss: 0.005336\n",
            "step 1700, Validation accuracy: 0.761926\n",
            "step 1800, training accuracy: 0.850000 original loss: 5.207683 matching loss: 0.005451\n",
            "step 1800, Validation accuracy: 0.760109\n",
            "step 1900, training accuracy: 0.750000 original loss: 5.576038 matching loss: 0.005491\n",
            "step 1900, Validation accuracy: 0.757292\n",
            "step 1999, training accuracy: 0.800000 original loss: 5.264012 matching loss: 0.005632\n",
            "step 1999, Validation accuracy: 0.765379\n",
            "get new weight for 0.76537937\n",
            "gsc/gsc_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.990000 original loss: 4.787958 matching loss: 0.000239\n",
            "step 0, Validation accuracy: 0.944101\n",
            "step 100, training accuracy: 1.000000 original loss: 4.646308 matching loss: 0.002016\n",
            "step 100, Validation accuracy: 0.944893\n",
            "get new weight for 0.9448931\n",
            "step 200, training accuracy: 1.000000 original loss: 4.658394 matching loss: 0.002462\n",
            "step 200, Validation accuracy: 0.947348\n",
            "get new weight for 0.9473476\n",
            "step 300, training accuracy: 1.000000 original loss: 4.678638 matching loss: 0.002390\n",
            "step 300, Validation accuracy: 0.949089\n",
            "get new weight for 0.94908947\n",
            "step 400, training accuracy: 1.000000 original loss: 4.679596 matching loss: 0.002508\n",
            "step 400, Validation accuracy: 0.950040\n",
            "get new weight for 0.95003957\n",
            "step 500, training accuracy: 1.000000 original loss: 4.657410 matching loss: 0.002597\n",
            "step 500, Validation accuracy: 0.950198\n",
            "get new weight for 0.95019794\n",
            "step 600, training accuracy: 0.990000 original loss: 4.672052 matching loss: 0.002879\n",
            "step 600, Validation accuracy: 0.948298\n",
            "step 700, training accuracy: 1.000000 original loss: 4.665112 matching loss: 0.002993\n",
            "step 700, Validation accuracy: 0.949723\n",
            "step 800, training accuracy: 1.000000 original loss: 4.673697 matching loss: 0.002890\n",
            "step 800, Validation accuracy: 0.950277\n",
            "get new weight for 0.9502771\n",
            "step 900, training accuracy: 1.000000 original loss: 4.657960 matching loss: 0.002938\n",
            "step 900, Validation accuracy: 0.951386\n",
            "get new weight for 0.9513856\n",
            "step 1000, training accuracy: 0.990000 original loss: 4.700909 matching loss: 0.003130\n",
            "step 1000, Validation accuracy: 0.947427\n",
            "step 1100, training accuracy: 0.990000 original loss: 4.651677 matching loss: 0.003265\n",
            "step 1100, Validation accuracy: 0.951940\n",
            "get new weight for 0.9519398\n",
            "step 1200, training accuracy: 1.000000 original loss: 4.640789 matching loss: 0.003456\n",
            "step 1200, Validation accuracy: 0.951623\n",
            "step 1300, training accuracy: 1.000000 original loss: 4.655241 matching loss: 0.003311\n",
            "step 1300, Validation accuracy: 0.951544\n",
            "step 1400, training accuracy: 1.000000 original loss: 4.663991 matching loss: 0.003355\n",
            "step 1400, Validation accuracy: 0.953048\n",
            "get new weight for 0.9530483\n",
            "step 1500, training accuracy: 1.000000 original loss: 4.643698 matching loss: 0.003397\n",
            "step 1500, Validation accuracy: 0.952098\n",
            "step 1600, training accuracy: 1.000000 original loss: 4.644141 matching loss: 0.003503\n",
            "step 1600, Validation accuracy: 0.952257\n",
            "step 1700, training accuracy: 1.000000 original loss: 4.652708 matching loss: 0.003713\n",
            "step 1700, Validation accuracy: 0.950752\n",
            "step 1800, training accuracy: 1.000000 original loss: 4.658604 matching loss: 0.003628\n",
            "step 1800, Validation accuracy: 0.950435\n",
            "step 1900, training accuracy: 1.000000 original loss: 4.641829 matching loss: 0.003747\n",
            "step 1900, Validation accuracy: 0.952732\n",
            "step 1999, training accuracy: 0.990000 original loss: 4.669641 matching loss: 0.004058\n",
            "step 1999, Validation accuracy: 0.953365\n",
            "get new weight for 0.953365\n",
            "gtsrb/gtsrb_weight.npy\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.981200\n",
            "GSC performance\n",
            "Inference accuracy: 0.755111\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.953365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnkoV02ENwf0",
        "outputId": "73bd288c-ff97-4ec7-b6b2-549ac9d5733b"
      },
      "source": [
        "duration = time.time() - beginning \n",
        "print(\"Time elapsed\", duration)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed 375.1979355812073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JSPuFQm_-nI",
        "outputId": "514d95b5-ec1e-4827-d402-226c83d45da9"
      },
      "source": [
        "# 6-10\n",
        "!bash ./joint_optimization.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.980200\n",
            "GSC performance\n",
            "Inference accuracy: 0.749932\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.951861\n",
            "2-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.982200\n",
            "GSC performance\n",
            "Inference accuracy: 0.754384\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.950990\n",
            "3-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.981500\n",
            "GSC performance\n",
            "Inference accuracy: 0.760927\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.953761\n",
            "4-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.981700\n",
            "GSC performance\n",
            "Inference accuracy: 0.768651\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.953682\n",
            "5-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.983300\n",
            "GSC performance\n",
            "Inference accuracy: 0.768469\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.953919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed1EFKJDCBwy",
        "outputId": "788040d6-bc30-496d-afd7-e45476dc3c54"
      },
      "source": [
        "# 11-15\n",
        "!bash ./joint_optimization.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.983500\n",
            "GSC performance\n",
            "Inference accuracy: 0.771558\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.955265\n",
            "2-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.984000\n",
            "GSC performance\n",
            "Inference accuracy: 0.773194\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.955661\n",
            "3-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.983400\n",
            "GSC performance\n",
            "Inference accuracy: 0.772013\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.956690\n",
            "4-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.984000\n",
            "GSC performance\n",
            "Inference accuracy: 0.773921\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.957799\n",
            "5-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.983700\n",
            "GSC performance\n",
            "Inference accuracy: 0.781645\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.959066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUOvy3_ICB8y",
        "outputId": "b4aef0a0-cdac-458e-897a-41db6887af9d"
      },
      "source": [
        "# 16-20\n",
        "!bash ./joint_optimization.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.983100\n",
            "GSC performance\n",
            "Inference accuracy: 0.783189\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.958116\n",
            "2-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.985100\n",
            "GSC performance\n",
            "Inference accuracy: 0.780736\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.959541\n",
            "3-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.984100\n",
            "GSC performance\n",
            "Inference accuracy: 0.778464\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.958987\n",
            "4-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.985200\n",
            "GSC performance\n",
            "Inference accuracy: 0.786552\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.959620\n",
            "5-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.985100\n",
            "GSC performance\n",
            "Inference accuracy: 0.784189\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.960491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpWh0Bg1CCCY",
        "outputId": "077189f4-bf60-4702-c7fa-d879af9a905e"
      },
      "source": [
        "!python weight_virtualization.py -mode=e -vnn_name=mnist"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.985100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTUiFeSDCCGP",
        "outputId": "c734ef23-77e2-4c48-bf4b-eeb4c926bd66"
      },
      "source": [
        "!python weight_virtualization.py -mode=e -vnn_name=gsc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inference accuracy: 0.784189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnkxMnBEIqlM",
        "outputId": "4ff9d37e-6918-4a97-c457-7a7da4de997b"
      },
      "source": [
        "!python weight_virtualization.py -mode=e -vnn_name=gtsrb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inference accuracy: 0.960491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84s3RUtzIr57",
        "outputId": "7413c05e-8375-4ef6-9f7e-743a82a1c10d"
      },
      "source": [
        "!python in-memory_execute.py | tee -a in_memory_execution_three_nets_with_mem.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MEMORY USAGE (MB): 269.81640625\n",
            "MEMORY USAGE (MB): 269.81640625\n",
            "MEMORY USAGE (MB): 270.0703125\n",
            "MEMORY USAGE (MB): 270.0703125\n",
            "MEMORY USAGE (MB): 270.0703125\n",
            "virtual_weight address: 139745037385728\n",
            "init virtual_weight 7.402 ms\n",
            "[VNN 0][gsc] init page table 4.485 ms\n",
            "[VNN 1][gtsrb] init page table 4.306 ms\n",
            "[VNN 2][mnist] init page table 3.877 ms\n",
            "tf.global_variables_initializer 748.745 ms\n",
            "MEMORY USAGE (MB): 998.06640625\n",
            "MEMORY USAGE (MB): 998.06640625\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "MEMORY USAGE (MB): 1465.4140625\n",
            "[Executing] mnist\n",
            "weights load time : 0.350 ms\n",
            "DNN execution time: 2385.304 ms\n",
            "Inference accuracy: 0.981200\n",
            "MEMORY USAGE (MB): 2959.31640625\n",
            "MEMORY USAGE (MB): 2959.31640625\n",
            "MEMORY USAGE (MB): 3628.859375\n",
            "[Executing] gsc\n",
            "weights load time : 0.343 ms\n",
            "DNN execution time: 495.673 ms\n",
            "Inference accuracy: 0.755111\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "[Executing] mnist\n",
            "weights load time : 0.289 ms\n",
            "DNN execution time: 26.045 ms\n",
            "Inference accuracy: 0.981200\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "[Executing] gsc\n",
            "weights load time : 0.190 ms\n",
            "DNN execution time: 34.370 ms\n",
            "Inference accuracy: 0.755111\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "[Executing] gsc\n",
            "weights load time : 0.168 ms\n",
            "DNN execution time: 34.456 ms\n",
            "Inference accuracy: 0.755111\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "[Executing] mnist\n",
            "weights load time : 0.192 ms\n",
            "DNN execution time: 25.697 ms\n",
            "Inference accuracy: 0.981200\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "[Executing] mnist\n",
            "weights load time : 0.216 ms\n",
            "DNN execution time: 25.676 ms\n",
            "Inference accuracy: 0.981200\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "[Executing] gsc\n",
            "weights load time : 0.239 ms\n",
            "DNN execution time: 35.464 ms\n",
            "Inference accuracy: 0.755111\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "[Executing] gsc\n",
            "weights load time : 0.162 ms\n",
            "DNN execution time: 34.100 ms\n",
            "Inference accuracy: 0.755111\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "[Executing] mnist\n",
            "weights load time : 0.230 ms\n",
            "DNN execution time: 26.394 ms\n",
            "Inference accuracy: 0.981200\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "[Executing] gsc\n",
            "weights load time : 0.157 ms\n",
            "DNN execution time: 34.288 ms\n",
            "Inference accuracy: 0.755111\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "[Executing] gsc\n",
            "weights load time : 0.157 ms\n",
            "DNN execution time: 33.768 ms\n",
            "Inference accuracy: 0.755111\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "[Executing] mnist\n",
            "weights load time : 0.186 ms\n",
            "DNN execution time: 25.738 ms\n",
            "Inference accuracy: 0.981200\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 3795.2578125\n",
            "MEMORY USAGE (MB): 4199.25\n",
            "[Executing] gtsrb\n",
            "weights load time : 0.257 ms\n",
            "DNN execution time: 525.034 ms\n",
            "Inference accuracy: 0.953365\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] mnist\n",
            "weights load time : 0.186 ms\n",
            "DNN execution time: 26.062 ms\n",
            "Inference accuracy: 0.981200\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] mnist\n",
            "weights load time : 0.174 ms\n",
            "DNN execution time: 25.715 ms\n",
            "Inference accuracy: 0.981200\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] gtsrb\n",
            "weights load time : 0.211 ms\n",
            "DNN execution time: 61.065 ms\n",
            "Inference accuracy: 0.953365\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] mnist\n",
            "weights load time : 0.197 ms\n",
            "DNN execution time: 25.677 ms\n",
            "Inference accuracy: 0.981200\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] gsc\n",
            "weights load time : 0.163 ms\n",
            "DNN execution time: 34.702 ms\n",
            "Inference accuracy: 0.755111\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] mnist\n",
            "weights load time : 0.220 ms\n",
            "DNN execution time: 25.687 ms\n",
            "Inference accuracy: 0.981200\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] mnist\n",
            "weights load time : 0.186 ms\n",
            "DNN execution time: 25.663 ms\n",
            "Inference accuracy: 0.981200\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] gsc\n",
            "weights load time : 0.157 ms\n",
            "DNN execution time: 34.360 ms\n",
            "Inference accuracy: 0.755111\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] gsc\n",
            "weights load time : 0.166 ms\n",
            "DNN execution time: 33.527 ms\n",
            "Inference accuracy: 0.755111\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] gsc\n",
            "weights load time : 0.201 ms\n",
            "DNN execution time: 35.719 ms\n",
            "Inference accuracy: 0.755111\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] gtsrb\n",
            "weights load time : 0.181 ms\n",
            "DNN execution time: 60.026 ms\n",
            "Inference accuracy: 0.953365\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] mnist\n",
            "weights load time : 0.200 ms\n",
            "DNN execution time: 26.080 ms\n",
            "Inference accuracy: 0.981200\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] gtsrb\n",
            "weights load time : 0.181 ms\n",
            "DNN execution time: 60.999 ms\n",
            "Inference accuracy: 0.953365\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] mnist\n",
            "weights load time : 0.223 ms\n",
            "DNN execution time: 26.265 ms\n",
            "Inference accuracy: 0.981200\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] mnist\n",
            "weights load time : 0.185 ms\n",
            "DNN execution time: 25.455 ms\n",
            "Inference accuracy: 0.981200\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "[Executing] gsc\n",
            "weights load time : 0.192 ms\n",
            "DNN execution time: 34.476 ms\n",
            "Inference accuracy: 0.755111\n",
            "MEMORY USAGE (MB): 4254.7890625\n",
            "total weights load time : 6.158 ms\n",
            "total DNN execution time: 4303.486 ms\n",
            "MEMORY USAGE (MB): 4254.96484375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDqG5gmwL0JD",
        "outputId": "8c2fdf06-d07d-4ba0-dea0-e4ead62333bc"
      },
      "source": [
        "!python baseline_execute.py | tee -a baseline_execution_three_nets.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Executing] gtsrb\n",
            "weights load time : 51.129 ms\n",
            "DNN execution time: 3153.365 ms\n",
            "Inference accuracy: 0.928029\n",
            "[Executing] gtsrb\n",
            "weights load time : 47.046 ms\n",
            "DNN execution time: 86.768 ms\n",
            "Inference accuracy: 0.928029\n",
            "[Executing] gsc\n",
            "weights load time : 52.399 ms\n",
            "DNN execution time: 397.472 ms\n",
            "Inference accuracy: 0.694684\n",
            "[Executing] gtsrb\n",
            "weights load time : 53.808 ms\n",
            "DNN execution time: 92.932 ms\n",
            "Inference accuracy: 0.928029\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "[Executing] mnist\n",
            "weights load time : 62.085 ms\n",
            "DNN execution time: 411.965 ms\n",
            "Inference accuracy: 0.980800\n",
            "[Executing] gsc\n",
            "weights load time : 51.925 ms\n",
            "DNN execution time: 60.894 ms\n",
            "Inference accuracy: 0.694684\n",
            "[Executing] gsc\n",
            "weights load time : 57.257 ms\n",
            "DNN execution time: 61.072 ms\n",
            "Inference accuracy: 0.694684\n",
            "[Executing] gsc\n",
            "weights load time : 54.235 ms\n",
            "DNN execution time: 62.108 ms\n",
            "Inference accuracy: 0.694684\n",
            "[Executing] gtsrb\n",
            "weights load time : 54.529 ms\n",
            "DNN execution time: 90.077 ms\n",
            "Inference accuracy: 0.928029\n",
            "[Executing] mnist\n",
            "weights load time : 68.591 ms\n",
            "DNN execution time: 60.935 ms\n",
            "Inference accuracy: 0.980800\n",
            "[Executing] gtsrb\n",
            "weights load time : 54.276 ms\n",
            "DNN execution time: 98.737 ms\n",
            "Inference accuracy: 0.928029\n",
            "[Executing] gtsrb\n",
            "weights load time : 57.991 ms\n",
            "DNN execution time: 100.269 ms\n",
            "Inference accuracy: 0.928029\n",
            "[Executing] gtsrb\n",
            "weights load time : 57.291 ms\n",
            "DNN execution time: 107.524 ms\n",
            "Inference accuracy: 0.928029\n",
            "[Executing] gtsrb\n",
            "weights load time : 56.615 ms\n",
            "DNN execution time: 101.227 ms\n",
            "Inference accuracy: 0.928029\n",
            "[Executing] mnist\n",
            "weights load time : 69.931 ms\n",
            "DNN execution time: 64.926 ms\n",
            "Inference accuracy: 0.980800\n",
            "[Executing] gtsrb\n",
            "weights load time : 56.346 ms\n",
            "DNN execution time: 103.121 ms\n",
            "Inference accuracy: 0.928029\n",
            "[Executing] mnist\n",
            "weights load time : 65.058 ms\n",
            "DNN execution time: 63.183 ms\n",
            "Inference accuracy: 0.980800\n",
            "[Executing] mnist\n",
            "weights load time : 68.994 ms\n",
            "DNN execution time: 64.803 ms\n",
            "Inference accuracy: 0.980800\n",
            "[Executing] mnist\n",
            "weights load time : 67.733 ms\n",
            "DNN execution time: 63.331 ms\n",
            "Inference accuracy: 0.980800\n",
            "[Executing] gtsrb\n",
            "weights load time : 54.959 ms\n",
            "DNN execution time: 101.929 ms\n",
            "Inference accuracy: 0.928029\n",
            "[Executing] gsc\n",
            "weights load time : 59.451 ms\n",
            "DNN execution time: 73.467 ms\n",
            "Inference accuracy: 0.694684\n",
            "[Executing] gtsrb\n",
            "weights load time : 56.453 ms\n",
            "DNN execution time: 99.975 ms\n",
            "Inference accuracy: 0.928029\n",
            "[Executing] gtsrb\n",
            "weights load time : 57.472 ms\n",
            "DNN execution time: 99.036 ms\n",
            "Inference accuracy: 0.928029\n",
            "[Executing] mnist\n",
            "weights load time : 68.712 ms\n",
            "DNN execution time: 62.900 ms\n",
            "Inference accuracy: 0.980800\n",
            "[Executing] gtsrb\n",
            "weights load time : 55.205 ms\n",
            "DNN execution time: 98.074 ms\n",
            "Inference accuracy: 0.928029\n",
            "[Executing] mnist\n",
            "weights load time : 69.088 ms\n",
            "DNN execution time: 61.556 ms\n",
            "Inference accuracy: 0.980800\n",
            "[Executing] gsc\n",
            "weights load time : 57.788 ms\n",
            "DNN execution time: 68.967 ms\n",
            "Inference accuracy: 0.694684\n",
            "[Executing] gtsrb\n",
            "weights load time : 55.195 ms\n",
            "DNN execution time: 101.368 ms\n",
            "Inference accuracy: 0.928029\n",
            "[Executing] gsc\n",
            "weights load time : 57.303 ms\n",
            "DNN execution time: 71.373 ms\n",
            "Inference accuracy: 0.694684\n",
            "[Executing] mnist\n",
            "weights load time : 66.459 ms\n",
            "DNN execution time: 62.633 ms\n",
            "Inference accuracy: 0.980800\n",
            "total weights load time : 1765.324 ms\n",
            "total DNN execution time: 6145.989 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve_KaBfYMQK9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}