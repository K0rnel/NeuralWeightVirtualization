{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TenNets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pq4XZ_IANwN2",
        "outputId": "38b45375-b249-4e1a-d2fc-ef78690d9b37"
      },
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow-gpu==1.13.1\n",
        "\n",
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.16.4"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.0:\n",
            "  Successfully uninstalled tensorflow-2.4.0\n",
            "Collecting tensorflow-gpu==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |████████████████████████████████| 345.2MB 17kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 54.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.19.4)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 62.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (51.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (1.0.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.4.0)\n",
            "Installing collected packages: keras-applications, tensorboard, mock, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1\n",
            "Uninstalling numpy-1.19.4:\n",
            "  Successfully uninstalled numpy-1.19.4\n",
            "Collecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 214kB/s \n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "Successfully installed numpy-1.16.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqhh07JKOPQS",
        "outputId": "c40975e4-eb4c-4314-9df8-1b00d1539c42"
      },
      "source": [
        "!git clone https://github.com/K0rnel/NeuralWeightVirtualization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'NeuralWeightVirtualization' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whmtzJYhOZGf",
        "outputId": "09b65ea8-ddaf-44a6-cb1a-a8c41362ad9e"
      },
      "source": [
        "%cd NeuralWeightVirtualization"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NeuralWeightVirtualization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia8tt4ZjOZZ3",
        "outputId": "1325322b-6dd8-4e22-d035-05ed2e9fabe0"
      },
      "source": [
        "!sh download_dataset.sh"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/9] Downloading CIFAR10 dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2344      0 --:--:-- --:--:-- --:--:--  2344\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  234M    0  234M    0     0  50.1M      0 --:--:--  0:00:04 --:--:-- 57.8M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    417      0 --:--:-- --:--:-- --:--:--   417\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100  781k  100  781k    0     0   637k      0  0:00:01  0:00:01 --:--:--  637k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2487      0 --:--:-- --:--:-- --:--:--  2487\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1171M    0 1171M    0     0   115M      0 --:--:--  0:00:10 --:--:--  128M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2071      0 --:--:-- --:--:-- --:--:--  2071\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 3906k    0 3906k    0     0  8510k      0 --:--:-- --:--:-- --:--:-- 8510k\n",
            "\n",
            "[2/9] Downloading Google Speech Command V2 dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    221      0 --:--:--  0:00:01 --:--:--   221\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 66.5M    0 66.5M    0     0  29.8M      0 --:--:--  0:00:02 --:--:--  440M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    557      0 --:--:-- --:--:-- --:--:--   557\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 3009k    0 3009k    0     0  3201k      0 --:--:-- --:--:-- --:--:-- 73.0M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2443      0 --:--:-- --:--:-- --:--:--  2457\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  513M    0  513M    0     0   113M      0 --:--:--  0:00:04 --:--:--  136M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    403      0 --:--:--  0:00:01 --:--:--   403\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 22.6M    0 22.6M    0     0  17.1M      0 --:--:--  0:00:01 --:--:-- 17.1M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    514      0 --:--:-- --:--:-- --:--:--   513\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 60.3M    0 60.3M    0     0  50.2M      0 --:--:--  0:00:01 --:--:-- 50.2M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    475      0 --:--:-- --:--:-- --:--:--   474\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 2729k    0 2729k    0     0  2218k      0 --:--:--  0:00:01 --:--:-- 29.2M\n",
            "\n",
            "[3/9] Downloading GTSRB dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    334      0 --:--:--  0:00:01 --:--:--   334\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 98.6M    0 98.6M    0     0  33.5M      0 --:--:--  0:00:02 --:--:-- 97.0M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    402      0 --:--:--  0:00:01 --:--:--   402\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 4243k    0 4243k    0     0  3469k      0 --:--:--  0:00:01 --:--:-- 3469k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   3777      0 --:--:-- --:--:-- --:--:--  3777\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  306M    0  306M    0     0  79.9M      0 --:--:--  0:00:03 --:--:-- 89.3M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    495      0 --:--:-- --:--:-- --:--:--   494\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 12.8M    0 12.8M    0     0  11.9M      0 --:--:--  0:00:01 --:--:-- 11.9M\n",
            "\n",
            "[4/9] Downloading SVHN dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2331      0 --:--:-- --:--:-- --:--:--  2331\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  305M    0  305M    0     0  87.0M      0 --:--:--  0:00:03 --:--:-- 96.6M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   1255      0 --:--:-- --:--:-- --:--:--  1255\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1017k  100 1017k    0     0  1524k      0 --:--:-- --:--:-- --:--:-- 1524k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   4080      0 --:--:-- --:--:-- --:--:--  4080\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  772M    0  772M    0     0  72.2M      0 --:--:--  0:00:10 --:--:-- 85.9M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    531      0 --:--:-- --:--:-- --:--:--   531\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 2575k    0 2575k    0     0  2464k      0 --:--:--  0:00:01 --:--:-- 2464k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    327      0 --:--:--  0:00:01 --:--:--   327\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 85.8M    0 85.8M    0     0  51.5M      0 --:--:--  0:00:01 --:--:-- 51.5M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   1152      0 --:--:-- --:--:-- --:--:--  1155\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  286k  100  286k    0     0   493k      0 --:--:-- --:--:-- --:--:-- 8577k\n",
            "\n",
            "[5/9] Downloading US8K dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   1251      0 --:--:-- --:--:-- --:--:--  1255\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  202M    0  202M    0     0  45.4M      0 --:--:--  0:00:04 --:--:-- 54.4M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    793      0 --:--:-- --:--:-- --:--:--   792\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  422k  100  422k    0     0   584k      0 --:--:-- --:--:-- --:--:--  584k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2443      0 --:--:-- --:--:-- --:--:--  2443\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1826M    0 1826M    0     0  56.5M      0 --:--:--  0:00:32 --:--:-- 34.3M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    418      0 --:--:-- --:--:-- --:--:--   417\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 3801k    0 3801k    0     0  3186k      0 --:--:--  0:00:01 --:--:-- 3186k\n",
            "\n",
            "[6/9] Downloading FMNIST dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    195      0 --:--:--  0:00:02 --:--:--   195\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "100 59.8M    0 59.8M    0     0  24.2M      0 --:--:--  0:00:02 --:--:-- 24.2M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2756      0 --:--:-- --:--:-- --:--:--  2756\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  390k  100  390k    0     0  1159k      0 --:--:-- --:--:-- --:--:-- 1159k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   4636      0 --:--:-- --:--:-- --:--:--  4636\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  358M    0  358M    0     0  84.2M      0 --:--:--  0:00:04 --:--:-- 94.5M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    434      0 --:--:-- --:--:-- --:--:--   434\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 2343k    0 2343k    0     0  2022k      0 --:--:--  0:00:01 --:--:-- 2022k\n",
            "\n",
            "[7/9] Downloading HHAR dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    393      0 --:--:--  0:00:01 --:--:--   393\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 21.8M    0 21.8M    0     0  16.7M      0 --:--:--  0:00:01 --:--:-- 16.7M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2873      0 --:--:-- --:--:-- --:--:--  2873\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 57392  100 57392    0     0   184k      0 --:--:-- --:--:-- --:--:--  184k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   4387      0 --:--:-- --:--:-- --:--:--  4387\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 2180M    0 2180M    0     0  49.2M      0 --:--:--  0:00:44 --:--:-- 28.6M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    529      0 --:--:-- --:--:-- --:--:--   529\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 5582k    0 5582k    0     0  5604k      0 --:--:-- --:--:-- --:--:-- 5604k\n",
            "\n",
            "[8/9] Downloading ESC10 dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    305      0 --:--:--  0:00:01 --:--:--   305\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 26.3M    0 26.3M    0     0  15.3M      0 --:--:--  0:00:01 --:--:--  190M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2914      0 --:--:-- --:--:-- --:--:--  2914\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 28208  100 28208    0     0  68968      0 --:--:-- --:--:-- --:--:-- 68968\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2181      0 --:--:-- --:--:-- --:--:--  2193\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  237M    0  237M    0     0  51.2M      0 --:--:--  0:00:04 --:--:-- 61.6M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   3344      0 --:--:-- --:--:-- --:--:--  3344\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  246k  100  246k    0     0   831k      0 --:--:-- --:--:-- --:--:--  831k\n",
            "\n",
            "[9/9] Downloading OBS dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    224      0 --:--:--  0:00:01 --:--:--   224\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 78.4M    0 78.4M    0     0  33.5M      0 --:--:--  0:00:02 --:--:-- 33.5M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2170      0 --:--:-- --:--:-- --:--:--  2170\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 91488  100 91488    0     0   240k      0 --:--:-- --:--:-- --:--:--  240k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   4975      0 --:--:-- --:--:-- --:--:--  4975\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  156M    0  156M    0     0   138M      0 --:--:--  0:00:01 --:--:--  195M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   1130      0 --:--:-- --:--:-- --:--:--  1130\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  178k  100  178k    0     0   323k      0 --:--:-- --:--:-- --:--:--  323k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0    190      0 --:--:--  0:00:02 --:--:--   190\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "100 78.4M    0 78.4M    0     0  30.6M      0 --:--:--  0:00:02 --:--:--  364M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   3961      0 --:--:-- --:--:-- --:--:--  3961\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 91488  100 91488    0     0   235k      0 --:--:-- --:--:-- --:--:-- 2791k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f5k0IbPOZhG",
        "outputId": "e51dd8c3-4e57-4daa-97dc-28014b458112"
      },
      "source": [
        "!python weight_virtualization.py -mode=a -network_path=mnist | tee -a mnist_matching_ten_nets.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init new weight pages\n",
            "add_vnn\n",
            "mnist/mnist_network_weight.npy\n",
            "compute_fisher\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "do_compute_fisher\n",
            "sample num:    0, data_idx: 35877\n",
            "sample num:    1, data_idx: 37866\n",
            "sample num:    2, data_idx: 24052\n",
            "sample num:    3, data_idx: 23020\n",
            "sample num:    4, data_idx: 41672\n",
            "sample num:    5, data_idx: 46875\n",
            "sample num:    6, data_idx: 33878\n",
            "sample num:    7, data_idx: 35174\n",
            "sample num:    8, data_idx:  5480\n",
            "sample num:    9, data_idx:  5614\n",
            "sample num:   10, data_idx: 36435\n",
            "sample num:   11, data_idx: 10468\n",
            "sample num:   12, data_idx: 40681\n",
            "sample num:   13, data_idx: 21030\n",
            "sample num:   14, data_idx: 23575\n",
            "sample num:   15, data_idx: 50513\n",
            "sample num:   16, data_idx: 44149\n",
            "sample num:   17, data_idx: 38713\n",
            "sample num:   18, data_idx:  4992\n",
            "sample num:   19, data_idx: 41966\n",
            "sample num:   20, data_idx: 23432\n",
            "sample num:   21, data_idx: 44455\n",
            "sample num:   22, data_idx:  5574\n",
            "sample num:   23, data_idx: 34996\n",
            "sample num:   24, data_idx: 37478\n",
            "sample num:   25, data_idx: 46761\n",
            "sample num:   26, data_idx:  5279\n",
            "sample num:   27, data_idx: 33866\n",
            "sample num:   28, data_idx:  1022\n",
            "sample num:   29, data_idx:  6939\n",
            "sample num:   30, data_idx: 16654\n",
            "sample num:   31, data_idx: 23684\n",
            "sample num:   32, data_idx: 50795\n",
            "sample num:   33, data_idx: 36560\n",
            "sample num:   34, data_idx: 21431\n",
            "sample num:   35, data_idx: 15968\n",
            "sample num:   36, data_idx: 10792\n",
            "sample num:   37, data_idx: 22101\n",
            "sample num:   38, data_idx: 31031\n",
            "sample num:   39, data_idx: 23189\n",
            "sample num:   40, data_idx: 20838\n",
            "sample num:   41, data_idx:  1417\n",
            "sample num:   42, data_idx: 47147\n",
            "sample num:   43, data_idx:  5773\n",
            "sample num:   44, data_idx: 15444\n",
            "sample num:   45, data_idx: 18971\n",
            "sample num:   46, data_idx: 47973\n",
            "sample num:   47, data_idx: 47110\n",
            "sample num:   48, data_idx: 36977\n",
            "sample num:   49, data_idx: 17467\n",
            "sample num:   50, data_idx: 44360\n",
            "sample num:   51, data_idx: 36374\n",
            "sample num:   52, data_idx:  7696\n",
            "sample num:   53, data_idx: 16558\n",
            "sample num:   54, data_idx: 46502\n",
            "sample num:   55, data_idx: 27991\n",
            "sample num:   56, data_idx: 12265\n",
            "sample num:   57, data_idx:  6998\n",
            "sample num:   58, data_idx: 28058\n",
            "sample num:   59, data_idx: 38212\n",
            "sample num:   60, data_idx:   467\n",
            "sample num:   61, data_idx: 44092\n",
            "sample num:   62, data_idx: 35077\n",
            "sample num:   63, data_idx:  2081\n",
            "sample num:   64, data_idx:  6695\n",
            "sample num:   65, data_idx: 15704\n",
            "sample num:   66, data_idx:  7590\n",
            "sample num:   67, data_idx: 16991\n",
            "sample num:   68, data_idx:  6071\n",
            "sample num:   69, data_idx: 43063\n",
            "sample num:   70, data_idx: 39933\n",
            "sample num:   71, data_idx: 34026\n",
            "sample num:   72, data_idx: 46287\n",
            "sample num:   73, data_idx: 44778\n",
            "sample num:   74, data_idx: 45005\n",
            "sample num:   75, data_idx: 29639\n",
            "sample num:   76, data_idx:  6433\n",
            "sample num:   77, data_idx: 41248\n",
            "sample num:   78, data_idx: 24591\n",
            "sample num:   79, data_idx: 19535\n",
            "sample num:   80, data_idx: 35929\n",
            "sample num:   81, data_idx: 43791\n",
            "sample num:   82, data_idx: 33904\n",
            "sample num:   83, data_idx: 19569\n",
            "sample num:   84, data_idx: 18908\n",
            "sample num:   85, data_idx: 37684\n",
            "sample num:   86, data_idx: 10341\n",
            "sample num:   87, data_idx: 37816\n",
            "sample num:   88, data_idx: 37234\n",
            "sample num:   89, data_idx: 20286\n",
            "sample num:   90, data_idx: 12625\n",
            "sample num:   91, data_idx:  5152\n",
            "sample num:   92, data_idx: 24114\n",
            "sample num:   93, data_idx:  8669\n",
            "sample num:   94, data_idx: 44220\n",
            "sample num:   95, data_idx: 29765\n",
            "sample num:   96, data_idx: 53546\n",
            "sample num:   97, data_idx: 39671\n",
            "sample num:   98, data_idx: 13791\n",
            "sample num:   99, data_idx: 16587\n",
            "mnist/mnist_network_fisher.npy\n",
            "[calculate_cost]\n",
            "toal_cost: 0.0\n",
            "458 pages allocated for 45706 weights\n",
            "total_network_cost: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rD_ehpaOZk3",
        "outputId": "4719e019-cf4c-4ced-a696-ec682c7b7375"
      },
      "source": [
        "!python weight_virtualization.py -mode=a -network_path=gsc | tee -a gsc_matching_ten_nets.txt\n",
        "!python weight_virtualization.py -mode=a -network_path=gtsrb | tee -a gtsrb_matching_ten_nets.txt\n",
        "!python weight_virtualization.py -mode=a -network_path=cifar10 | tee -a cifar10_matching_ten_nets.txt\n",
        "!python weight_virtualization.py -mode=a -network_path=svhn | tee -a svhn_matching_ten_nets.txt\n",
        "\n",
        "!python weight_virtualization.py -mode=a -network_path=fmnist | tee -a fmnist_matching_ten_nets.txt\n",
        "!python weight_virtualization.py -mode=a -network_path=us8k | tee -a us8k_matching_ten_nets.txt\n",
        "!python weight_virtualization.py -mode=a -network_path=hhar | tee -a hhar_matching_ten_nets.txt\n",
        "!python weight_virtualization.py -mode=a -network_path=esc10 | tee -a esc10_matching_ten_nets.txt\n",
        "!python weight_virtualization.py -mode=a -network_path=obs | tee -a obs_matching_ten_nets.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "add_vnn\n",
            "gsc/gsc_network_weight.npy\n",
            "compute_fisher\n",
            "do_compute_fisher\n",
            "sample num:    0, data_idx: 58535\n",
            "sample num:    1, data_idx: 28103\n",
            "sample num:    2, data_idx: 68073\n",
            "sample num:    3, data_idx: 50892\n",
            "sample num:    4, data_idx: 58782\n",
            "sample num:    5, data_idx: 71673\n",
            "sample num:    6, data_idx: 64132\n",
            "sample num:    7, data_idx: 48282\n",
            "sample num:    8, data_idx: 73542\n",
            "sample num:    9, data_idx: 56980\n",
            "sample num:   10, data_idx: 30144\n",
            "sample num:   11, data_idx: 53050\n",
            "sample num:   12, data_idx: 58398\n",
            "sample num:   13, data_idx: 67071\n",
            "sample num:   14, data_idx:   934\n",
            "sample num:   15, data_idx: 31572\n",
            "sample num:   16, data_idx: 45743\n",
            "sample num:   17, data_idx: 31417\n",
            "sample num:   18, data_idx: 28687\n",
            "sample num:   19, data_idx: 58182\n",
            "sample num:   20, data_idx: 52528\n",
            "sample num:   21, data_idx: 67056\n",
            "sample num:   22, data_idx:  3947\n",
            "sample num:   23, data_idx: 16213\n",
            "sample num:   24, data_idx: 43264\n",
            "sample num:   25, data_idx: 68916\n",
            "sample num:   26, data_idx: 17697\n",
            "sample num:   27, data_idx: 71020\n",
            "sample num:   28, data_idx: 43688\n",
            "sample num:   29, data_idx: 39053\n",
            "sample num:   30, data_idx: 37822\n",
            "sample num:   31, data_idx: 46933\n",
            "sample num:   32, data_idx: 12206\n",
            "sample num:   33, data_idx: 69131\n",
            "sample num:   34, data_idx:  3609\n",
            "sample num:   35, data_idx: 79848\n",
            "sample num:   36, data_idx: 36185\n",
            "sample num:   37, data_idx: 16081\n",
            "sample num:   38, data_idx: 45603\n",
            "sample num:   39, data_idx: 67341\n",
            "sample num:   40, data_idx: 11935\n",
            "sample num:   41, data_idx: 78036\n",
            "sample num:   42, data_idx: 16742\n",
            "sample num:   43, data_idx: 30627\n",
            "sample num:   44, data_idx: 44945\n",
            "sample num:   45, data_idx:  2317\n",
            "sample num:   46, data_idx: 34343\n",
            "sample num:   47, data_idx: 11820\n",
            "sample num:   48, data_idx: 33541\n",
            "sample num:   49, data_idx: 41289\n",
            "sample num:   50, data_idx: 64279\n",
            "sample num:   51, data_idx: 14493\n",
            "sample num:   52, data_idx: 22039\n",
            "sample num:   53, data_idx: 57898\n",
            "sample num:   54, data_idx: 56062\n",
            "sample num:   55, data_idx: 84264\n",
            "sample num:   56, data_idx: 60095\n",
            "sample num:   57, data_idx: 41825\n",
            "sample num:   58, data_idx: 75861\n",
            "sample num:   59, data_idx: 50747\n",
            "sample num:   60, data_idx:  7836\n",
            "sample num:   61, data_idx: 66523\n",
            "sample num:   62, data_idx: 28239\n",
            "sample num:   63, data_idx: 82043\n",
            "sample num:   64, data_idx: 33844\n",
            "sample num:   65, data_idx: 29496\n",
            "sample num:   66, data_idx:  9509\n",
            "sample num:   67, data_idx: 57454\n",
            "sample num:   68, data_idx: 42608\n",
            "sample num:   69, data_idx: 69496\n",
            "sample num:   70, data_idx:  8185\n",
            "sample num:   71, data_idx: 54869\n",
            "sample num:   72, data_idx: 72439\n",
            "sample num:   73, data_idx: 30708\n",
            "sample num:   74, data_idx: 68790\n",
            "sample num:   75, data_idx: 74505\n",
            "sample num:   76, data_idx:  5125\n",
            "sample num:   77, data_idx: 80518\n",
            "sample num:   78, data_idx: 38270\n",
            "sample num:   79, data_idx: 62464\n",
            "sample num:   80, data_idx: 28151\n",
            "sample num:   81, data_idx:  1435\n",
            "sample num:   82, data_idx: 61000\n",
            "sample num:   83, data_idx: 55851\n",
            "sample num:   84, data_idx: 83043\n",
            "sample num:   85, data_idx: 34379\n",
            "sample num:   86, data_idx:  7888\n",
            "sample num:   87, data_idx: 66533\n",
            "sample num:   88, data_idx: 33230\n",
            "sample num:   89, data_idx: 68547\n",
            "sample num:   90, data_idx: 19634\n",
            "sample num:   91, data_idx: 65026\n",
            "sample num:   92, data_idx: 53337\n",
            "sample num:   93, data_idx: 79178\n",
            "sample num:   94, data_idx: 28645\n",
            "sample num:   95, data_idx: 36023\n",
            "sample num:   96, data_idx: 60974\n",
            "sample num:   97, data_idx: 27711\n",
            "sample num:   98, data_idx: 64154\n",
            "sample num:   99, data_idx: 20460\n",
            "gsc/gsc_network_fisher.npy\n",
            "[match_page_by_cost]\n",
            "occupation: 0\n",
            "len(page_list): 263\n",
            "len(network_page_list): 656\n",
            "cost: 0.0\n",
            "\n",
            "occupation: 1\n",
            "len(page_list): 458\n",
            "len(network_page_list): 393\n",
            "cost: 0.0042342544\n",
            "\n",
            "assing_page 265.277 ms\n",
            "[calculate_cost]\n",
            "toal_cost: 0.0042353538647148525\n",
            "656 pages allocated for 65531 weights\n",
            "total_network_cost: 0.009812595322728157\n",
            "       0-th page\n",
            "     262-th page\n",
            "       0-th page\n",
            "     392-th page\n",
            "add_vnn\n",
            "gtsrb/gtsrb_network_weight.npy\n",
            "compute_fisher\n",
            "do_compute_fisher\n",
            "sample num:    0, data_idx: 25979\n",
            "sample num:    1, data_idx:  1304\n",
            "sample num:    2, data_idx: 15607\n",
            "sample num:    3, data_idx: 29223\n",
            "sample num:    4, data_idx: 20648\n",
            "sample num:    5, data_idx: 27705\n",
            "sample num:    6, data_idx: 28839\n",
            "sample num:    7, data_idx:  5049\n",
            "sample num:    8, data_idx: 31329\n",
            "sample num:    9, data_idx:  4687\n",
            "sample num:   10, data_idx: 31118\n",
            "sample num:   11, data_idx: 19410\n",
            "sample num:   12, data_idx: 13182\n",
            "sample num:   13, data_idx: 26268\n",
            "sample num:   14, data_idx: 34794\n",
            "sample num:   15, data_idx: 37970\n",
            "sample num:   16, data_idx: 35530\n",
            "sample num:   17, data_idx: 15822\n",
            "sample num:   18, data_idx: 26186\n",
            "sample num:   19, data_idx: 11493\n",
            "sample num:   20, data_idx: 37068\n",
            "sample num:   21, data_idx: 22520\n",
            "sample num:   22, data_idx: 22388\n",
            "sample num:   23, data_idx: 34491\n",
            "sample num:   24, data_idx: 16466\n",
            "sample num:   25, data_idx: 28185\n",
            "sample num:   26, data_idx: 34991\n",
            "sample num:   27, data_idx: 26501\n",
            "sample num:   28, data_idx: 17514\n",
            "sample num:   29, data_idx: 24815\n",
            "sample num:   30, data_idx:  8506\n",
            "sample num:   31, data_idx: 33550\n",
            "sample num:   32, data_idx: 28024\n",
            "sample num:   33, data_idx: 31827\n",
            "sample num:   34, data_idx: 21380\n",
            "sample num:   35, data_idx: 25726\n",
            "sample num:   36, data_idx:   311\n",
            "sample num:   37, data_idx: 33395\n",
            "sample num:   38, data_idx: 26580\n",
            "sample num:   39, data_idx: 34156\n",
            "sample num:   40, data_idx: 18593\n",
            "sample num:   41, data_idx: 27873\n",
            "sample num:   42, data_idx: 32534\n",
            "sample num:   43, data_idx: 12833\n",
            "sample num:   44, data_idx: 32656\n",
            "sample num:   45, data_idx: 21398\n",
            "sample num:   46, data_idx: 20507\n",
            "sample num:   47, data_idx: 29742\n",
            "sample num:   48, data_idx:  2066\n",
            "sample num:   49, data_idx: 23273\n",
            "sample num:   50, data_idx: 15682\n",
            "sample num:   51, data_idx: 20368\n",
            "sample num:   52, data_idx: 12141\n",
            "sample num:   53, data_idx: 27808\n",
            "sample num:   54, data_idx: 19762\n",
            "sample num:   55, data_idx:  4795\n",
            "sample num:   56, data_idx: 26272\n",
            "sample num:   57, data_idx:  3796\n",
            "sample num:   58, data_idx: 11307\n",
            "sample num:   59, data_idx: 28486\n",
            "sample num:   60, data_idx: 26390\n",
            "sample num:   61, data_idx: 33305\n",
            "sample num:   62, data_idx: 35402\n",
            "sample num:   63, data_idx: 26594\n",
            "sample num:   64, data_idx:  8632\n",
            "sample num:   65, data_idx: 13467\n",
            "sample num:   66, data_idx: 21384\n",
            "sample num:   67, data_idx:  7229\n",
            "sample num:   68, data_idx:  7520\n",
            "sample num:   69, data_idx: 18639\n",
            "sample num:   70, data_idx: 30002\n",
            "sample num:   71, data_idx: 27341\n",
            "sample num:   72, data_idx: 37351\n",
            "sample num:   73, data_idx: 30163\n",
            "sample num:   74, data_idx: 21899\n",
            "sample num:   75, data_idx: 10141\n",
            "sample num:   76, data_idx: 13909\n",
            "sample num:   77, data_idx: 27110\n",
            "sample num:   78, data_idx:  1722\n",
            "sample num:   79, data_idx: 28432\n",
            "sample num:   80, data_idx: 23634\n",
            "sample num:   81, data_idx:  4757\n",
            "sample num:   82, data_idx: 18490\n",
            "sample num:   83, data_idx: 37258\n",
            "sample num:   84, data_idx: 11159\n",
            "sample num:   85, data_idx:  6637\n",
            "sample num:   86, data_idx: 22515\n",
            "sample num:   87, data_idx: 19451\n",
            "sample num:   88, data_idx: 17566\n",
            "sample num:   89, data_idx: 33081\n",
            "sample num:   90, data_idx: 16183\n",
            "sample num:   91, data_idx:  2294\n",
            "sample num:   92, data_idx: 37128\n",
            "sample num:   93, data_idx: 25077\n",
            "sample num:   94, data_idx: 25060\n",
            "sample num:   95, data_idx: 36747\n",
            "sample num:   96, data_idx: 33719\n",
            "sample num:   97, data_idx: 10752\n",
            "sample num:   98, data_idx: 13815\n",
            "sample num:   99, data_idx: 24036\n",
            "gtsrb/gtsrb_network_fisher.npy\n",
            "[match_page_by_cost]\n",
            "occupation: 0\n",
            "len(page_list): 0\n",
            "len(network_page_list): 665\n",
            "cost: 0\n",
            "\n",
            "occupation: 1\n",
            "len(page_list): 328\n",
            "len(network_page_list): 665\n",
            "cost: 1.1654733\n",
            "\n",
            "occupation: 2\n",
            "len(page_list): 393\n",
            "len(network_page_list): 337\n",
            "cost: 0.004022163\n",
            "\n",
            "assing_page 140.489 ms\n",
            "[calculate_cost]\n",
            "toal_cost: 1.1696915934053322\n",
            "665 pages allocated for 66475 weights\n",
            "total_network_cost: 5.152259470894933\n",
            "       0-th page\n",
            "     327-th page\n",
            "       0-th page\n",
            "     336-th page\n",
            "tcmalloc: large alloc 1228800000 bytes == 0x20cbe000 @  0x7f64e1aff1e7 0x7f64df448ca1 0x7f64df4b29c5 0x7f64df4b355e 0x7f64df54ca6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "add_vnn\n",
            "cifar10/cifar10_network_weight.npy\n",
            "compute_fisher\n",
            "do_compute_fisher\n",
            "sample num:    0, data_idx:  5783\n",
            "sample num:    1, data_idx: 16208\n",
            "sample num:    2, data_idx: 26883\n",
            "sample num:    3, data_idx: 36601\n",
            "sample num:    4, data_idx: 24778\n",
            "sample num:    5, data_idx: 42500\n",
            "sample num:    6, data_idx: 28994\n",
            "sample num:    7, data_idx:  7971\n",
            "sample num:    8, data_idx:  8424\n",
            "sample num:    9, data_idx: 29845\n",
            "sample num:   10, data_idx: 13536\n",
            "sample num:   11, data_idx: 29646\n",
            "sample num:   12, data_idx: 33837\n",
            "sample num:   13, data_idx: 33036\n",
            "sample num:   14, data_idx: 17397\n",
            "sample num:   15, data_idx:  7247\n",
            "sample num:   16, data_idx: 28045\n",
            "sample num:   17, data_idx: 33832\n",
            "sample num:   18, data_idx: 40090\n",
            "sample num:   19, data_idx: 49838\n",
            "sample num:   20, data_idx: 21881\n",
            "sample num:   21, data_idx: 48390\n",
            "sample num:   22, data_idx: 33767\n",
            "sample num:   23, data_idx: 39423\n",
            "sample num:   24, data_idx:  6388\n",
            "sample num:   25, data_idx:   700\n",
            "sample num:   26, data_idx: 43809\n",
            "sample num:   27, data_idx: 33914\n",
            "sample num:   28, data_idx: 31220\n",
            "sample num:   29, data_idx: 21480\n",
            "sample num:   30, data_idx: 29575\n",
            "sample num:   31, data_idx: 35227\n",
            "sample num:   32, data_idx: 11372\n",
            "sample num:   33, data_idx: 21759\n",
            "sample num:   34, data_idx: 25951\n",
            "sample num:   35, data_idx: 19442\n",
            "sample num:   36, data_idx: 46393\n",
            "sample num:   37, data_idx: 13650\n",
            "sample num:   38, data_idx: 17854\n",
            "sample num:   39, data_idx:  5283\n",
            "sample num:   40, data_idx: 25609\n",
            "sample num:   41, data_idx: 20556\n",
            "sample num:   42, data_idx:  8733\n",
            "sample num:   43, data_idx: 36370\n",
            "sample num:   44, data_idx: 20456\n",
            "sample num:   45, data_idx: 33638\n",
            "sample num:   46, data_idx: 40691\n",
            "sample num:   47, data_idx: 15528\n",
            "sample num:   48, data_idx: 25381\n",
            "sample num:   49, data_idx:  9335\n",
            "sample num:   50, data_idx: 22946\n",
            "sample num:   51, data_idx: 35895\n",
            "sample num:   52, data_idx: 38637\n",
            "sample num:   53, data_idx: 26672\n",
            "sample num:   54, data_idx: 38193\n",
            "sample num:   55, data_idx: 18403\n",
            "sample num:   56, data_idx: 34487\n",
            "sample num:   57, data_idx: 12274\n",
            "sample num:   58, data_idx: 25036\n",
            "sample num:   59, data_idx: 47934\n",
            "sample num:   60, data_idx: 23977\n",
            "sample num:   61, data_idx: 28805\n",
            "sample num:   62, data_idx: 23595\n",
            "sample num:   63, data_idx: 44223\n",
            "sample num:   64, data_idx: 47235\n",
            "sample num:   65, data_idx: 39921\n",
            "sample num:   66, data_idx:  5542\n",
            "sample num:   67, data_idx: 44211\n",
            "sample num:   68, data_idx: 37787\n",
            "sample num:   69, data_idx: 17376\n",
            "sample num:   70, data_idx: 32415\n",
            "sample num:   71, data_idx: 22328\n",
            "sample num:   72, data_idx:  6063\n",
            "sample num:   73, data_idx: 45617\n",
            "sample num:   74, data_idx: 20462\n",
            "sample num:   75, data_idx:  2305\n",
            "sample num:   76, data_idx: 49049\n",
            "sample num:   77, data_idx: 37360\n",
            "sample num:   78, data_idx: 23583\n",
            "sample num:   79, data_idx:  5240\n",
            "sample num:   80, data_idx: 26084\n",
            "sample num:   81, data_idx:  2624\n",
            "sample num:   82, data_idx: 41643\n",
            "sample num:   83, data_idx: 15878\n",
            "sample num:   84, data_idx:  1579\n",
            "sample num:   85, data_idx: 28923\n",
            "sample num:   86, data_idx: 42252\n",
            "sample num:   87, data_idx:  7461\n",
            "sample num:   88, data_idx: 20944\n",
            "sample num:   89, data_idx: 26107\n",
            "sample num:   90, data_idx:  4907\n",
            "sample num:   91, data_idx: 48792\n",
            "sample num:   92, data_idx: 17426\n",
            "sample num:   93, data_idx:  6504\n",
            "sample num:   94, data_idx: 44345\n",
            "sample num:   95, data_idx: 49110\n",
            "sample num:   96, data_idx: 19023\n",
            "sample num:   97, data_idx:  2706\n",
            "sample num:   98, data_idx: 21922\n",
            "sample num:   99, data_idx: 14105\n",
            "cifar10/cifar10_network_fisher.npy\n",
            "[match_page_by_cost]\n",
            "occupation: 0\n",
            "len(page_list): 0\n",
            "len(network_page_list): 455\n",
            "cost: 0\n",
            "\n",
            "occupation: 1\n",
            "len(page_list): 0\n",
            "len(network_page_list): 455\n",
            "cost: 0\n",
            "\n",
            "occupation: 2\n",
            "len(page_list): 384\n",
            "len(network_page_list): 455\n",
            "cost: 16.384817\n",
            "\n",
            "occupation: 3\n",
            "len(page_list): 337\n",
            "len(network_page_list): 71\n",
            "cost: 0.0005166788\n",
            "\n",
            "assing_page 113.386 ms\n",
            "[calculate_cost]\n",
            "toal_cost: 16.38532425781841\n",
            "455 pages allocated for 45490 weights\n",
            "total_network_cost: 53.50064909644425\n",
            "       0-th page\n",
            "     383-th page\n",
            "       0-th page\n",
            "      70-th page\n",
            "add_vnn\n",
            "svhn/svhn_network_weight.npy\n",
            "compute_fisher\n",
            "do_compute_fisher\n",
            "sample num:    0, data_idx: 39243\n",
            "sample num:    1, data_idx: 61246\n",
            "sample num:    2, data_idx: 29077\n",
            "sample num:    3, data_idx: 57062\n",
            "sample num:    4, data_idx: 37208\n",
            "sample num:    5, data_idx: 51089\n",
            "sample num:    6, data_idx:  4525\n",
            "sample num:    7, data_idx: 12628\n",
            "sample num:    8, data_idx: 21527\n",
            "sample num:    9, data_idx: 34780\n",
            "sample num:   10, data_idx: 43562\n",
            "sample num:   11, data_idx:  7698\n",
            "sample num:   12, data_idx:  4889\n",
            "sample num:   13, data_idx: 26042\n",
            "sample num:   14, data_idx: 24445\n",
            "sample num:   15, data_idx: 17218\n",
            "sample num:   16, data_idx: 17706\n",
            "sample num:   17, data_idx:   489\n",
            "sample num:   18, data_idx: 22549\n",
            "sample num:   19, data_idx: 32527\n",
            "sample num:   20, data_idx: 33766\n",
            "sample num:   21, data_idx: 47253\n",
            "sample num:   22, data_idx: 41080\n",
            "sample num:   23, data_idx: 51818\n",
            "sample num:   24, data_idx: 31678\n",
            "sample num:   25, data_idx: 51729\n",
            "sample num:   26, data_idx: 58054\n",
            "sample num:   27, data_idx: 40794\n",
            "sample num:   28, data_idx: 55610\n",
            "sample num:   29, data_idx: 59408\n",
            "sample num:   30, data_idx: 60679\n",
            "sample num:   31, data_idx: 16357\n",
            "sample num:   32, data_idx: 40066\n",
            "sample num:   33, data_idx:  8257\n",
            "sample num:   34, data_idx: 54999\n",
            "sample num:   35, data_idx: 49489\n",
            "sample num:   36, data_idx: 26381\n",
            "sample num:   37, data_idx: 32599\n",
            "sample num:   38, data_idx:  8922\n",
            "sample num:   39, data_idx: 17457\n",
            "sample num:   40, data_idx: 37414\n",
            "sample num:   41, data_idx:  4545\n",
            "sample num:   42, data_idx: 60584\n",
            "sample num:   43, data_idx: 32460\n",
            "sample num:   44, data_idx: 44787\n",
            "sample num:   45, data_idx:  1695\n",
            "sample num:   46, data_idx: 61976\n",
            "sample num:   47, data_idx: 43311\n",
            "sample num:   48, data_idx: 58901\n",
            "sample num:   49, data_idx: 39297\n",
            "sample num:   50, data_idx: 46691\n",
            "sample num:   51, data_idx: 56016\n",
            "sample num:   52, data_idx:  2971\n",
            "sample num:   53, data_idx: 38715\n",
            "sample num:   54, data_idx: 49247\n",
            "sample num:   55, data_idx:  1351\n",
            "sample num:   56, data_idx: 33195\n",
            "sample num:   57, data_idx: 63261\n",
            "sample num:   58, data_idx: 20857\n",
            "sample num:   59, data_idx: 28828\n",
            "sample num:   60, data_idx: 58728\n",
            "sample num:   61, data_idx: 53275\n",
            "sample num:   62, data_idx:  5193\n",
            "sample num:   63, data_idx: 26888\n",
            "sample num:   64, data_idx: 34381\n",
            "sample num:   65, data_idx: 58363\n",
            "sample num:   66, data_idx: 56057\n",
            "sample num:   67, data_idx:  3670\n",
            "sample num:   68, data_idx: 39863\n",
            "sample num:   69, data_idx: 30439\n",
            "sample num:   70, data_idx: 17846\n",
            "sample num:   71, data_idx: 29659\n",
            "sample num:   72, data_idx: 22233\n",
            "sample num:   73, data_idx: 62779\n",
            "sample num:   74, data_idx: 27049\n",
            "sample num:   75, data_idx: 47522\n",
            "sample num:   76, data_idx: 12727\n",
            "sample num:   77, data_idx: 54557\n",
            "sample num:   78, data_idx: 48171\n",
            "sample num:   79, data_idx: 52437\n",
            "sample num:   80, data_idx: 64255\n",
            "sample num:   81, data_idx: 52801\n",
            "sample num:   82, data_idx:  7212\n",
            "sample num:   83, data_idx:  6638\n",
            "sample num:   84, data_idx: 25747\n",
            "sample num:   85, data_idx: 18292\n",
            "sample num:   86, data_idx: 60613\n",
            "sample num:   87, data_idx:   574\n",
            "sample num:   88, data_idx: 61634\n",
            "sample num:   89, data_idx:  8934\n",
            "sample num:   90, data_idx: 60002\n",
            "sample num:   91, data_idx:   121\n",
            "sample num:   92, data_idx: 47684\n",
            "sample num:   93, data_idx: 64351\n",
            "sample num:   94, data_idx: 54093\n",
            "sample num:   95, data_idx: 25705\n",
            "sample num:   96, data_idx: 20304\n",
            "sample num:   97, data_idx:  7010\n",
            "sample num:   98, data_idx: 59118\n",
            "sample num:   99, data_idx: 16139\n",
            "svhn/svhn_network_fisher.npy\n",
            "[match_page_by_cost]\n",
            "occupation: 0\n",
            "len(page_list): 0\n",
            "len(network_page_list): 455\n",
            "cost: 0\n",
            "\n",
            "occupation: 1\n",
            "len(page_list): 0\n",
            "len(network_page_list): 455\n",
            "cost: 0\n",
            "\n",
            "occupation: 2\n",
            "len(page_list): 0\n",
            "len(network_page_list): 455\n",
            "cost: 0\n",
            "\n",
            "occupation: 3\n",
            "len(page_list): 650\n",
            "len(network_page_list): 455\n",
            "cost: 3.99143\n",
            "\n",
            "assing_page 100.715 ms\n",
            "[calculate_cost]\n",
            "toal_cost: 3.9915305366266693\n",
            "455 pages allocated for 45490 weights\n",
            "total_network_cost: 79.56870651431382\n",
            "       0-th page\n",
            "     454-th page\n",
            "add_vnn\n",
            "fmnist/fmnist_network_weight.npy\n",
            "compute_fisher\n",
            "do_compute_fisher\n",
            "sample num:    0, data_idx: 19798\n",
            "sample num:    1, data_idx: 43496\n",
            "sample num:    2, data_idx:  8286\n",
            "sample num:    3, data_idx: 32002\n",
            "sample num:    4, data_idx: 50538\n",
            "sample num:    5, data_idx: 48582\n",
            "sample num:    6, data_idx: 45061\n",
            "sample num:    7, data_idx: 37587\n",
            "sample num:    8, data_idx:  2783\n",
            "sample num:    9, data_idx: 21513\n",
            "sample num:   10, data_idx: 59661\n",
            "sample num:   11, data_idx: 17367\n",
            "sample num:   12, data_idx: 37126\n",
            "sample num:   13, data_idx: 32085\n",
            "sample num:   14, data_idx: 41675\n",
            "sample num:   15, data_idx: 14625\n",
            "sample num:   16, data_idx: 56603\n",
            "sample num:   17, data_idx: 29645\n",
            "sample num:   18, data_idx: 17074\n",
            "sample num:   19, data_idx: 23533\n",
            "sample num:   20, data_idx: 28336\n",
            "sample num:   21, data_idx:  6681\n",
            "sample num:   22, data_idx:  7069\n",
            "sample num:   23, data_idx: 33093\n",
            "sample num:   24, data_idx: 57145\n",
            "sample num:   25, data_idx: 33938\n",
            "sample num:   26, data_idx: 55560\n",
            "sample num:   27, data_idx: 23283\n",
            "sample num:   28, data_idx: 37767\n",
            "sample num:   29, data_idx: 12701\n",
            "sample num:   30, data_idx: 16606\n",
            "sample num:   31, data_idx: 54709\n",
            "sample num:   32, data_idx: 26318\n",
            "sample num:   33, data_idx: 10352\n",
            "sample num:   34, data_idx: 39355\n",
            "sample num:   35, data_idx: 49024\n",
            "sample num:   36, data_idx: 52626\n",
            "sample num:   37, data_idx: 48167\n",
            "sample num:   38, data_idx: 20691\n",
            "sample num:   39, data_idx: 38256\n",
            "sample num:   40, data_idx: 16110\n",
            "sample num:   41, data_idx: 53541\n",
            "sample num:   42, data_idx:  2828\n",
            "sample num:   43, data_idx: 22982\n",
            "sample num:   44, data_idx: 37457\n",
            "sample num:   45, data_idx: 43451\n",
            "sample num:   46, data_idx:  2522\n",
            "sample num:   47, data_idx: 52763\n",
            "sample num:   48, data_idx: 45078\n",
            "sample num:   49, data_idx: 10706\n",
            "sample num:   50, data_idx: 53757\n",
            "sample num:   51, data_idx: 25847\n",
            "sample num:   52, data_idx: 46609\n",
            "sample num:   53, data_idx: 28279\n",
            "sample num:   54, data_idx: 30297\n",
            "sample num:   55, data_idx: 19313\n",
            "sample num:   56, data_idx: 32463\n",
            "sample num:   57, data_idx: 12826\n",
            "sample num:   58, data_idx: 35176\n",
            "sample num:   59, data_idx: 10114\n",
            "sample num:   60, data_idx:  5417\n",
            "sample num:   61, data_idx: 24753\n",
            "sample num:   62, data_idx: 24473\n",
            "sample num:   63, data_idx: 41249\n",
            "sample num:   64, data_idx: 15207\n",
            "sample num:   65, data_idx: 24385\n",
            "sample num:   66, data_idx: 13973\n",
            "sample num:   67, data_idx: 14768\n",
            "sample num:   68, data_idx: 44932\n",
            "sample num:   69, data_idx: 48905\n",
            "sample num:   70, data_idx: 18981\n",
            "sample num:   71, data_idx:  5565\n",
            "sample num:   72, data_idx:  4888\n",
            "sample num:   73, data_idx: 23000\n",
            "sample num:   74, data_idx: 17794\n",
            "sample num:   75, data_idx:  1913\n",
            "sample num:   76, data_idx:  7604\n",
            "sample num:   77, data_idx: 38235\n",
            "sample num:   78, data_idx: 56865\n",
            "sample num:   79, data_idx: 48384\n",
            "sample num:   80, data_idx: 46245\n",
            "sample num:   81, data_idx: 59333\n",
            "sample num:   82, data_idx: 58772\n",
            "sample num:   83, data_idx: 10849\n",
            "sample num:   84, data_idx: 26026\n",
            "sample num:   85, data_idx: 17273\n",
            "sample num:   86, data_idx: 14688\n",
            "sample num:   87, data_idx: 32458\n",
            "sample num:   88, data_idx: 54378\n",
            "sample num:   89, data_idx: 10036\n",
            "sample num:   90, data_idx: 39651\n",
            "sample num:   91, data_idx: 57274\n",
            "sample num:   92, data_idx: 49361\n",
            "sample num:   93, data_idx: 28782\n",
            "sample num:   94, data_idx: 35998\n",
            "sample num:   95, data_idx: 59881\n",
            "sample num:   96, data_idx:  3355\n",
            "sample num:   97, data_idx: 30610\n",
            "sample num:   98, data_idx: 24770\n",
            "sample num:   99, data_idx: 34011\n",
            "fmnist/fmnist_network_fisher.npy\n",
            "[match_page_by_cost]\n",
            "occupation: 0\n",
            "len(page_list): 0\n",
            "len(network_page_list): 700\n",
            "cost: 0\n",
            "\n",
            "occupation: 1\n",
            "len(page_list): 0\n",
            "len(network_page_list): 700\n",
            "cost: 0\n",
            "\n",
            "occupation: 2\n",
            "len(page_list): 0\n",
            "len(network_page_list): 700\n",
            "cost: 0\n",
            "\n",
            "occupation: 3\n",
            "len(page_list): 195\n",
            "len(network_page_list): 700\n",
            "cost: 3.5111954\n",
            "\n",
            "occupation: 4\n",
            "len(page_list): 526\n",
            "len(network_page_list): 505\n",
            "cost: 0.038693868\n",
            "\n",
            "assing_page 148.148 ms\n",
            "[calculate_cost]\n",
            "toal_cost: 3.549944239751312\n",
            "700 pages allocated for 69966 weights\n",
            "total_network_cost: 113.00448002107441\n",
            "       0-th page\n",
            "     194-th page\n",
            "       0-th page\n",
            "     504-th page\n",
            "tcmalloc: large alloc 1914986496 bytes == 0x8080000 @  0x7f321a61a1e7 0x7f3217f63ca1 0x7f3217fcd9c5 0x7f3217fce55e 0x7f3218067a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "add_vnn\n",
            "us8k/us8k_network_weight.npy\n",
            "compute_fisher\n",
            "do_compute_fisher\n",
            "sample num:    0, data_idx: 36059\n",
            "sample num:    1, data_idx: 38422\n",
            "sample num:    2, data_idx: 12293\n",
            "sample num:    3, data_idx: 40946\n",
            "sample num:    4, data_idx: 42887\n",
            "sample num:    5, data_idx: 36466\n",
            "sample num:    6, data_idx: 39356\n",
            "sample num:    7, data_idx: 39773\n",
            "sample num:    8, data_idx: 26275\n",
            "sample num:    9, data_idx:   820\n",
            "sample num:   10, data_idx: 15601\n",
            "sample num:   11, data_idx: 21787\n",
            "sample num:   12, data_idx: 46749\n",
            "sample num:   13, data_idx: 37429\n",
            "sample num:   14, data_idx: 38517\n",
            "sample num:   15, data_idx: 12609\n",
            "sample num:   16, data_idx: 27697\n",
            "sample num:   17, data_idx: 44841\n",
            "sample num:   18, data_idx: 29895\n",
            "sample num:   19, data_idx: 17642\n",
            "sample num:   20, data_idx:  9765\n",
            "sample num:   21, data_idx: 39805\n",
            "sample num:   22, data_idx:  7354\n",
            "sample num:   23, data_idx: 23429\n",
            "sample num:   24, data_idx: 24545\n",
            "sample num:   25, data_idx: 46519\n",
            "sample num:   26, data_idx: 33027\n",
            "sample num:   27, data_idx: 27041\n",
            "sample num:   28, data_idx:  1667\n",
            "sample num:   29, data_idx: 19814\n",
            "sample num:   30, data_idx:   632\n",
            "sample num:   31, data_idx: 29575\n",
            "sample num:   32, data_idx: 10739\n",
            "sample num:   33, data_idx: 36571\n",
            "sample num:   34, data_idx: 19756\n",
            "sample num:   35, data_idx:  4405\n",
            "sample num:   36, data_idx:  5852\n",
            "sample num:   37, data_idx: 35083\n",
            "sample num:   38, data_idx: 24665\n",
            "sample num:   39, data_idx:  9025\n",
            "sample num:   40, data_idx: 39805\n",
            "sample num:   41, data_idx:  3737\n",
            "sample num:   42, data_idx: 11283\n",
            "sample num:   43, data_idx:  2359\n",
            "sample num:   44, data_idx: 32265\n",
            "sample num:   45, data_idx:  1567\n",
            "sample num:   46, data_idx: 44044\n",
            "sample num:   47, data_idx: 23518\n",
            "sample num:   48, data_idx: 14762\n",
            "sample num:   49, data_idx: 21196\n",
            "sample num:   50, data_idx: 15965\n",
            "sample num:   51, data_idx: 42275\n",
            "sample num:   52, data_idx: 32064\n",
            "sample num:   53, data_idx:  7992\n",
            "sample num:   54, data_idx: 23663\n",
            "sample num:   55, data_idx:  7373\n",
            "sample num:   56, data_idx: 20605\n",
            "sample num:   57, data_idx: 18630\n",
            "sample num:   58, data_idx: 45110\n",
            "sample num:   59, data_idx: 30156\n",
            "sample num:   60, data_idx: 26487\n",
            "sample num:   61, data_idx:  4999\n",
            "sample num:   62, data_idx: 34733\n",
            "sample num:   63, data_idx: 40991\n",
            "sample num:   64, data_idx: 43192\n",
            "sample num:   65, data_idx: 34127\n",
            "sample num:   66, data_idx: 47692\n",
            "sample num:   67, data_idx: 38337\n",
            "sample num:   68, data_idx:  4155\n",
            "sample num:   69, data_idx: 10865\n",
            "sample num:   70, data_idx: 47418\n",
            "sample num:   71, data_idx: 27509\n",
            "sample num:   72, data_idx: 47093\n",
            "sample num:   73, data_idx:  2782\n",
            "sample num:   74, data_idx: 13363\n",
            "sample num:   75, data_idx: 42543\n",
            "sample num:   76, data_idx:  7623\n",
            "sample num:   77, data_idx: 19764\n",
            "sample num:   78, data_idx: 26168\n",
            "sample num:   79, data_idx: 46152\n",
            "sample num:   80, data_idx: 12765\n",
            "sample num:   81, data_idx: 32241\n",
            "sample num:   82, data_idx: 41487\n",
            "sample num:   83, data_idx:  2019\n",
            "sample num:   84, data_idx: 18274\n",
            "sample num:   85, data_idx: 10114\n",
            "sample num:   86, data_idx:   945\n",
            "sample num:   87, data_idx: 16147\n",
            "sample num:   88, data_idx: 31251\n",
            "sample num:   89, data_idx:  6001\n",
            "sample num:   90, data_idx: 31174\n",
            "sample num:   91, data_idx: 44250\n",
            "sample num:   92, data_idx:  1570\n",
            "sample num:   93, data_idx: 48647\n",
            "sample num:   94, data_idx: 45023\n",
            "sample num:   95, data_idx: 10205\n",
            "sample num:   96, data_idx: 13923\n",
            "sample num:   97, data_idx: 43722\n",
            "sample num:   98, data_idx:  5313\n",
            "sample num:   99, data_idx: 22057\n",
            "us8k/us8k_network_fisher.npy\n",
            "[match_page_by_cost]\n",
            "occupation: 0\n",
            "len(page_list): 0\n",
            "len(network_page_list): 669\n",
            "cost: 0\n",
            "\n",
            "occupation: 1\n",
            "len(page_list): 0\n",
            "len(network_page_list): 669\n",
            "cost: 0\n",
            "\n",
            "occupation: 2\n",
            "len(page_list): 0\n",
            "len(network_page_list): 669\n",
            "cost: 0\n",
            "\n",
            "occupation: 3\n",
            "len(page_list): 0\n",
            "len(network_page_list): 669\n",
            "cost: 0\n",
            "\n",
            "occupation: 4\n",
            "len(page_list): 216\n",
            "len(network_page_list): 669\n",
            "cost: 9.985345\n",
            "\n",
            "occupation: 5\n",
            "len(page_list): 505\n",
            "len(network_page_list): 453\n",
            "cost: 0.06752682\n",
            "\n",
            "assing_page 146.569 ms\n",
            "[calculate_cost]\n",
            "toal_cost: 10.063458234926657\n",
            "669 pages allocated for 66854 weights\n",
            "total_network_cost: 307.73666979558766\n",
            "       0-th page\n",
            "     215-th page\n",
            "       0-th page\n",
            "     452-th page\n",
            "tcmalloc: large alloc 2286338048 bytes == 0x7c16000 @  0x7f1fa38c81e7 0x7f1fa1211ca1 0x7f1fa127b9c5 0x7f1fa127c55e 0x7f1fa1315a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "add_vnn\n",
            "hhar/hhar_network_weight.npy\n",
            "compute_fisher\n",
            "do_compute_fisher\n",
            "sample num:    0, data_idx: 73019\n",
            "sample num:    1, data_idx: 24005\n",
            "sample num:    2, data_idx: 71407\n",
            "sample num:    3, data_idx: 87812\n",
            "sample num:    4, data_idx: 11680\n",
            "sample num:    5, data_idx: 48537\n",
            "sample num:    6, data_idx: 43682\n",
            "sample num:    7, data_idx: 72282\n",
            "sample num:    8, data_idx: 79611\n",
            "sample num:    9, data_idx: 41105\n",
            "sample num:   10, data_idx: 35871\n",
            "sample num:   11, data_idx: 48253\n",
            "sample num:   12, data_idx: 19454\n",
            "sample num:   13, data_idx: 15811\n",
            "sample num:   14, data_idx:  1612\n",
            "sample num:   15, data_idx: 84469\n",
            "sample num:   16, data_idx: 90677\n",
            "sample num:   17, data_idx: 72823\n",
            "sample num:   18, data_idx: 86107\n",
            "sample num:   19, data_idx: 71754\n",
            "sample num:   20, data_idx: 87964\n",
            "sample num:   21, data_idx: 69770\n",
            "sample num:   22, data_idx: 86998\n",
            "sample num:   23, data_idx:  2387\n",
            "sample num:   24, data_idx: 29949\n",
            "sample num:   25, data_idx:  9860\n",
            "sample num:   26, data_idx: 62175\n",
            "sample num:   27, data_idx: 39635\n",
            "sample num:   28, data_idx: 69914\n",
            "sample num:   29, data_idx: 80548\n",
            "sample num:   30, data_idx:  1420\n",
            "sample num:   31, data_idx:  6403\n",
            "sample num:   32, data_idx: 70256\n",
            "sample num:   33, data_idx: 32413\n",
            "sample num:   34, data_idx: 21415\n",
            "sample num:   35, data_idx: 13503\n",
            "sample num:   36, data_idx: 33850\n",
            "sample num:   37, data_idx: 97959\n",
            "sample num:   38, data_idx: 76263\n",
            "sample num:   39, data_idx: 73843\n",
            "sample num:   40, data_idx: 27496\n",
            "sample num:   41, data_idx: 39222\n",
            "sample num:   42, data_idx: 105699\n",
            "sample num:   43, data_idx: 103220\n",
            "sample num:   44, data_idx: 79188\n",
            "sample num:   45, data_idx: 69190\n",
            "sample num:   46, data_idx: 116664\n",
            "sample num:   47, data_idx: 31012\n",
            "sample num:   48, data_idx: 118905\n",
            "sample num:   49, data_idx:  4738\n",
            "sample num:   50, data_idx: 22517\n",
            "sample num:   51, data_idx:  8092\n",
            "sample num:   52, data_idx: 11327\n",
            "sample num:   53, data_idx: 101241\n",
            "sample num:   54, data_idx: 70876\n",
            "sample num:   55, data_idx: 35456\n",
            "sample num:   56, data_idx: 103718\n",
            "sample num:   57, data_idx:  6373\n",
            "sample num:   58, data_idx:  6269\n",
            "sample num:   59, data_idx: 117069\n",
            "sample num:   60, data_idx: 61453\n",
            "sample num:   61, data_idx:  7949\n",
            "sample num:   62, data_idx: 23446\n",
            "sample num:   63, data_idx: 31313\n",
            "sample num:   64, data_idx: 22899\n",
            "sample num:   65, data_idx: 12477\n",
            "sample num:   66, data_idx: 93791\n",
            "sample num:   67, data_idx: 24432\n",
            "sample num:   68, data_idx: 19604\n",
            "sample num:   69, data_idx: 61091\n",
            "sample num:   70, data_idx: 53628\n",
            "sample num:   71, data_idx: 26551\n",
            "sample num:   72, data_idx: 65692\n",
            "sample num:   73, data_idx: 11489\n",
            "sample num:   74, data_idx: 30943\n",
            "sample num:   75, data_idx: 116058\n",
            "sample num:   76, data_idx: 30195\n",
            "sample num:   77, data_idx: 63387\n",
            "sample num:   78, data_idx: 47205\n",
            "sample num:   79, data_idx: 22243\n",
            "sample num:   80, data_idx: 111314\n",
            "sample num:   81, data_idx: 59098\n",
            "sample num:   82, data_idx: 47266\n",
            "sample num:   83, data_idx: 41506\n",
            "sample num:   84, data_idx: 41958\n",
            "sample num:   85, data_idx: 112601\n",
            "sample num:   86, data_idx: 31135\n",
            "sample num:   87, data_idx:  8369\n",
            "sample num:   88, data_idx: 67354\n",
            "sample num:   89, data_idx: 85875\n",
            "sample num:   90, data_idx: 93677\n",
            "sample num:   91, data_idx: 93038\n",
            "sample num:   92, data_idx: 65286\n",
            "sample num:   93, data_idx: 94204\n",
            "sample num:   94, data_idx:  5956\n",
            "sample num:   95, data_idx: 27441\n",
            "sample num:   96, data_idx: 102276\n",
            "sample num:   97, data_idx: 74441\n",
            "sample num:   98, data_idx: 69646\n",
            "sample num:   99, data_idx: 52186\n",
            "hhar/hhar_network_fisher.npy\n",
            "[match_page_by_cost]\n",
            "occupation: 0\n",
            "len(page_list): 0\n",
            "len(network_page_list): 652\n",
            "cost: 0\n",
            "\n",
            "occupation: 1\n",
            "len(page_list): 0\n",
            "len(network_page_list): 652\n",
            "cost: 0\n",
            "\n",
            "occupation: 2\n",
            "len(page_list): 0\n",
            "len(network_page_list): 652\n",
            "cost: 0\n",
            "\n",
            "occupation: 3\n",
            "len(page_list): 0\n",
            "len(network_page_list): 652\n",
            "cost: 0\n",
            "\n",
            "occupation: 4\n",
            "len(page_list): 0\n",
            "len(network_page_list): 652\n",
            "cost: 0\n",
            "\n",
            "occupation: 5\n",
            "len(page_list): 268\n",
            "len(network_page_list): 652\n",
            "cost: 9.816581\n",
            "\n",
            "occupation: 6\n",
            "len(page_list): 453\n",
            "len(network_page_list): 384\n",
            "cost: 0.028850462\n",
            "\n",
            "assing_page 145.464 ms\n",
            "[calculate_cost]\n",
            "toal_cost: 9.845482973545586\n",
            "652 pages allocated for 65114 weights\n",
            "total_network_cost: 375.8033974338323\n",
            "       0-th page\n",
            "     267-th page\n",
            "       0-th page\n",
            "     383-th page\n",
            "add_vnn\n",
            "esc10/esc10_network_weight.npy\n",
            "compute_fisher\n",
            "do_compute_fisher\n",
            "sample num:    0, data_idx:   248\n",
            "sample num:    1, data_idx:  1329\n",
            "sample num:    2, data_idx:  3448\n",
            "sample num:    3, data_idx:  2556\n",
            "sample num:    4, data_idx:  4806\n",
            "sample num:    5, data_idx:   627\n",
            "sample num:    6, data_idx:   467\n",
            "sample num:    7, data_idx:  5488\n",
            "sample num:    8, data_idx:  5625\n",
            "sample num:    9, data_idx:  6087\n",
            "sample num:   10, data_idx:  5049\n",
            "sample num:   11, data_idx:  3163\n",
            "sample num:   12, data_idx:  4452\n",
            "sample num:   13, data_idx:  2921\n",
            "sample num:   14, data_idx:  5806\n",
            "sample num:   15, data_idx:  3848\n",
            "sample num:   16, data_idx:  3167\n",
            "sample num:   17, data_idx:  3417\n",
            "sample num:   18, data_idx:  6276\n",
            "sample num:   19, data_idx:  4520\n",
            "sample num:   20, data_idx:  3812\n",
            "sample num:   21, data_idx:  4834\n",
            "sample num:   22, data_idx:   374\n",
            "sample num:   23, data_idx:  1492\n",
            "sample num:   24, data_idx:  5932\n",
            "sample num:   25, data_idx:  2619\n",
            "sample num:   26, data_idx:  2636\n",
            "sample num:   27, data_idx:  2766\n",
            "sample num:   28, data_idx:   969\n",
            "sample num:   29, data_idx:  1464\n",
            "sample num:   30, data_idx:     0\n",
            "sample num:   31, data_idx:  4375\n",
            "sample num:   32, data_idx:  2202\n",
            "sample num:   33, data_idx:  2978\n",
            "sample num:   34, data_idx:   855\n",
            "sample num:   35, data_idx:  5100\n",
            "sample num:   36, data_idx:  1622\n",
            "sample num:   37, data_idx:  5271\n",
            "sample num:   38, data_idx:  2982\n",
            "sample num:   39, data_idx:   767\n",
            "sample num:   40, data_idx:  3197\n",
            "sample num:   41, data_idx:  3152\n",
            "sample num:   42, data_idx:  1901\n",
            "sample num:   43, data_idx:  5245\n",
            "sample num:   44, data_idx:   151\n",
            "sample num:   45, data_idx:  5799\n",
            "sample num:   46, data_idx:  5943\n",
            "sample num:   47, data_idx:   843\n",
            "sample num:   48, data_idx:   375\n",
            "sample num:   49, data_idx:  3614\n",
            "sample num:   50, data_idx:  2203\n",
            "sample num:   51, data_idx:   146\n",
            "sample num:   52, data_idx:  5297\n",
            "sample num:   53, data_idx:  2333\n",
            "sample num:   54, data_idx:  6170\n",
            "sample num:   55, data_idx:  3980\n",
            "sample num:   56, data_idx:  2552\n",
            "sample num:   57, data_idx:  3863\n",
            "sample num:   58, data_idx:  1907\n",
            "sample num:   59, data_idx:  5699\n",
            "sample num:   60, data_idx:   753\n",
            "sample num:   61, data_idx:  3258\n",
            "sample num:   62, data_idx:  4910\n",
            "sample num:   63, data_idx:  4675\n",
            "sample num:   64, data_idx:  3413\n",
            "sample num:   65, data_idx:  2722\n",
            "sample num:   66, data_idx:   585\n",
            "sample num:   67, data_idx:  2314\n",
            "sample num:   68, data_idx:  3650\n",
            "sample num:   69, data_idx:  3613\n",
            "sample num:   70, data_idx:   810\n",
            "sample num:   71, data_idx:  6014\n",
            "sample num:   72, data_idx:  1959\n",
            "sample num:   73, data_idx:  3763\n",
            "sample num:   74, data_idx:  6006\n",
            "sample num:   75, data_idx:  5989\n",
            "sample num:   76, data_idx:  5754\n",
            "sample num:   77, data_idx:  2291\n",
            "sample num:   78, data_idx:  3259\n",
            "sample num:   79, data_idx:  5937\n",
            "sample num:   80, data_idx:  1618\n",
            "sample num:   81, data_idx:  1471\n",
            "sample num:   82, data_idx:  3876\n",
            "sample num:   83, data_idx:  1828\n",
            "sample num:   84, data_idx:  4977\n",
            "sample num:   85, data_idx:  1373\n",
            "sample num:   86, data_idx:  1282\n",
            "sample num:   87, data_idx:  1980\n",
            "sample num:   88, data_idx:  5390\n",
            "sample num:   89, data_idx:  2078\n",
            "sample num:   90, data_idx:  1512\n",
            "sample num:   91, data_idx:  2408\n",
            "sample num:   92, data_idx:   102\n",
            "sample num:   93, data_idx:  2612\n",
            "sample num:   94, data_idx:  3097\n",
            "sample num:   95, data_idx:  4678\n",
            "sample num:   96, data_idx:  1655\n",
            "sample num:   97, data_idx:  4134\n",
            "sample num:   98, data_idx:  4998\n",
            "sample num:   99, data_idx:  4514\n",
            "esc10/esc10_network_fisher.npy\n",
            "[match_page_by_cost]\n",
            "occupation: 0\n",
            "len(page_list): 0\n",
            "len(network_page_list): 652\n",
            "cost: 0\n",
            "\n",
            "occupation: 1\n",
            "len(page_list): 0\n",
            "len(network_page_list): 652\n",
            "cost: 0\n",
            "\n",
            "occupation: 2\n",
            "len(page_list): 0\n",
            "len(network_page_list): 652\n",
            "cost: 0\n",
            "\n",
            "occupation: 3\n",
            "len(page_list): 0\n",
            "len(network_page_list): 652\n",
            "cost: 0\n",
            "\n",
            "occupation: 4\n",
            "len(page_list): 0\n",
            "len(network_page_list): 652\n",
            "cost: 0\n",
            "\n",
            "occupation: 5\n",
            "len(page_list): 0\n",
            "len(network_page_list): 652\n",
            "cost: 0\n",
            "\n",
            "occupation: 6\n",
            "len(page_list): 337\n",
            "len(network_page_list): 652\n",
            "cost: 13.264575\n",
            "\n",
            "occupation: 7\n",
            "len(page_list): 384\n",
            "len(network_page_list): 315\n",
            "cost: 0.05866583\n",
            "\n",
            "assing_page 144.602 ms\n",
            "[calculate_cost]\n",
            "toal_cost: 13.330318228981923\n",
            "652 pages allocated for 65154 weights\n",
            "total_network_cost: 442.4908014703542\n",
            "       0-th page\n",
            "     336-th page\n",
            "       0-th page\n",
            "     314-th page\n",
            "add_vnn\n",
            "obs/obs_network_weight.npy\n",
            "compute_fisher\n",
            "do_compute_fisher\n",
            "sample num:    0, data_idx:  4541\n",
            "sample num:    1, data_idx:  3111\n",
            "sample num:    2, data_idx:  4319\n",
            "sample num:    3, data_idx:  3626\n",
            "sample num:    4, data_idx:  5614\n",
            "sample num:    5, data_idx:  4381\n",
            "sample num:    6, data_idx:  5062\n",
            "sample num:    7, data_idx:  4430\n",
            "sample num:    8, data_idx:  4462\n",
            "sample num:    9, data_idx:  2768\n",
            "sample num:   10, data_idx:  1883\n",
            "sample num:   11, data_idx:  5213\n",
            "sample num:   12, data_idx:  5202\n",
            "sample num:   13, data_idx:   742\n",
            "sample num:   14, data_idx:  3121\n",
            "sample num:   15, data_idx:  2375\n",
            "sample num:   16, data_idx:  5545\n",
            "sample num:   17, data_idx:   670\n",
            "sample num:   18, data_idx:  3151\n",
            "sample num:   19, data_idx:  2416\n",
            "sample num:   20, data_idx:  2965\n",
            "sample num:   21, data_idx:   375\n",
            "sample num:   22, data_idx:  2571\n",
            "sample num:   23, data_idx:   415\n",
            "sample num:   24, data_idx:  2289\n",
            "sample num:   25, data_idx:   424\n",
            "sample num:   26, data_idx:    10\n",
            "sample num:   27, data_idx:  2724\n",
            "sample num:   28, data_idx:  1786\n",
            "sample num:   29, data_idx:   697\n",
            "sample num:   30, data_idx:  4372\n",
            "sample num:   31, data_idx:  1740\n",
            "sample num:   32, data_idx:  3062\n",
            "sample num:   33, data_idx:  3540\n",
            "sample num:   34, data_idx:   922\n",
            "sample num:   35, data_idx:  2886\n",
            "sample num:   36, data_idx:  1134\n",
            "sample num:   37, data_idx:   992\n",
            "sample num:   38, data_idx:  1852\n",
            "sample num:   39, data_idx:  5230\n",
            "sample num:   40, data_idx:   386\n",
            "sample num:   41, data_idx:  2428\n",
            "sample num:   42, data_idx:  4531\n",
            "sample num:   43, data_idx:  1279\n",
            "sample num:   44, data_idx:  3111\n",
            "sample num:   45, data_idx:  4948\n",
            "sample num:   46, data_idx:  3164\n",
            "sample num:   47, data_idx:  2952\n",
            "sample num:   48, data_idx:   650\n",
            "sample num:   49, data_idx:  1811\n",
            "sample num:   50, data_idx:  3162\n",
            "sample num:   51, data_idx:   992\n",
            "sample num:   52, data_idx:  3401\n",
            "sample num:   53, data_idx:  1458\n",
            "sample num:   54, data_idx:  1183\n",
            "sample num:   55, data_idx:  4241\n",
            "sample num:   56, data_idx:   514\n",
            "sample num:   57, data_idx:  2490\n",
            "sample num:   58, data_idx:  4280\n",
            "sample num:   59, data_idx:   912\n",
            "sample num:   60, data_idx:  3766\n",
            "sample num:   61, data_idx:  4585\n",
            "sample num:   62, data_idx:  5382\n",
            "sample num:   63, data_idx:   262\n",
            "sample num:   64, data_idx:   576\n",
            "sample num:   65, data_idx:  1311\n",
            "sample num:   66, data_idx:  4238\n",
            "sample num:   67, data_idx:  1307\n",
            "sample num:   68, data_idx:  2126\n",
            "sample num:   69, data_idx:  3442\n",
            "sample num:   70, data_idx:  1918\n",
            "sample num:   71, data_idx:  4834\n",
            "sample num:   72, data_idx:  4714\n",
            "sample num:   73, data_idx:  3204\n",
            "sample num:   74, data_idx:  1992\n",
            "sample num:   75, data_idx:  3558\n",
            "sample num:   76, data_idx:  3959\n",
            "sample num:   77, data_idx:  1652\n",
            "sample num:   78, data_idx:   620\n",
            "sample num:   79, data_idx:   504\n",
            "sample num:   80, data_idx:  1345\n",
            "sample num:   81, data_idx:  2431\n",
            "sample num:   82, data_idx:    20\n",
            "sample num:   83, data_idx:  1139\n",
            "sample num:   84, data_idx:  1648\n",
            "sample num:   85, data_idx:  1664\n",
            "sample num:   86, data_idx:  5404\n",
            "sample num:   87, data_idx:  1585\n",
            "sample num:   88, data_idx:  4312\n",
            "sample num:   89, data_idx:  1582\n",
            "sample num:   90, data_idx:  4851\n",
            "sample num:   91, data_idx:  4678\n",
            "sample num:   92, data_idx:    60\n",
            "sample num:   93, data_idx:  5685\n",
            "sample num:   94, data_idx:  4833\n",
            "sample num:   95, data_idx:  2970\n",
            "sample num:   96, data_idx:  1113\n",
            "sample num:   97, data_idx:  2248\n",
            "sample num:   98, data_idx:  5201\n",
            "sample num:   99, data_idx:  4232\n",
            "obs/obs_network_fisher.npy\n",
            "[match_page_by_cost]\n",
            "occupation: 0\n",
            "len(page_list): 0\n",
            "len(network_page_list): 721\n",
            "cost: 0\n",
            "\n",
            "occupation: 1\n",
            "len(page_list): 0\n",
            "len(network_page_list): 721\n",
            "cost: 0\n",
            "\n",
            "occupation: 2\n",
            "len(page_list): 0\n",
            "len(network_page_list): 721\n",
            "cost: 0\n",
            "\n",
            "occupation: 3\n",
            "len(page_list): 0\n",
            "len(network_page_list): 721\n",
            "cost: 0\n",
            "\n",
            "occupation: 4\n",
            "len(page_list): 0\n",
            "len(network_page_list): 721\n",
            "cost: 0\n",
            "\n",
            "occupation: 5\n",
            "len(page_list): 0\n",
            "len(network_page_list): 721\n",
            "cost: 0\n",
            "\n",
            "occupation: 6\n",
            "len(page_list): 0\n",
            "len(network_page_list): 721\n",
            "cost: 0\n",
            "\n",
            "occupation: 7\n",
            "len(page_list): 406\n",
            "len(network_page_list): 721\n",
            "cost: 1.8498628\n",
            "\n",
            "occupation: 8\n",
            "len(page_list): 315\n",
            "len(network_page_list): 315\n",
            "cost: 0.008099677\n",
            "\n",
            "assing_page 165.870 ms\n",
            "[calculate_cost]\n",
            "toal_cost: 1.8579676552762976\n",
            "721 pages allocated for 72012 weights\n",
            "total_network_cost: 482.16070451028645\n",
            "       0-th page\n",
            "     405-th page\n",
            "       0-th page\n",
            "     314-th page\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K5CVS9N1Po_4",
        "outputId": "c0b77de9-94d9-49cf-de0b-b4099b001480"
      },
      "source": [
        "!bash ./joint_optimization.sh"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-th joint optimization\n",
            "get_matching_loss\n",
            "v_train\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "step 0, training accuracy: 0.110000 original loss: 6.908136 matching loss: 23.646179\n",
            "step 0, Validation accuracy: 0.098000\n",
            "step 100, training accuracy: 0.440000 original loss: 6.428722 matching loss: 11.774940\n",
            "step 100, Validation accuracy: 0.460600\n",
            "get new weight for 0.4606\n",
            "step 200, training accuracy: 0.830000 original loss: 5.319136 matching loss: 6.750893\n",
            "step 200, Validation accuracy: 0.846200\n",
            "get new weight for 0.8462\n",
            "step 300, training accuracy: 0.860000 original loss: 5.108892 matching loss: 4.120907\n",
            "step 300, Validation accuracy: 0.897200\n",
            "get new weight for 0.8972\n",
            "step 400, training accuracy: 0.950000 original loss: 4.889930 matching loss: 2.660734\n",
            "step 400, Validation accuracy: 0.917300\n",
            "get new weight for 0.9173\n",
            "step 500, training accuracy: 0.920000 original loss: 4.879127 matching loss: 1.850509\n",
            "step 500, Validation accuracy: 0.928600\n",
            "get new weight for 0.9286\n",
            "step 600, training accuracy: 0.960000 original loss: 4.822653 matching loss: 1.408439\n",
            "step 600, Validation accuracy: 0.933800\n",
            "get new weight for 0.9338\n",
            "step 700, training accuracy: 0.970000 original loss: 4.811711 matching loss: 1.169810\n",
            "step 700, Validation accuracy: 0.939900\n",
            "get new weight for 0.9399\n",
            "step 800, training accuracy: 0.930000 original loss: 4.850423 matching loss: 1.038219\n",
            "step 800, Validation accuracy: 0.943800\n",
            "get new weight for 0.9438\n",
            "step 900, training accuracy: 0.930000 original loss: 5.029294 matching loss: 0.961698\n",
            "step 900, Validation accuracy: 0.948400\n",
            "get new weight for 0.9484\n",
            "step 1000, training accuracy: 0.960000 original loss: 4.795759 matching loss: 0.913484\n",
            "step 1000, Validation accuracy: 0.949000\n",
            "get new weight for 0.949\n",
            "step 1100, training accuracy: 0.970000 original loss: 4.764738 matching loss: 0.880375\n",
            "step 1100, Validation accuracy: 0.953200\n",
            "get new weight for 0.9532\n",
            "step 1200, training accuracy: 0.960000 original loss: 4.769519 matching loss: 0.856108\n",
            "step 1200, Validation accuracy: 0.956000\n",
            "get new weight for 0.956\n",
            "step 1300, training accuracy: 0.940000 original loss: 4.795105 matching loss: 0.837605\n",
            "step 1300, Validation accuracy: 0.959000\n",
            "get new weight for 0.959\n",
            "step 1400, training accuracy: 0.940000 original loss: 4.842044 matching loss: 0.823038\n",
            "step 1400, Validation accuracy: 0.957800\n",
            "step 1500, training accuracy: 0.990000 original loss: 4.707203 matching loss: 0.811424\n",
            "step 1500, Validation accuracy: 0.960200\n",
            "get new weight for 0.9602\n",
            "step 1600, training accuracy: 0.980000 original loss: 4.733941 matching loss: 0.801359\n",
            "step 1600, Validation accuracy: 0.965200\n",
            "get new weight for 0.9652\n",
            "step 1700, training accuracy: 0.980000 original loss: 4.724785 matching loss: 0.792961\n",
            "step 1700, Validation accuracy: 0.964000\n",
            "step 1800, training accuracy: 0.990000 original loss: 4.685941 matching loss: 0.786441\n",
            "step 1800, Validation accuracy: 0.965200\n",
            "step 1900, training accuracy: 0.970000 original loss: 4.766644 matching loss: 0.780527\n",
            "step 1900, Validation accuracy: 0.966300\n",
            "get new weight for 0.9663\n",
            "step 1999, training accuracy: 0.990000 original loss: 4.671009 matching loss: 0.775695\n",
            "step 1999, Validation accuracy: 0.965900\n",
            "mnist/mnist_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.000000 original loss: 8.161730 matching loss: 16.617659\n",
            "step 0, Validation accuracy: 0.038528\n",
            "step 100, training accuracy: 0.050000 original loss: 8.101689 matching loss: 7.621521\n",
            "step 100, Validation accuracy: 0.036347\n",
            "step 200, training accuracy: 0.060000 original loss: 8.016202 matching loss: 4.352705\n",
            "step 200, Validation accuracy: 0.082962\n",
            "get new weight for 0.08296229\n",
            "step 300, training accuracy: 0.220000 original loss: 7.425772 matching loss: 3.018557\n",
            "step 300, Validation accuracy: 0.215084\n",
            "get new weight for 0.21508405\n",
            "step 400, training accuracy: 0.300000 original loss: 7.093395 matching loss: 2.442392\n",
            "step 400, Validation accuracy: 0.314948\n",
            "get new weight for 0.31494775\n",
            "step 500, training accuracy: 0.390000 original loss: 6.832417 matching loss: 2.170810\n",
            "step 500, Validation accuracy: 0.385825\n",
            "get new weight for 0.38582462\n",
            "step 600, training accuracy: 0.420000 original loss: 6.617156 matching loss: 2.025252\n",
            "step 600, Validation accuracy: 0.445434\n",
            "get new weight for 0.44543388\n",
            "step 700, training accuracy: 0.510000 original loss: 6.540614 matching loss: 1.940769\n",
            "step 700, Validation accuracy: 0.461517\n",
            "get new weight for 0.46151748\n",
            "step 800, training accuracy: 0.520000 original loss: 6.339669 matching loss: 1.889475\n",
            "step 800, Validation accuracy: 0.489505\n",
            "get new weight for 0.48950478\n",
            "step 900, training accuracy: 0.620000 original loss: 6.160292 matching loss: 1.856777\n",
            "step 900, Validation accuracy: 0.511131\n",
            "get new weight for 0.5111313\n",
            "step 1000, training accuracy: 0.550000 original loss: 6.224907 matching loss: 1.835104\n",
            "step 1000, Validation accuracy: 0.531213\n",
            "get new weight for 0.5312131\n",
            "step 1100, training accuracy: 0.530000 original loss: 6.349139 matching loss: 1.820780\n",
            "step 1100, Validation accuracy: 0.542117\n",
            "get new weight for 0.54211724\n",
            "step 1200, training accuracy: 0.540000 original loss: 6.201436 matching loss: 1.811044\n",
            "step 1200, Validation accuracy: 0.545207\n",
            "get new weight for 0.5452067\n",
            "step 1300, training accuracy: 0.480000 original loss: 6.283035 matching loss: 1.803091\n",
            "step 1300, Validation accuracy: 0.565288\n",
            "get new weight for 0.5652885\n",
            "step 1400, training accuracy: 0.620000 original loss: 6.003675 matching loss: 1.798165\n",
            "step 1400, Validation accuracy: 0.576011\n",
            "get new weight for 0.5760109\n",
            "step 1500, training accuracy: 0.530000 original loss: 6.139400 matching loss: 1.793838\n",
            "step 1500, Validation accuracy: 0.575829\n",
            "step 1600, training accuracy: 0.590000 original loss: 6.086370 matching loss: 1.790558\n",
            "step 1600, Validation accuracy: 0.587915\n",
            "get new weight for 0.5879146\n",
            "step 1700, training accuracy: 0.600000 original loss: 6.126954 matching loss: 1.788715\n",
            "step 1700, Validation accuracy: 0.596002\n",
            "get new weight for 0.5960018\n",
            "step 1800, training accuracy: 0.600000 original loss: 6.181786 matching loss: 1.786633\n",
            "step 1800, Validation accuracy: 0.606815\n",
            "get new weight for 0.6068151\n",
            "step 1900, training accuracy: 0.610000 original loss: 6.212773 matching loss: 1.785156\n",
            "step 1900, Validation accuracy: 0.613176\n",
            "get new weight for 0.6131758\n",
            "step 1999, training accuracy: 0.730000 original loss: 5.902799 matching loss: 1.784202\n",
            "step 1999, Validation accuracy: 0.613812\n",
            "get new weight for 0.6138119\n",
            "gsc/gsc_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.060000 original loss: 9.100127 matching loss: 1.351018\n",
            "step 0, Validation accuracy: 0.052257\n",
            "step 100, training accuracy: 0.080000 original loss: 7.974407 matching loss: 1.061111\n",
            "step 100, Validation accuracy: 0.057007\n",
            "get new weight for 0.057007127\n",
            "step 200, training accuracy: 0.230000 original loss: 7.342291 matching loss: 1.043916\n",
            "step 200, Validation accuracy: 0.228029\n",
            "get new weight for 0.2280285\n",
            "step 300, training accuracy: 0.600000 original loss: 6.198692 matching loss: 1.082151\n",
            "step 300, Validation accuracy: 0.628662\n",
            "get new weight for 0.62866193\n",
            "step 400, training accuracy: 0.710000 original loss: 5.790298 matching loss: 1.080222\n",
            "step 400, Validation accuracy: 0.713539\n",
            "get new weight for 0.7135392\n",
            "step 500, training accuracy: 0.830000 original loss: 5.467731 matching loss: 1.075118\n",
            "step 500, Validation accuracy: 0.781473\n",
            "get new weight for 0.7814727\n",
            "step 600, training accuracy: 0.790000 original loss: 5.490925 matching loss: 1.070741\n",
            "step 600, Validation accuracy: 0.811639\n",
            "get new weight for 0.81163895\n",
            "step 700, training accuracy: 0.820000 original loss: 5.360700 matching loss: 1.066520\n",
            "step 700, Validation accuracy: 0.818211\n",
            "get new weight for 0.8182106\n",
            "step 800, training accuracy: 0.880000 original loss: 5.248821 matching loss: 1.063299\n",
            "step 800, Validation accuracy: 0.825337\n",
            "get new weight for 0.8253365\n",
            "step 900, training accuracy: 0.910000 original loss: 5.151975 matching loss: 1.060463\n",
            "step 900, Validation accuracy: 0.836580\n",
            "get new weight for 0.83657956\n",
            "step 1000, training accuracy: 0.850000 original loss: 5.217893 matching loss: 1.057568\n",
            "step 1000, Validation accuracy: 0.845051\n",
            "get new weight for 0.84505147\n",
            "step 1100, training accuracy: 0.940000 original loss: 5.091258 matching loss: 1.055017\n",
            "step 1100, Validation accuracy: 0.857957\n",
            "get new weight for 0.85795724\n",
            "step 1200, training accuracy: 0.920000 original loss: 5.035288 matching loss: 1.052532\n",
            "step 1200, Validation accuracy: 0.862549\n",
            "get new weight for 0.8625495\n",
            "step 1300, training accuracy: 0.940000 original loss: 5.123837 matching loss: 1.050070\n",
            "step 1300, Validation accuracy: 0.858116\n",
            "step 1400, training accuracy: 0.950000 original loss: 5.004729 matching loss: 1.048468\n",
            "step 1400, Validation accuracy: 0.865717\n",
            "get new weight for 0.8657166\n",
            "step 1500, training accuracy: 0.970000 original loss: 5.112855 matching loss: 1.046165\n",
            "step 1500, Validation accuracy: 0.866508\n",
            "get new weight for 0.8665083\n",
            "step 1600, training accuracy: 0.920000 original loss: 4.994977 matching loss: 1.044603\n",
            "step 1600, Validation accuracy: 0.864608\n",
            "step 1700, training accuracy: 0.910000 original loss: 5.086749 matching loss: 1.042848\n",
            "step 1700, Validation accuracy: 0.871655\n",
            "get new weight for 0.8716548\n",
            "step 1800, training accuracy: 0.930000 original loss: 5.036214 matching loss: 1.041375\n",
            "step 1800, Validation accuracy: 0.865004\n",
            "step 1900, training accuracy: 0.950000 original loss: 5.074601 matching loss: 1.039952\n",
            "step 1900, Validation accuracy: 0.864212\n",
            "step 1999, training accuracy: 0.960000 original loss: 4.965826 matching loss: 1.038360\n",
            "step 1999, Validation accuracy: 0.877910\n",
            "get new weight for 0.8779097\n",
            "gtsrb/gtsrb_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "tcmalloc: large alloc 1228800000 bytes == 0x1f880000 @  0x7ff3963541e7 0x7ff393c9dca1 0x7ff393d079c5 0x7ff393d0855e 0x7ff393da1a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "step 0, training accuracy: 0.100000 original loss: 7.197973 matching loss: 1.172004\n",
            "step 0, Validation accuracy: 0.110700\n",
            "step 100, training accuracy: 0.450000 original loss: 6.123147 matching loss: 0.938168\n",
            "step 100, Validation accuracy: 0.464300\n",
            "get new weight for 0.4643\n",
            "step 200, training accuracy: 0.400000 original loss: 6.167960 matching loss: 0.901876\n",
            "step 200, Validation accuracy: 0.492600\n",
            "get new weight for 0.4926\n",
            "step 300, training accuracy: 0.590000 original loss: 5.916327 matching loss: 0.878987\n",
            "step 300, Validation accuracy: 0.498300\n",
            "get new weight for 0.4983\n",
            "step 400, training accuracy: 0.590000 original loss: 5.989363 matching loss: 0.862924\n",
            "step 400, Validation accuracy: 0.502100\n",
            "get new weight for 0.5021\n",
            "step 500, training accuracy: 0.490000 original loss: 6.015127 matching loss: 0.851124\n",
            "step 500, Validation accuracy: 0.500500\n",
            "step 600, training accuracy: 0.540000 original loss: 5.966082 matching loss: 0.842503\n",
            "step 600, Validation accuracy: 0.514500\n",
            "get new weight for 0.5145\n",
            "step 700, training accuracy: 0.520000 original loss: 5.922680 matching loss: 0.834943\n",
            "step 700, Validation accuracy: 0.510500\n",
            "step 800, training accuracy: 0.610000 original loss: 5.786073 matching loss: 0.828398\n",
            "step 800, Validation accuracy: 0.517600\n",
            "get new weight for 0.5176\n",
            "step 900, training accuracy: 0.570000 original loss: 5.896418 matching loss: 0.824530\n",
            "step 900, Validation accuracy: 0.524000\n",
            "get new weight for 0.524\n",
            "step 1000, training accuracy: 0.490000 original loss: 6.061164 matching loss: 0.820102\n",
            "step 1000, Validation accuracy: 0.517400\n",
            "step 1100, training accuracy: 0.550000 original loss: 5.782859 matching loss: 0.816226\n",
            "step 1100, Validation accuracy: 0.534300\n",
            "get new weight for 0.5343\n",
            "step 1200, training accuracy: 0.610000 original loss: 5.857477 matching loss: 0.812291\n",
            "step 1200, Validation accuracy: 0.533600\n",
            "step 1300, training accuracy: 0.420000 original loss: 6.110744 matching loss: 0.809407\n",
            "step 1300, Validation accuracy: 0.526600\n",
            "step 1400, training accuracy: 0.580000 original loss: 5.816230 matching loss: 0.806744\n",
            "step 1400, Validation accuracy: 0.537900\n",
            "get new weight for 0.5379\n",
            "step 1500, training accuracy: 0.560000 original loss: 5.892382 matching loss: 0.804373\n",
            "step 1500, Validation accuracy: 0.540900\n",
            "get new weight for 0.5409\n",
            "step 1600, training accuracy: 0.480000 original loss: 5.968727 matching loss: 0.802652\n",
            "step 1600, Validation accuracy: 0.537400\n",
            "step 1700, training accuracy: 0.610000 original loss: 5.719062 matching loss: 0.800992\n",
            "step 1700, Validation accuracy: 0.534300\n",
            "step 1800, training accuracy: 0.560000 original loss: 5.868605 matching loss: 0.798849\n",
            "step 1800, Validation accuracy: 0.542800\n",
            "get new weight for 0.5428\n",
            "step 1900, training accuracy: 0.650000 original loss: 5.808925 matching loss: 0.797232\n",
            "step 1900, Validation accuracy: 0.538500\n",
            "step 1999, training accuracy: 0.530000 original loss: 5.823369 matching loss: 0.795629\n",
            "step 1999, Validation accuracy: 0.534400\n",
            "cifar10/cifar10_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.120000 original loss: 7.429467 matching loss: 0.196063\n",
            "step 0, Validation accuracy: 0.138483\n",
            "step 100, training accuracy: 0.850000 original loss: 5.510912 matching loss: 0.161761\n",
            "step 100, Validation accuracy: 0.752459\n",
            "get new weight for 0.7524585\n",
            "step 200, training accuracy: 0.810000 original loss: 5.500884 matching loss: 0.153633\n",
            "step 200, Validation accuracy: 0.773049\n",
            "get new weight for 0.7730486\n",
            "step 300, training accuracy: 0.820000 original loss: 5.342404 matching loss: 0.147036\n",
            "step 300, Validation accuracy: 0.789298\n",
            "get new weight for 0.78929776\n",
            "step 400, training accuracy: 0.830000 original loss: 5.331624 matching loss: 0.141792\n",
            "step 400, Validation accuracy: 0.799285\n",
            "get new weight for 0.7992855\n",
            "step 500, training accuracy: 0.810000 original loss: 5.332761 matching loss: 0.137886\n",
            "step 500, Validation accuracy: 0.810310\n",
            "get new weight for 0.81031036\n",
            "step 600, training accuracy: 0.840000 original loss: 5.280345 matching loss: 0.134264\n",
            "step 600, Validation accuracy: 0.808159\n",
            "step 700, training accuracy: 0.850000 original loss: 5.306737 matching loss: 0.131739\n",
            "step 700, Validation accuracy: 0.812769\n",
            "get new weight for 0.8127689\n",
            "step 800, training accuracy: 0.830000 original loss: 5.217870 matching loss: 0.129106\n",
            "step 800, Validation accuracy: 0.818224\n",
            "get new weight for 0.8182237\n",
            "step 900, training accuracy: 0.800000 original loss: 5.407910 matching loss: 0.126980\n",
            "step 900, Validation accuracy: 0.817609\n",
            "step 1000, training accuracy: 0.790000 original loss: 5.427209 matching loss: 0.125089\n",
            "step 1000, Validation accuracy: 0.820337\n",
            "get new weight for 0.8203365\n",
            "step 1100, training accuracy: 0.810000 original loss: 5.324554 matching loss: 0.123177\n",
            "step 1100, Validation accuracy: 0.818954\n",
            "step 1200, training accuracy: 0.810000 original loss: 5.229410 matching loss: 0.121521\n",
            "step 1200, Validation accuracy: 0.820529\n",
            "get new weight for 0.82052857\n",
            "step 1300, training accuracy: 0.790000 original loss: 5.256465 matching loss: 0.120105\n",
            "step 1300, Validation accuracy: 0.821681\n",
            "get new weight for 0.821681\n",
            "step 1400, training accuracy: 0.860000 original loss: 5.123026 matching loss: 0.118683\n",
            "step 1400, Validation accuracy: 0.823679\n",
            "get new weight for 0.82367855\n",
            "step 1500, training accuracy: 0.870000 original loss: 5.168230 matching loss: 0.117456\n",
            "step 1500, Validation accuracy: 0.825330\n",
            "get new weight for 0.8253304\n",
            "step 1600, training accuracy: 0.820000 original loss: 5.218414 matching loss: 0.116452\n",
            "step 1600, Validation accuracy: 0.825676\n",
            "get new weight for 0.8256761\n",
            "step 1700, training accuracy: 0.830000 original loss: 5.250937 matching loss: 0.115460\n",
            "step 1700, Validation accuracy: 0.822795\n",
            "step 1800, training accuracy: 0.850000 original loss: 5.175395 matching loss: 0.114523\n",
            "step 1800, Validation accuracy: 0.828250\n",
            "get new weight for 0.8282499\n",
            "step 1900, training accuracy: 0.870000 original loss: 5.120961 matching loss: 0.113755\n",
            "step 1900, Validation accuracy: 0.830055\n",
            "get new weight for 0.8300553\n",
            "step 1999, training accuracy: 0.840000 original loss: 5.118798 matching loss: 0.113154\n",
            "step 1999, Validation accuracy: 0.824601\n",
            "svhn/svhn_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "tcmalloc: large alloc 1914986496 bytes == 0x1fd7e000 @  0x7f4e8c5bf1e7 0x7f4e89f08ca1 0x7f4e89f729c5 0x7f4e89f7355e 0x7f4e8a00ca6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "step 0, training accuracy: 0.050000 original loss: 7.194140 matching loss: 0.767111\n",
            "step 0, Validation accuracy: 0.081221\n",
            "step 100, training accuracy: 0.470000 original loss: 6.171716 matching loss: 0.635593\n",
            "step 100, Validation accuracy: 0.351526\n",
            "get new weight for 0.35152635\n",
            "step 200, training accuracy: 0.500000 original loss: 6.021143 matching loss: 0.627652\n",
            "step 200, Validation accuracy: 0.327660\n",
            "step 300, training accuracy: 0.550000 original loss: 5.912767 matching loss: 0.621454\n",
            "step 300, Validation accuracy: 0.387049\n",
            "get new weight for 0.38704902\n",
            "step 400, training accuracy: 0.680000 original loss: 5.757800 matching loss: 0.618098\n",
            "step 400, Validation accuracy: 0.381499\n",
            "step 500, training accuracy: 0.660000 original loss: 5.852400 matching loss: 0.615777\n",
            "step 500, Validation accuracy: 0.391489\n",
            "get new weight for 0.39148936\n",
            "step 600, training accuracy: 0.540000 original loss: 5.758728 matching loss: 0.613813\n",
            "step 600, Validation accuracy: 0.391119\n",
            "step 700, training accuracy: 0.700000 original loss: 5.839600 matching loss: 0.612796\n",
            "step 700, Validation accuracy: 0.387419\n",
            "step 800, training accuracy: 0.730000 original loss: 5.697527 matching loss: 0.611635\n",
            "step 800, Validation accuracy: 0.414986\n",
            "get new weight for 0.41498613\n",
            "step 900, training accuracy: 0.750000 original loss: 5.632544 matching loss: 0.611490\n",
            "step 900, Validation accuracy: 0.416096\n",
            "get new weight for 0.4160962\n",
            "step 1000, training accuracy: 0.700000 original loss: 5.648666 matching loss: 0.610671\n",
            "step 1000, Validation accuracy: 0.426642\n",
            "get new weight for 0.426642\n",
            "step 1100, training accuracy: 0.610000 original loss: 5.751158 matching loss: 0.610482\n",
            "step 1100, Validation accuracy: 0.412396\n",
            "step 1200, training accuracy: 0.640000 original loss: 5.692476 matching loss: 0.609171\n",
            "step 1200, Validation accuracy: 0.376688\n",
            "step 1300, training accuracy: 0.680000 original loss: 5.617882 matching loss: 0.608990\n",
            "step 1300, Validation accuracy: 0.397965\n",
            "step 1400, training accuracy: 0.740000 original loss: 5.540179 matching loss: 0.608888\n",
            "step 1400, Validation accuracy: 0.418871\n",
            "step 1500, training accuracy: 0.770000 original loss: 5.466356 matching loss: 0.608673\n",
            "step 1500, Validation accuracy: 0.400925\n",
            "step 1600, training accuracy: 0.600000 original loss: 5.705627 matching loss: 0.608576\n",
            "step 1600, Validation accuracy: 0.398890\n",
            "step 1700, training accuracy: 0.700000 original loss: 5.564889 matching loss: 0.608165\n",
            "step 1700, Validation accuracy: 0.403145\n",
            "step 1800, training accuracy: 0.730000 original loss: 5.549002 matching loss: 0.608633\n",
            "step 1800, Validation accuracy: 0.390934\n",
            "step 1900, training accuracy: 0.750000 original loss: 5.563614 matching loss: 0.608204\n",
            "step 1900, Validation accuracy: 0.423497\n",
            "step 1999, training accuracy: 0.720000 original loss: 5.531621 matching loss: 0.608088\n",
            "step 1999, Validation accuracy: 0.419426\n",
            "us8k/us8k_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.140000 original loss: 7.172507 overlapping loss: 0.314713\n",
            "step 0, Validation accuracy: 0.118200\n",
            "step 100, training accuracy: 0.720000 original loss: 5.494143 overlapping loss: 0.272777\n",
            "step 100, Validation accuracy: 0.786600\n",
            "get new weight for 0.7866\n",
            "step 200, training accuracy: 0.800000 original loss: 5.311924 overlapping loss: 0.265465\n",
            "step 200, Validation accuracy: 0.799900\n",
            "get new weight for 0.7999\n",
            "step 300, training accuracy: 0.830000 original loss: 5.139337 overlapping loss: 0.260922\n",
            "step 300, Validation accuracy: 0.821000\n",
            "get new weight for 0.821\n",
            "step 400, training accuracy: 0.790000 original loss: 5.126062 overlapping loss: 0.257233\n",
            "step 400, Validation accuracy: 0.830800\n",
            "get new weight for 0.8308\n",
            "step 500, training accuracy: 0.860000 original loss: 5.055066 overlapping loss: 0.254971\n",
            "step 500, Validation accuracy: 0.835600\n",
            "get new weight for 0.8356\n",
            "step 600, training accuracy: 0.880000 original loss: 5.028499 overlapping loss: 0.252682\n",
            "step 600, Validation accuracy: 0.838600\n",
            "get new weight for 0.8386\n",
            "step 700, training accuracy: 0.850000 original loss: 5.024564 overlapping loss: 0.250468\n",
            "step 700, Validation accuracy: 0.840800\n",
            "get new weight for 0.8408\n",
            "step 800, training accuracy: 0.820000 original loss: 5.128602 overlapping loss: 0.248618\n",
            "step 800, Validation accuracy: 0.840500\n",
            "step 900, training accuracy: 0.920000 original loss: 4.889703 overlapping loss: 0.247771\n",
            "step 900, Validation accuracy: 0.848800\n",
            "get new weight for 0.8488\n",
            "step 1000, training accuracy: 0.830000 original loss: 5.089837 overlapping loss: 0.246356\n",
            "step 1000, Validation accuracy: 0.846600\n",
            "step 1100, training accuracy: 0.770000 original loss: 5.214576 overlapping loss: 0.245560\n",
            "step 1100, Validation accuracy: 0.852000\n",
            "get new weight for 0.852\n",
            "step 1200, training accuracy: 0.830000 original loss: 5.042150 overlapping loss: 0.244671\n",
            "step 1200, Validation accuracy: 0.843600\n",
            "step 1300, training accuracy: 0.820000 original loss: 5.043039 overlapping loss: 0.243417\n",
            "step 1300, Validation accuracy: 0.850700\n",
            "step 1400, training accuracy: 0.890000 original loss: 5.004617 overlapping loss: 0.242680\n",
            "step 1400, Validation accuracy: 0.857100\n",
            "get new weight for 0.8571\n",
            "step 1500, training accuracy: 0.910000 original loss: 4.942611 overlapping loss: 0.241527\n",
            "step 1500, Validation accuracy: 0.849500\n",
            "step 1600, training accuracy: 0.880000 original loss: 5.052157 overlapping loss: 0.241079\n",
            "step 1600, Validation accuracy: 0.852400\n",
            "step 1700, training accuracy: 0.870000 original loss: 5.015924 overlapping loss: 0.240296\n",
            "step 1700, Validation accuracy: 0.852000\n",
            "step 1800, training accuracy: 0.830000 original loss: 5.060613 overlapping loss: 0.240104\n",
            "step 1800, Validation accuracy: 0.855700\n",
            "step 1900, training accuracy: 0.910000 original loss: 4.914492 overlapping loss: 0.239595\n",
            "step 1900, Validation accuracy: 0.858800\n",
            "get new weight for 0.8588\n",
            "step 1999, training accuracy: 0.890000 original loss: 4.940089 overlapping loss: 0.239552\n",
            "step 1999, Validation accuracy: 0.853000\n",
            "fmnist/fmnist_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "tcmalloc: large alloc 2286338048 bytes == 0x20122000 @  0x7fba096df1e7 0x7fba07028ca1 0x7fba070929c5 0x7fba0709355e 0x7fba0712ca6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "step 0, training accuracy: 0.060000 original loss: 6.881091 overlapping loss: 0.230181\n",
            "step 0, Validation accuracy: 0.193630\n",
            "step 100, training accuracy: 0.560000 original loss: 5.743619 overlapping loss: 0.203552\n",
            "step 100, Validation accuracy: 0.368818\n",
            "get new weight for 0.3688181\n",
            "step 200, training accuracy: 0.570000 original loss: 5.561863 overlapping loss: 0.212909\n",
            "step 200, Validation accuracy: 0.481140\n",
            "get new weight for 0.48114\n",
            "step 300, training accuracy: 0.770000 original loss: 5.319056 overlapping loss: 0.219240\n",
            "step 300, Validation accuracy: 0.650461\n",
            "get new weight for 0.650461\n",
            "step 400, training accuracy: 0.800000 original loss: 5.195782 overlapping loss: 0.221771\n",
            "step 400, Validation accuracy: 0.585918\n",
            "step 500, training accuracy: 0.870000 original loss: 5.112202 overlapping loss: 0.222744\n",
            "step 500, Validation accuracy: 0.689019\n",
            "get new weight for 0.68901926\n",
            "step 600, training accuracy: 0.820000 original loss: 5.110168 overlapping loss: 0.222494\n",
            "step 600, Validation accuracy: 0.734283\n",
            "get new weight for 0.7342833\n",
            "step 700, training accuracy: 0.850000 original loss: 5.025432 overlapping loss: 0.223619\n",
            "step 700, Validation accuracy: 0.759430\n",
            "get new weight for 0.75943\n",
            "step 800, training accuracy: 0.810000 original loss: 5.090869 overlapping loss: 0.223913\n",
            "step 800, Validation accuracy: 0.800503\n",
            "get new weight for 0.80050296\n",
            "step 900, training accuracy: 0.850000 original loss: 5.092107 overlapping loss: 0.224539\n",
            "step 900, Validation accuracy: 0.792959\n",
            "step 1000, training accuracy: 0.860000 original loss: 5.011076 overlapping loss: 0.224965\n",
            "step 1000, Validation accuracy: 0.808885\n",
            "get new weight for 0.80888516\n",
            "step 1100, training accuracy: 0.880000 original loss: 4.969814 overlapping loss: 0.225545\n",
            "step 1100, Validation accuracy: 0.842414\n",
            "get new weight for 0.8424141\n",
            "step 1200, training accuracy: 0.880000 original loss: 4.975398 overlapping loss: 0.226250\n",
            "step 1200, Validation accuracy: 0.853311\n",
            "get new weight for 0.853311\n",
            "step 1300, training accuracy: 0.830000 original loss: 4.996510 overlapping loss: 0.226544\n",
            "step 1300, Validation accuracy: 0.784577\n",
            "step 1400, training accuracy: 0.820000 original loss: 4.990698 overlapping loss: 0.226942\n",
            "step 1400, Validation accuracy: 0.838223\n",
            "step 1500, training accuracy: 0.920000 original loss: 4.899446 overlapping loss: 0.226961\n",
            "step 1500, Validation accuracy: 0.852473\n",
            "step 1600, training accuracy: 0.860000 original loss: 4.982945 overlapping loss: 0.227358\n",
            "step 1600, Validation accuracy: 0.836547\n",
            "step 1700, training accuracy: 0.890000 original loss: 4.976050 overlapping loss: 0.227956\n",
            "step 1700, Validation accuracy: 0.867561\n",
            "get new weight for 0.86756074\n",
            "step 1800, training accuracy: 0.900000 original loss: 4.948290 overlapping loss: 0.228236\n",
            "step 1800, Validation accuracy: 0.860017\n",
            "step 1900, training accuracy: 0.930000 original loss: 4.839001 overlapping loss: 0.228432\n",
            "step 1900, Validation accuracy: 0.865046\n",
            "step 1999, training accuracy: 0.920000 original loss: 4.892290 overlapping loss: 0.228158\n",
            "step 1999, Validation accuracy: 0.863370\n",
            "hhar/hhar_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.250000 original loss: 6.882494 overlapping loss: 0.130706\n",
            "step 0, Validation accuracy: 0.165242\n",
            "step 100, training accuracy: 0.520000 original loss: 5.970051 overlapping loss: 0.118528\n",
            "step 100, Validation accuracy: 0.521368\n",
            "get new weight for 0.52136755\n",
            "step 200, training accuracy: 0.520000 original loss: 5.850939 overlapping loss: 0.121639\n",
            "step 200, Validation accuracy: 0.571225\n",
            "get new weight for 0.57122505\n",
            "step 300, training accuracy: 0.670000 original loss: 5.712535 overlapping loss: 0.123309\n",
            "step 300, Validation accuracy: 0.626781\n",
            "get new weight for 0.6267806\n",
            "step 400, training accuracy: 0.640000 original loss: 5.649178 overlapping loss: 0.123729\n",
            "step 400, Validation accuracy: 0.649573\n",
            "get new weight for 0.6495727\n",
            "step 500, training accuracy: 0.650000 original loss: 5.554787 overlapping loss: 0.125371\n",
            "step 500, Validation accuracy: 0.662393\n",
            "get new weight for 0.66239315\n",
            "step 600, training accuracy: 0.710000 original loss: 5.448822 overlapping loss: 0.126265\n",
            "step 600, Validation accuracy: 0.685185\n",
            "get new weight for 0.6851852\n",
            "step 700, training accuracy: 0.720000 original loss: 5.533196 overlapping loss: 0.127309\n",
            "step 700, Validation accuracy: 0.672365\n",
            "step 800, training accuracy: 0.790000 original loss: 5.348278 overlapping loss: 0.128070\n",
            "step 800, Validation accuracy: 0.695157\n",
            "get new weight for 0.6951567\n",
            "step 900, training accuracy: 0.820000 original loss: 5.303404 overlapping loss: 0.129267\n",
            "step 900, Validation accuracy: 0.709402\n",
            "get new weight for 0.7094017\n",
            "step 1000, training accuracy: 0.760000 original loss: 5.381357 overlapping loss: 0.130393\n",
            "step 1000, Validation accuracy: 0.703704\n",
            "step 1100, training accuracy: 0.860000 original loss: 5.235289 overlapping loss: 0.131493\n",
            "step 1100, Validation accuracy: 0.698006\n",
            "step 1200, training accuracy: 0.830000 original loss: 5.171600 overlapping loss: 0.132015\n",
            "step 1200, Validation accuracy: 0.707977\n",
            "step 1300, training accuracy: 0.900000 original loss: 5.121794 overlapping loss: 0.132935\n",
            "step 1300, Validation accuracy: 0.709402\n",
            "step 1400, training accuracy: 0.790000 original loss: 5.264533 overlapping loss: 0.133772\n",
            "step 1400, Validation accuracy: 0.730769\n",
            "get new weight for 0.7307692\n",
            "step 1500, training accuracy: 0.840000 original loss: 5.203156 overlapping loss: 0.134813\n",
            "step 1500, Validation accuracy: 0.735043\n",
            "get new weight for 0.73504275\n",
            "step 1600, training accuracy: 0.780000 original loss: 5.240239 overlapping loss: 0.135258\n",
            "step 1600, Validation accuracy: 0.736467\n",
            "get new weight for 0.73646724\n",
            "step 1700, training accuracy: 0.900000 original loss: 5.138908 overlapping loss: 0.136145\n",
            "step 1700, Validation accuracy: 0.730769\n",
            "step 1800, training accuracy: 0.830000 original loss: 5.197111 overlapping loss: 0.137219\n",
            "step 1800, Validation accuracy: 0.732194\n",
            "step 1900, training accuracy: 0.850000 original loss: 5.235605 overlapping loss: 0.138481\n",
            "step 1900, Validation accuracy: 0.735043\n",
            "step 1999, training accuracy: 0.880000 original loss: 5.085695 overlapping loss: 0.138932\n",
            "step 1999, Validation accuracy: 0.735043\n",
            "esc10/esc10_weight.npy\n",
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.170000 original loss: 20.548557 overlapping loss: 0.148953\n",
            "step 0, Validation accuracy: 0.197198\n",
            "step 100, training accuracy: 0.870000 original loss: 5.001563 overlapping loss: 0.144959\n",
            "step 100, Validation accuracy: 0.872855\n",
            "get new weight for 0.87285465\n",
            "step 200, training accuracy: 0.900000 original loss: 4.956614 overlapping loss: 0.137271\n",
            "step 200, Validation accuracy: 0.916637\n",
            "get new weight for 0.9166375\n",
            "step 300, training accuracy: 0.900000 original loss: 4.881775 overlapping loss: 0.133279\n",
            "step 300, Validation accuracy: 0.925394\n",
            "get new weight for 0.92539406\n",
            "step 400, training accuracy: 0.960000 original loss: 4.834137 overlapping loss: 0.130320\n",
            "step 400, Validation accuracy: 0.954466\n",
            "get new weight for 0.95446587\n",
            "step 500, training accuracy: 0.960000 original loss: 4.764930 overlapping loss: 0.127610\n",
            "step 500, Validation accuracy: 0.941156\n",
            "step 600, training accuracy: 0.970000 original loss: 4.727645 overlapping loss: 0.125656\n",
            "step 600, Validation accuracy: 0.968476\n",
            "get new weight for 0.96847636\n",
            "step 700, training accuracy: 1.000000 original loss: 4.728739 overlapping loss: 0.124024\n",
            "step 700, Validation accuracy: 0.974781\n",
            "get new weight for 0.9747811\n",
            "step 800, training accuracy: 0.960000 original loss: 4.782866 overlapping loss: 0.122208\n",
            "step 800, Validation accuracy: 0.973380\n",
            "step 900, training accuracy: 0.990000 original loss: 4.737497 overlapping loss: 0.121181\n",
            "step 900, Validation accuracy: 0.981436\n",
            "get new weight for 0.9814361\n",
            "step 1000, training accuracy: 0.980000 original loss: 4.774446 overlapping loss: 0.119987\n",
            "step 1000, Validation accuracy: 0.971278\n",
            "step 1100, training accuracy: 0.990000 original loss: 4.720085 overlapping loss: 0.118060\n",
            "step 1100, Validation accuracy: 0.985639\n",
            "get new weight for 0.9856392\n",
            "step 1200, training accuracy: 0.990000 original loss: 4.736935 overlapping loss: 0.116908\n",
            "step 1200, Validation accuracy: 0.980035\n",
            "step 1300, training accuracy: 0.960000 original loss: 4.724004 overlapping loss: 0.115148\n",
            "step 1300, Validation accuracy: 0.980736\n",
            "step 1400, training accuracy: 0.990000 original loss: 4.721651 overlapping loss: 0.113741\n",
            "step 1400, Validation accuracy: 0.991243\n",
            "get new weight for 0.9912434\n",
            "step 1500, training accuracy: 0.990000 original loss: 4.673385 overlapping loss: 0.112549\n",
            "step 1500, Validation accuracy: 0.991594\n",
            "get new weight for 0.9915937\n",
            "step 1600, training accuracy: 1.000000 original loss: 4.672751 overlapping loss: 0.111804\n",
            "step 1600, Validation accuracy: 0.993695\n",
            "get new weight for 0.99369526\n",
            "step 1700, training accuracy: 1.000000 original loss: 4.654012 overlapping loss: 0.110188\n",
            "step 1700, Validation accuracy: 0.992995\n",
            "step 1800, training accuracy: 1.000000 original loss: 4.659369 overlapping loss: 0.109105\n",
            "step 1800, Validation accuracy: 0.991944\n",
            "step 1900, training accuracy: 1.000000 original loss: 4.680376 overlapping loss: 0.109054\n",
            "step 1900, Validation accuracy: 0.995096\n",
            "get new weight for 0.9950963\n",
            "step 1999, training accuracy: 1.000000 original loss: 4.656367 overlapping loss: 0.107169\n",
            "step 1999, Validation accuracy: 0.994046\n",
            "obs/obs_weight.npy\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.183100\n",
            "GSC performance\n",
            "Inference accuracy: 0.200182\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.083135\n",
            "CIFAR10 performance\n",
            "tcmalloc: large alloc 1228800000 bytes == 0x56a4000 @  0x7f5c1c1c21e7 0x7f5c19b0bca1 0x7f5c19b759c5 0x7f5c19b7655e 0x7f5c19c0fa6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.301200\n",
            "SVHN performance\n",
            "Inference accuracy: 0.489859\n",
            "US8K performance\n",
            "tcmalloc: large alloc 1914986496 bytes == 0x614e000 @  0x7f319d8ed1e7 0x7f319b236ca1 0x7f319b2a09c5 0x7f319b2a155e 0x7f319b33aa6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.300648\n",
            "FMNIST performance\n",
            "Inference accuracy: 0.571600\n",
            "HHAR performance\n",
            "tcmalloc: large alloc 2286338048 bytes == 0x4e86000 @  0x7f35ced941e7 0x7f35cc6ddca1 0x7f35cc7479c5 0x7f35cc74855e 0x7f35cc7e1a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.562448\n",
            "EC10 performance\n",
            "Inference accuracy: 0.673789\n",
            "OBS performance\n",
            "Inference accuracy: 0.995096\n",
            "2-th joint optimization\n",
            "get_matching_loss\n",
            "v_train\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "step 0, training accuracy: 0.100000 original loss: 20.224098 matching loss: 0.047688\n",
            "step 0, Validation accuracy: 0.183100\n",
            "step 100, training accuracy: 0.850000 original loss: 5.265814 matching loss: 0.051190\n",
            "step 100, Validation accuracy: 0.865100\n",
            "get new weight for 0.8651\n",
            "step 200, training accuracy: 0.910000 original loss: 5.085840 matching loss: 0.050768\n",
            "step 200, Validation accuracy: 0.907800\n",
            "get new weight for 0.9078\n",
            "step 300, training accuracy: 0.930000 original loss: 5.033676 matching loss: 0.050745\n",
            "step 300, Validation accuracy: 0.924200\n",
            "get new weight for 0.9242\n",
            "step 400, training accuracy: 0.870000 original loss: 5.158443 matching loss: 0.050904\n",
            "step 400, Validation accuracy: 0.936200\n",
            "get new weight for 0.9362\n",
            "step 500, training accuracy: 0.890000 original loss: 4.992852 matching loss: 0.051094\n",
            "step 500, Validation accuracy: 0.942300\n",
            "get new weight for 0.9423\n",
            "step 600, training accuracy: 0.900000 original loss: 4.979131 matching loss: 0.051386\n",
            "step 600, Validation accuracy: 0.946500\n",
            "get new weight for 0.9465\n",
            "step 700, training accuracy: 0.960000 original loss: 4.863824 matching loss: 0.051545\n",
            "step 700, Validation accuracy: 0.948700\n",
            "get new weight for 0.9487\n",
            "step 800, training accuracy: 0.960000 original loss: 4.793414 matching loss: 0.051598\n",
            "step 800, Validation accuracy: 0.951700\n",
            "get new weight for 0.9517\n",
            "step 900, training accuracy: 0.970000 original loss: 4.819789 matching loss: 0.051496\n",
            "step 900, Validation accuracy: 0.954900\n",
            "get new weight for 0.9549\n",
            "step 1000, training accuracy: 0.950000 original loss: 4.793771 matching loss: 0.051451\n",
            "step 1000, Validation accuracy: 0.957100\n",
            "get new weight for 0.9571\n",
            "step 1100, training accuracy: 0.970000 original loss: 4.796223 matching loss: 0.051422\n",
            "step 1100, Validation accuracy: 0.958700\n",
            "get new weight for 0.9587\n",
            "step 1200, training accuracy: 0.960000 original loss: 4.798314 matching loss: 0.051175\n",
            "step 1200, Validation accuracy: 0.961300\n",
            "get new weight for 0.9613\n",
            "step 1300, training accuracy: 0.950000 original loss: 4.838214 matching loss: 0.051060\n",
            "step 1300, Validation accuracy: 0.960400\n",
            "Traceback (most recent call last):\n",
            "  File \"weight_virtualization.py\", line 1301, in <module>\n",
            "    main(parse_arguments(sys.argv[1:]))\n",
            "  File \"weight_virtualization.py\", line 1172, in main\n",
            "    wv.train_vnn(vnn, args.iter)\n",
            "  File \"weight_virtualization.py\", line 187, in train_vnn\n",
            "    100, iteration, self.get_weight_from_vnn)\n",
            "  File \"/content/NeuralWeightVirtualization/mnist/pintle.py\", line 88, in v_train\n",
            "    input_data, labels = next_batch(train_set, batch_size)\n",
            "  File \"/content/NeuralWeightVirtualization/mnist/pintle.py\", line 11, in next_batch\n",
            "    data_num = np.random.choice(data.shape[0], size=batch_size, replace=False)\n",
            "KeyboardInterrupt\n",
            "get_matching_loss\n",
            "v_train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-6-327185dcf352>\", line 1, in <module>\n",
            "    get_ipython().system('bash ./joint_optimization.sh')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\", line 102, in system\n",
            "    output = _system_commands._system_compat(self, *args, **kwargs)  # pylint:disable=protected-access\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\", line 440, in _system_compat\n",
            "    shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\", line 195, in _run_command\n",
            "    return _monitor_process(parent_pty, epoll, p, cmd, update_stdin_widget)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\", line 222, in _monitor_process\n",
            "    result = _poll_process(parent_pty, epoll, p, cmd, decoder, state)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\", line 269, in _poll_process\n",
            "    events = epoll.poll()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 742, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 395, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 428, in _joinrealpath\n",
            "    newpath = join(path, name)\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 80, in join\n",
            "    a = os.fspath(a)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZG9nw21t6of"
      },
      "source": [
        "import time\n",
        "beginning = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud4gWexMV3HL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cba7749-153c-4c68-83a1-0d619c4ad7d5"
      },
      "source": [
        "# 6-10\n",
        "!bash ./joint_optimization.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.532900\n",
            "GSC performance\n",
            "Inference accuracy: 0.618810\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.670071\n",
            "CIFAR10 performance\n",
            "tcmalloc: large alloc 1228800000 bytes == 0x63a8000 @  0x7fd422c111e7 0x7fd42055aca1 0x7fd4205c49c5 0x7fd4205c555e 0x7fd42065ea6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.519800\n",
            "SVHN performance\n",
            "Inference accuracy: 0.805317\n",
            "US8K performance\n",
            "tcmalloc: large alloc 1914986496 bytes == 0x5920000 @  0x7f6a49e0c1e7 0x7f6a47755ca1 0x7f6a477bf9c5 0x7f6a477c055e 0x7f6a47859a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.347271\n",
            "FMNIST performance\n",
            "Inference accuracy: 0.786900\n",
            "HHAR performance\n",
            "tcmalloc: large alloc 2286338048 bytes == 0x4a96000 @  0x7f29801311e7 0x7f297da7aca1 0x7f297dae49c5 0x7f297dae555e 0x7f297db7ea6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.874267\n",
            "EC10 performance\n",
            "Inference accuracy: 0.774929\n",
            "OBS performance\n",
            "Inference accuracy: 0.112084\n",
            "2-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.650100\n",
            "GSC performance\n",
            "Inference accuracy: 0.652612\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.739588\n",
            "CIFAR10 performance\n",
            "tcmalloc: large alloc 1228800000 bytes == 0x66ec000 @  0x7f97d8e841e7 0x7f97d67cdca1 0x7f97d68379c5 0x7f97d683855e 0x7f97d68d1a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.536600\n",
            "SVHN performance\n",
            "Inference accuracy: 0.815496\n",
            "US8K performance\n",
            "tcmalloc: large alloc 1914986496 bytes == 0x6166000 @  0x7f32ffca51e7 0x7f32fd5eeca1 0x7f32fd6589c5 0x7f32fd65955e 0x7f32fd6f2a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.371138\n",
            "FMNIST performance\n",
            "Inference accuracy: 0.788800\n",
            "HHAR performance\n",
            "tcmalloc: large alloc 2286338048 bytes == 0x5082000 @  0x7f0549bdd1e7 0x7f0547526ca1 0x7f05475909c5 0x7f054759155e 0x7f054762aa6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.858340\n",
            "EC10 performance\n",
            "Inference accuracy: 0.786325\n",
            "OBS performance\n",
            "Inference accuracy: 0.098074\n",
            "3-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.671500\n",
            "GSC performance\n",
            "Inference accuracy: 0.671422\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.755978\n",
            "CIFAR10 performance\n",
            "tcmalloc: large alloc 1228800000 bytes == 0x5106000 @  0x7f82ce6f01e7 0x7f82cc039ca1 0x7f82cc0a39c5 0x7f82cc0a455e 0x7f82cc13da6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.537100\n",
            "SVHN performance\n",
            "Inference accuracy: 0.814805\n",
            "US8K performance\n",
            "tcmalloc: large alloc 1914986496 bytes == 0x58ce000 @  0x7f2867b2d1e7 0x7f2865476ca1 0x7f28654e09c5 0x7f28654e155e 0x7f286557aa6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.376688\n",
            "FMNIST performance\n",
            "Inference accuracy: 0.809800\n",
            "HHAR performance\n",
            "tcmalloc: large alloc 2286338048 bytes == 0x5716000 @  0x7feb0b01b1e7 0x7feb08964ca1 0x7feb089ce9c5 0x7feb089cf55e 0x7feb08a68a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.890193\n",
            "EC10 performance\n",
            "Inference accuracy: 0.776353\n",
            "OBS performance\n",
            "Inference accuracy: 0.085464\n",
            "4-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.098000\n",
            "GSC performance\n",
            "Inference accuracy: 0.014993\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.004751\n",
            "CIFAR10 performance\n",
            "tcmalloc: large alloc 1228800000 bytes == 0x4e2c000 @  0x7f1ca68921e7 0x7f1ca41dbca1 0x7f1ca42459c5 0x7f1ca424655e 0x7f1ca42dfa6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.100000\n",
            "SVHN performance\n",
            "Inference accuracy: 0.066994\n",
            "US8K performance\n",
            "tcmalloc: large alloc 1914986496 bytes == 0x661e000 @  0x7f2138d361e7 0x7f213667fca1 0x7f21366e99c5 0x7f21366ea55e 0x7f2136783a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.103608\n",
            "FMNIST performance\n",
            "Inference accuracy: 0.100000\n",
            "HHAR performance\n",
            "tcmalloc: large alloc 2286338048 bytes == 0x6642000 @  0x7f671ffdc1e7 0x7f671d925ca1 0x7f671d98f9c5 0x7f671d99055e 0x7f671da29a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.098910\n",
            "EC10 performance\n",
            "Inference accuracy: 0.128205\n",
            "OBS performance\n",
            "Inference accuracy: 0.077058\n",
            "5-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.098000\n",
            "GSC performance\n",
            "Inference accuracy: 0.014993\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.004751\n",
            "CIFAR10 performance\n",
            "tcmalloc: large alloc 1228800000 bytes == 0x5084000 @  0x7ff2824b51e7 0x7ff27fdfeca1 0x7ff27fe689c5 0x7ff27fe6955e 0x7ff27ff02a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.100000\n",
            "SVHN performance\n",
            "Inference accuracy: 0.066994\n",
            "US8K performance\n",
            "tcmalloc: large alloc 1914986496 bytes == 0x4e34000 @  0x7f6a4eb091e7 0x7f6a4c452ca1 0x7f6a4c4bc9c5 0x7f6a4c4bd55e 0x7f6a4c556a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.103608\n",
            "FMNIST performance\n",
            "Inference accuracy: 0.100000\n",
            "HHAR performance\n",
            "tcmalloc: large alloc 2286338048 bytes == 0x5a24000 @  0x7f1820c311e7 0x7f181e57aca1 0x7f181e5e49c5 0x7f181e5e555e 0x7f181e67ea6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.098910\n",
            "EC10 performance\n",
            "Inference accuracy: 0.128205\n",
            "OBS performance\n",
            "Inference accuracy: 0.077058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKJVcHDsuGXL",
        "outputId": "8dee045c-1135-4ccd-cfe5-4bf376aab4b0"
      },
      "source": [
        "duration = time.time() - beginning \n",
        "print(\"Time elapsed\", duration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed 3969.3265335559845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZq55fMH_gg4"
      },
      "source": [
        "import time\n",
        "beginning = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UchJZMmaV298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "outputId": "7e89a093-35f0-4f22-d1cc-a012ec0e77ac"
      },
      "source": [
        "# 11-15\n",
        "!bash ./joint_optimization.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-th joint optimization\n",
            "MNIST performance\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Inference accuracy: 0.098000\n",
            "GSC performance\n",
            "Inference accuracy: 0.014993\n",
            "GTSRB performance\n",
            "Inference accuracy: 0.004751\n",
            "CIFAR10 performance\n",
            "tcmalloc: large alloc 1228800000 bytes == 0x4e3c000 @  0x7f831fa1a1e7 0x7f831d363ca1 0x7f831d3cd9c5 0x7f831d3ce55e 0x7f831d467a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.100000\n",
            "SVHN performance\n",
            "Inference accuracy: 0.066994\n",
            "US8K performance\n",
            "tcmalloc: large alloc 1914986496 bytes == 0x5d3c000 @  0x7f80f62061e7 0x7f80f3b4fca1 0x7f80f3bb99c5 0x7f80f3bba55e 0x7f80f3c53a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.103608\n",
            "FMNIST performance\n",
            "Inference accuracy: 0.100000\n",
            "HHAR performance\n",
            "tcmalloc: large alloc 2286338048 bytes == 0x6606000 @  0x7f6404fdd1e7 0x7f6402926ca1 0x7f64029909c5 0x7f640299155e 0x7f6402a2aa6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Inference accuracy: 0.098910\n",
            "EC10 performance\n",
            "Inference accuracy: 0.128205\n",
            "OBS performance\n",
            "Inference accuracy: 0.077058\n",
            "2-th joint optimization\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-85b05f15e468>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 11-15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash ./joint_optimization.sh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m   result = _run_command(\n\u001b[0;32m--> 440\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    441\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    267\u001b[0m   \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m   \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m   \u001b[0minput_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHLBJM2W_hGX"
      },
      "source": [
        "duration = time.time() - beginning \n",
        "print(\"Time elapsed\", duration)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNvcors-V3MA"
      },
      "source": [
        "# 16-20\n",
        "!bash ./joint_optimization.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u9uyO5-V3PL"
      },
      "source": [
        "!python weight_virtualization.py -mode=e -vnn_name=mnist\n",
        "!python weight_virtualization.py -mode=e -vnn_name=gsc\n",
        "!python weight_virtualization.py -mode=e -vnn_name=gtsrb\n",
        "!python weight_virtualization.py -mode=e -vnn_name=cifar10\n",
        "!python weight_virtualization.py -mode=e -vnn_name=svhn\n",
        "!python weight_virtualization.py -mode=e -vnn_name=fmnist\n",
        "!python weight_virtualization.py -mode=e -vnn_name=us8k\n",
        "!python weight_virtualization.py -mode=e -vnn_name=hhar\n",
        "!python weight_virtualization.py -mode=e -vnn_name=esc10\n",
        "!python weight_virtualization.py -mode=e -vnn_name=obs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLIMQhpTt307"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8sHGRaut3x-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKbfMYfVt3vC",
        "outputId": "e96d7500-24f4-424a-dc1d-ebb3a27787cd"
      },
      "source": [
        "!python weight_virtualization.py -mode=t -vnn_name=gsc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "get_matching_loss\n",
            "v_train\n",
            "step 0, training accuracy: 0.040000 original loss: nan matching loss: nan\n",
            "step 0, Validation accuracy: 0.014993\n",
            "step 100, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 100, Validation accuracy: 0.014993\n",
            "step 200, training accuracy: 0.000000 original loss: nan matching loss: nan\n",
            "step 200, Validation accuracy: 0.014993\n",
            "step 300, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 300, Validation accuracy: 0.014993\n",
            "step 400, training accuracy: 0.030000 original loss: nan matching loss: nan\n",
            "step 400, Validation accuracy: 0.014993\n",
            "step 500, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 500, Validation accuracy: 0.014993\n",
            "step 600, training accuracy: 0.030000 original loss: nan matching loss: nan\n",
            "step 600, Validation accuracy: 0.014993\n",
            "step 700, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 700, Validation accuracy: 0.014993\n",
            "step 800, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 800, Validation accuracy: 0.014993\n",
            "step 900, training accuracy: 0.030000 original loss: nan matching loss: nan\n",
            "step 900, Validation accuracy: 0.014993\n",
            "step 1000, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 1000, Validation accuracy: 0.014993\n",
            "step 1100, training accuracy: 0.000000 original loss: nan matching loss: nan\n",
            "step 1100, Validation accuracy: 0.014993\n",
            "step 1200, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 1200, Validation accuracy: 0.014993\n",
            "step 1300, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 1300, Validation accuracy: 0.014993\n",
            "step 1400, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 1400, Validation accuracy: 0.014993\n",
            "step 1500, training accuracy: 0.000000 original loss: nan matching loss: nan\n",
            "step 1500, Validation accuracy: 0.014993\n",
            "step 1600, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 1600, Validation accuracy: 0.014993\n",
            "step 1700, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 1700, Validation accuracy: 0.014993\n",
            "step 1800, training accuracy: 0.000000 original loss: nan matching loss: nan\n",
            "step 1800, Validation accuracy: 0.014993\n",
            "step 1900, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 1900, Validation accuracy: 0.014993\n",
            "step 2000, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 2000, Validation accuracy: 0.014993\n",
            "step 2100, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 2100, Validation accuracy: 0.014993\n",
            "step 2200, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 2200, Validation accuracy: 0.014993\n",
            "step 2300, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 2300, Validation accuracy: 0.014993\n",
            "step 2400, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 2400, Validation accuracy: 0.014993\n",
            "step 2500, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 2500, Validation accuracy: 0.014993\n",
            "step 2600, training accuracy: 0.000000 original loss: nan matching loss: nan\n",
            "step 2600, Validation accuracy: 0.014993\n",
            "step 2700, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 2700, Validation accuracy: 0.014993\n",
            "step 2800, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 2800, Validation accuracy: 0.014993\n",
            "step 2900, training accuracy: 0.030000 original loss: nan matching loss: nan\n",
            "step 2900, Validation accuracy: 0.014993\n",
            "step 3000, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 3000, Validation accuracy: 0.014993\n",
            "step 3100, training accuracy: 0.000000 original loss: nan matching loss: nan\n",
            "step 3100, Validation accuracy: 0.014993\n",
            "step 3200, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 3200, Validation accuracy: 0.014993\n",
            "step 3300, training accuracy: 0.030000 original loss: nan matching loss: nan\n",
            "step 3300, Validation accuracy: 0.014993\n",
            "step 3400, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 3400, Validation accuracy: 0.014993\n",
            "step 3500, training accuracy: 0.000000 original loss: nan matching loss: nan\n",
            "step 3500, Validation accuracy: 0.014993\n",
            "step 3600, training accuracy: 0.040000 original loss: nan matching loss: nan\n",
            "step 3600, Validation accuracy: 0.014993\n",
            "step 3700, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 3700, Validation accuracy: 0.014993\n",
            "step 3800, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 3800, Validation accuracy: 0.014993\n",
            "step 3900, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 3900, Validation accuracy: 0.014993\n",
            "step 4000, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 4000, Validation accuracy: 0.014993\n",
            "step 4100, training accuracy: 0.000000 original loss: nan matching loss: nan\n",
            "step 4100, Validation accuracy: 0.014993\n",
            "step 4200, training accuracy: 0.050000 original loss: nan matching loss: nan\n",
            "step 4200, Validation accuracy: 0.014993\n",
            "step 4300, training accuracy: 0.030000 original loss: nan matching loss: nan\n",
            "step 4300, Validation accuracy: 0.014993\n",
            "step 4400, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 4400, Validation accuracy: 0.014993\n",
            "step 4500, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 4500, Validation accuracy: 0.014993\n",
            "step 4600, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 4600, Validation accuracy: 0.014993\n",
            "step 4700, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 4700, Validation accuracy: 0.014993\n",
            "step 4800, training accuracy: 0.000000 original loss: nan matching loss: nan\n",
            "step 4800, Validation accuracy: 0.014993\n",
            "step 4900, training accuracy: 0.020000 original loss: nan matching loss: nan\n",
            "step 4900, Validation accuracy: 0.014993\n",
            "step 4999, training accuracy: 0.010000 original loss: nan matching loss: nan\n",
            "step 4999, Validation accuracy: 0.014993\n",
            "gsc/gsc_weight.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ_md0h4t3sL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYUh5uywt3pC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDz1mD3KD3wU",
        "outputId": "5efd53f9-dc5c-437a-b13e-f599634074d3"
      },
      "source": [
        "!python in-memory_execute.py | tee -a in_memory_execution_ten_nets_with_mem.txt"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1228800000 bytes == 0x282aa000 @  0x7f48daa531e7 0x7f48d85dcca1 0x7f48d86469c5 0x7f48d864755e 0x7f48d86e0a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "tcmalloc: large alloc 1914986496 bytes == 0x107950000 @  0x7f48daa531e7 0x7f48d85dcca1 0x7f48d86469c5 0x7f48d864755e 0x7f48d86e0a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "tcmalloc: large alloc 2286338048 bytes == 0x7f4593b94000 @  0x7f48daa531e7 0x7f48d85dcca1 0x7f48d86469c5 0x7f48d864755e 0x7f48d86e0a6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "MEMORY USAGE (MB): 267.5859375\n",
            "MEMORY USAGE (MB): 267.5859375\n",
            "MEMORY USAGE (MB): 267.7890625\n",
            "MEMORY USAGE (MB): 267.7890625\n",
            "MEMORY USAGE (MB): 267.7890625\n",
            "virtual_weight address: 139948444352512\n",
            "init virtual_weight 14.012 ms\n",
            "[VNN 0][cifar10] init page table 6.068 ms\n",
            "[VNN 1][esc10] init page table 4.673 ms\n",
            "[VNN 2][fmnist] init page table 4.143 ms\n",
            "[VNN 3][gsc] init page table 4.082 ms\n",
            "[VNN 4][gtsrb] init page table 3.838 ms\n",
            "[VNN 5][hhar] init page table 3.853 ms\n",
            "[VNN 6][mnist] init page table 3.812 ms\n",
            "[VNN 7][obs] init page table 3.632 ms\n",
            "[VNN 8][svhn] init page table 3.966 ms\n",
            "[VNN 9][us8k] init page table 4.170 ms\n",
            "tf.global_variables_initializer 8003.615 ms\n",
            "MEMORY USAGE (MB): 1177.1875\n",
            "MEMORY USAGE (MB): 1177.1875\n",
            "MEMORY USAGE (MB): 2589.87109375\n",
            "[Executing] cifar10\n",
            "weights load time : 0.345 ms\n",
            "DNN execution time: 6877.636 ms\n",
            "Inference accuracy: 0.301200\n",
            "MEMORY USAGE (MB): 4331.14453125\n",
            "MEMORY USAGE (MB): 4331.14453125\n",
            "MEMORY USAGE (MB): 4568.66796875\n",
            "[Executing] esc10\n",
            "weights load time : 0.261 ms\n",
            "DNN execution time: 413.616 ms\n",
            "Inference accuracy: 0.673789\n",
            "MEMORY USAGE (MB): 4568.92578125\n",
            "MEMORY USAGE (MB): 4568.92578125\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "MEMORY USAGE (MB): 4955.3671875\n",
            "[Executing] mnist\n",
            "weights load time : 0.336 ms\n",
            "DNN execution time: 729.600 ms\n",
            "Inference accuracy: 0.183100\n",
            "MEMORY USAGE (MB): 4955.3671875\n",
            "MEMORY USAGE (MB): 4955.3671875\n",
            "MEMORY USAGE (MB): 6973.83203125\n",
            "[Executing] us8k\n",
            "weights load time : 0.377 ms\n",
            "DNN execution time: 990.389 ms\n",
            "Inference accuracy: 0.300648\n",
            "MEMORY USAGE (MB): 7072.3828125\n",
            "MEMORY USAGE (MB): 7072.3828125\n",
            "MEMORY USAGE (MB): 7272.37109375\n",
            "[Executing] gtsrb\n",
            "weights load time : 0.331 ms\n",
            "DNN execution time: 938.180 ms\n",
            "Inference accuracy: 0.083135\n",
            "MEMORY USAGE (MB): 7471.3515625\n",
            "MEMORY USAGE (MB): 7471.3515625\n",
            "MEMORY USAGE (MB): 7838.9296875\n",
            "[Executing] fmnist\n",
            "weights load time : 0.254 ms\n",
            "DNN execution time: 978.579 ms\n",
            "Inference accuracy: 0.571600\n",
            "MEMORY USAGE (MB): 7869.8671875\n",
            "MEMORY USAGE (MB): 7869.8671875\n",
            "MEMORY USAGE (MB): 7869.8671875\n",
            "[Executing] gtsrb\n",
            "weights load time : 0.197 ms\n",
            "DNN execution time: 78.961 ms\n",
            "Inference accuracy: 0.083135\n",
            "MEMORY USAGE (MB): 7921.171875\n",
            "MEMORY USAGE (MB): 7921.171875\n",
            "MEMORY USAGE (MB): 7921.171875\n",
            "[Executing] us8k\n",
            "weights load time : 0.165 ms\n",
            "DNN execution time: 115.510 ms\n",
            "Inference accuracy: 0.300648\n",
            "MEMORY USAGE (MB): 7973.5078125\n",
            "MEMORY USAGE (MB): 7973.5078125\n",
            "MEMORY USAGE (MB): 7973.5078125\n",
            "[Executing] cifar10\n",
            "weights load time : 0.196 ms\n",
            "DNN execution time: 107.122 ms\n",
            "Inference accuracy: 0.301200\n",
            "MEMORY USAGE (MB): 7989.234375\n",
            "MEMORY USAGE (MB): 7989.234375\n",
            "MEMORY USAGE (MB): 10021.171875\n",
            "[Executing] hhar\n",
            "weights load time : 0.266 ms\n",
            "DNN execution time: 581.461 ms\n",
            "Inference accuracy: 0.562448\n",
            "MEMORY USAGE (MB): 10061.390625\n",
            "MEMORY USAGE (MB): 10061.390625\n",
            "MEMORY USAGE (MB): 10375.4296875\n",
            "[Executing] obs\n",
            "weights load time : 0.277 ms\n",
            "DNN execution time: 978.851 ms\n",
            "Inference accuracy: 0.995096\n",
            "MEMORY USAGE (MB): 10415.1328125\n",
            "MEMORY USAGE (MB): 10415.1328125\n",
            "MEMORY USAGE (MB): 10415.1328125\n",
            "[Executing] obs\n",
            "weights load time : 0.204 ms\n",
            "DNN execution time: 73.556 ms\n",
            "Inference accuracy: 0.995096\n",
            "MEMORY USAGE (MB): 10415.1328125\n",
            "MEMORY USAGE (MB): 10415.1328125\n",
            "MEMORY USAGE (MB): 10415.1328125\n",
            "[Executing] gtsrb\n",
            "weights load time : 0.191 ms\n",
            "DNN execution time: 66.589 ms\n",
            "Inference accuracy: 0.083135\n",
            "MEMORY USAGE (MB): 10427.5078125\n",
            "MEMORY USAGE (MB): 10427.5078125\n",
            "MEMORY USAGE (MB): 10427.5078125\n",
            "[Executing] gtsrb\n",
            "weights load time : 0.187 ms\n",
            "DNN execution time: 59.492 ms\n",
            "Inference accuracy: 0.083135\n",
            "MEMORY USAGE (MB): 10427.5078125\n",
            "MEMORY USAGE (MB): 10427.5078125\n",
            "MEMORY USAGE (MB): 10427.5078125\n",
            "[Executing] obs\n",
            "weights load time : 0.225 ms\n",
            "DNN execution time: 72.977 ms\n",
            "Inference accuracy: 0.995096\n",
            "MEMORY USAGE (MB): 10427.5078125\n",
            "MEMORY USAGE (MB): 10427.5078125\n",
            "MEMORY USAGE (MB): 10427.5078125\n",
            "[Executing] us8k\n",
            "weights load time : 0.242 ms\n",
            "DNN execution time: 137.023 ms\n",
            "Inference accuracy: 0.300648\n",
            "MEMORY USAGE (MB): 10529.0859375\n",
            "MEMORY USAGE (MB): 10529.0859375\n",
            "MEMORY USAGE (MB): 10529.0859375\n",
            "[Executing] gtsrb\n",
            "weights load time : 0.214 ms\n",
            "DNN execution time: 61.125 ms\n",
            "Inference accuracy: 0.083135\n",
            "MEMORY USAGE (MB): 10529.0859375\n",
            "MEMORY USAGE (MB): 10529.0859375\n",
            "MEMORY USAGE (MB): 10529.0859375\n",
            "[Executing] fmnist\n",
            "weights load time : 0.210 ms\n",
            "DNN execution time: 75.430 ms\n",
            "Inference accuracy: 0.571600\n",
            "MEMORY USAGE (MB): 10547.90625\n",
            "MEMORY USAGE (MB): 10547.90625\n",
            "MEMORY USAGE (MB): 11351.21484375\n",
            "[Executing] svhn\n",
            "weights load time : 0.400 ms\n",
            "DNN execution time: 1271.885 ms\n",
            "Inference accuracy: 0.489859\n",
            "MEMORY USAGE (MB): 11350.95703125\n",
            "MEMORY USAGE (MB): 11350.95703125\n",
            "MEMORY USAGE (MB): 11350.95703125\n",
            "[Executing] fmnist\n",
            "weights load time : 0.190 ms\n",
            "DNN execution time: 75.253 ms\n",
            "Inference accuracy: 0.571600\n",
            "MEMORY USAGE (MB): 11380.86328125\n",
            "MEMORY USAGE (MB): 11380.86328125\n",
            "MEMORY USAGE (MB): 11380.86328125\n",
            "[Executing] obs\n",
            "weights load time : 0.189 ms\n",
            "DNN execution time: 89.854 ms\n",
            "Inference accuracy: 0.995096\n",
            "MEMORY USAGE (MB): 11404.24609375\n",
            "MEMORY USAGE (MB): 11404.24609375\n",
            "MEMORY USAGE (MB): 11404.24609375\n",
            "[Executing] svhn\n",
            "weights load time : 0.196 ms\n",
            "DNN execution time: 112.931 ms\n",
            "Inference accuracy: 0.489859\n",
            "MEMORY USAGE (MB): 11404.24609375\n",
            "MEMORY USAGE (MB): 11404.24609375\n",
            "MEMORY USAGE (MB): 11703.99609375\n",
            "[Executing] gsc\n",
            "weights load time : 0.496 ms\n",
            "DNN execution time: 860.059 ms\n",
            "Inference accuracy: 0.200182\n",
            "MEMORY USAGE (MB): 11758.63671875\n",
            "MEMORY USAGE (MB): 11758.63671875\n",
            "MEMORY USAGE (MB): 11758.63671875\n",
            "[Executing] esc10\n",
            "weights load time : 0.202 ms\n",
            "DNN execution time: 42.670 ms\n",
            "Inference accuracy: 0.673789\n",
            "MEMORY USAGE (MB): 11762.171875\n",
            "MEMORY USAGE (MB): 11762.171875\n",
            "MEMORY USAGE (MB): 11762.171875\n",
            "[Executing] fmnist\n",
            "weights load time : 0.178 ms\n",
            "DNN execution time: 64.119 ms\n",
            "Inference accuracy: 0.571600\n",
            "MEMORY USAGE (MB): 11762.171875\n",
            "MEMORY USAGE (MB): 11762.171875\n",
            "MEMORY USAGE (MB): 11762.171875\n",
            "[Executing] hhar\n",
            "weights load time : 0.178 ms\n",
            "DNN execution time: 16.957 ms\n",
            "Inference accuracy: 0.562448\n",
            "MEMORY USAGE (MB): 11762.171875\n",
            "MEMORY USAGE (MB): 11762.171875\n",
            "MEMORY USAGE (MB): 11762.171875\n",
            "[Executing] obs\n",
            "weights load time : 0.189 ms\n",
            "DNN execution time: 102.975 ms\n",
            "Inference accuracy: 0.995096\n",
            "MEMORY USAGE (MB): 11762.46484375\n",
            "MEMORY USAGE (MB): 11762.46484375\n",
            "MEMORY USAGE (MB): 11762.46484375\n",
            "[Executing] gtsrb\n",
            "weights load time : 0.203 ms\n",
            "DNN execution time: 67.980 ms\n",
            "Inference accuracy: 0.083135\n",
            "MEMORY USAGE (MB): 11763.234375\n",
            "MEMORY USAGE (MB): 11763.234375\n",
            "MEMORY USAGE (MB): 11763.234375\n",
            "[Executing] gsc\n",
            "weights load time : 0.170 ms\n",
            "DNN execution time: 34.560 ms\n",
            "Inference accuracy: 0.200182\n",
            "MEMORY USAGE (MB): 11763.234375\n",
            "MEMORY USAGE (MB): 11763.234375\n",
            "MEMORY USAGE (MB): 11763.234375\n",
            "[Executing] esc10\n",
            "weights load time : 0.447 ms\n",
            "DNN execution time: 14.793 ms\n",
            "Inference accuracy: 0.673789\n",
            "MEMORY USAGE (MB): 11763.35546875\n",
            "total weights load time : 7.517 ms\n",
            "total DNN execution time: 16090.134 ms\n",
            "MEMORY USAGE (MB): 11760.76953125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSqPWh0ZEFD0",
        "outputId": "06320cfa-51b5-4bb4-a1c5-c3fc438632c6"
      },
      "source": [
        "!python baseline_execute.py | tee -a baseline_execution_ten_nets.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1228800000 bytes == 0xb2d78000 @  0x7f34dfcff1e7 0x7f34dd888ca1 0x7f34dd8f29c5 0x7f34dd8f355e 0x7f34dd98ca6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "tcmalloc: large alloc 2286338048 bytes == 0x156042000 @  0x7f34dfcff1e7 0x7f34dd888ca1 0x7f34dd8f29c5 0x7f34dd8f355e 0x7f34dd98ca6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "tcmalloc: large alloc 1914986496 bytes == 0x7f318db12000 @  0x7f34dfcff1e7 0x7f34dd888ca1 0x7f34dd8f29c5 0x7f34dd8f355e 0x7f34dd98ca6e 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x516069 0x566fae 0x510e51 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "[Executing] mnist\n",
            "weights load time : 83.578 ms\n",
            "DNN execution time: 12664.413 ms\n",
            "Inference accuracy: 0.980800\n",
            "[Executing] fmnist\n",
            "weights load time : 94.949 ms\n",
            "DNN execution time: 787.147 ms\n",
            "Inference accuracy: 0.874900\n",
            "[Executing] cifar10\n",
            "weights load time : 69.775 ms\n",
            "DNN execution time: 515.315 ms\n",
            "Inference accuracy: 0.555200\n",
            "[Executing] svhn\n",
            "weights load time : 129.116 ms\n",
            "DNN execution time: 904.194 ms\n",
            "Inference accuracy: 0.814843\n",
            "[Executing] svhn\n",
            "weights load time : 125.559 ms\n",
            "DNN execution time: 181.548 ms\n",
            "Inference accuracy: 0.814843\n",
            "[Executing] hhar\n",
            "weights load time : 69.093 ms\n",
            "DNN execution time: 313.694 ms\n",
            "Inference accuracy: 0.898575\n",
            "[Executing] gsc\n",
            "weights load time : 57.279 ms\n",
            "DNN execution time: 498.571 ms\n",
            "Inference accuracy: 0.694684\n",
            "[Executing] cifar10\n",
            "weights load time : 73.544 ms\n",
            "DNN execution time: 175.581 ms\n",
            "Inference accuracy: 0.555200\n",
            "[Executing] hhar\n",
            "weights load time : 66.229 ms\n",
            "DNN execution time: 47.177 ms\n",
            "Inference accuracy: 0.898575\n",
            "[Executing] esc10\n",
            "weights load time : 117.893 ms\n",
            "DNN execution time: 173.277 ms\n",
            "Inference accuracy: 0.816239\n",
            "[Executing] hhar\n",
            "weights load time : 63.421 ms\n",
            "DNN execution time: 47.036 ms\n",
            "Inference accuracy: 0.898575\n",
            "[Executing] cifar10\n",
            "weights load time : 68.075 ms\n",
            "DNN execution time: 123.307 ms\n",
            "Inference accuracy: 0.555200\n",
            "[Executing] us8k\n",
            "weights load time : 76.200 ms\n",
            "DNN execution time: 712.786 ms\n",
            "Inference accuracy: 0.439408\n",
            "[Executing] cifar10\n",
            "weights load time : 65.647 ms\n",
            "DNN execution time: 142.164 ms\n",
            "Inference accuracy: 0.555200\n",
            "[Executing] us8k\n",
            "weights load time : 70.658 ms\n",
            "DNN execution time: 130.592 ms\n",
            "Inference accuracy: 0.439408\n",
            "[Executing] obs\n",
            "weights load time : 70.403 ms\n",
            "DNN execution time: 720.139 ms\n",
            "Inference accuracy: 0.999650\n",
            "[Executing] cifar10\n",
            "weights load time : 68.874 ms\n",
            "DNN execution time: 177.049 ms\n",
            "Inference accuracy: 0.555200\n",
            "[Executing] svhn\n",
            "weights load time : 137.030 ms\n",
            "DNN execution time: 195.273 ms\n",
            "Inference accuracy: 0.814843\n",
            "[Executing] svhn\n",
            "weights load time : 133.256 ms\n",
            "DNN execution time: 181.381 ms\n",
            "Inference accuracy: 0.814843\n",
            "[Executing] obs\n",
            "weights load time : 58.821 ms\n",
            "DNN execution time: 102.929 ms\n",
            "Inference accuracy: 0.999650\n",
            "[Executing] gtsrb\n",
            "weights load time : 70.017 ms\n",
            "DNN execution time: 593.631 ms\n",
            "Inference accuracy: 0.928029\n",
            "[Executing] cifar10\n",
            "weights load time : 71.265 ms\n",
            "DNN execution time: 193.442 ms\n",
            "Inference accuracy: 0.555200\n",
            "[Executing] us8k\n",
            "weights load time : 71.930 ms\n",
            "DNN execution time: 136.983 ms\n",
            "Inference accuracy: 0.439408\n",
            "[Executing] esc10\n",
            "weights load time : 131.983 ms\n",
            "DNN execution time: 74.857 ms\n",
            "Inference accuracy: 0.816239\n",
            "[Executing] gsc\n",
            "weights load time : 58.692 ms\n",
            "DNN execution time: 69.143 ms\n",
            "Inference accuracy: 0.694684\n",
            "[Executing] hhar\n",
            "weights load time : 67.203 ms\n",
            "DNN execution time: 50.660 ms\n",
            "Inference accuracy: 0.898575\n",
            "[Executing] fmnist\n",
            "weights load time : 104.593 ms\n",
            "DNN execution time: 136.458 ms\n",
            "Inference accuracy: 0.874900\n",
            "[Executing] mnist\n",
            "weights load time : 67.118 ms\n",
            "DNN execution time: 64.422 ms\n",
            "Inference accuracy: 0.980800\n",
            "[Executing] hhar\n",
            "weights load time : 62.438 ms\n",
            "DNN execution time: 49.685 ms\n",
            "Inference accuracy: 0.898575\n",
            "[Executing] svhn\n",
            "weights load time : 150.128 ms\n",
            "DNN execution time: 217.476 ms\n",
            "Inference accuracy: 0.814843\n",
            "total weights load time : 2554.767 ms\n",
            "total DNN execution time: 20380.330 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBII3iLut3fR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ethum8Dt3Zq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzJDQWLensyi",
        "outputId": "c6db5362-8141-484e-834f-eed7eed66a6d"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XRvw9Nzns3S",
        "outputId": "1e32dcf2-3d3a-4aac-a1c3-449c638e1b91"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralWeightVirtualization  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPK04Aouns7c",
        "outputId": "b15559e8-358c-4f40-853a-04868f707c7b"
      },
      "source": [
        "!zip -r ./NeuralWeightVirtualization.zip ./NeuralWeightVirtualization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: NeuralWeightVirtualization/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/weight_page_occupation.npy (deflated 60%)\n",
            "  adding: NeuralWeightVirtualization/hhar_matching_ten_nets.txt (deflated 78%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation.so (deflated 64%)\n",
            "  adding: NeuralWeightVirtualization/cifar10_data.py (deflated 72%)\n",
            "  adding: NeuralWeightVirtualization/GSC_v2_test_data.npy (deflated 6%)\n",
            "  adding: NeuralWeightVirtualization/cifar10_accuracy_ten_nets.txt (deflated 63%)\n",
            "  adding: NeuralWeightVirtualization/svhn/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/svhn/svhn_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/svhn/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/svhn/svhn_network_fisher.npy (deflated 22%)\n",
            "  adding: NeuralWeightVirtualization/svhn/svhn_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/svhn/svhn.meta (deflated 93%)\n",
            "  adding: NeuralWeightVirtualization/svhn/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/svhn/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/svhn/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/svhn/svhn.data-00000-of-00001 (deflated 8%)\n",
            "  adding: NeuralWeightVirtualization/svhn/svhn.index (deflated 47%)\n",
            "  adding: NeuralWeightVirtualization/svhn/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/hhar_train_label.npy (deflated 98%)\n",
            "  adding: NeuralWeightVirtualization/cifar10.vnn (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/cifar10_test_data.npy (deflated 82%)\n",
            "  adding: NeuralWeightVirtualization/L46_Project_Notebook.ipynb (deflated 86%)\n",
            "  adding: NeuralWeightVirtualization/esc10_test_data.npy (deflated 8%)\n",
            "  adding: NeuralWeightVirtualization/mnist.accuracy (deflated 9%)\n",
            "  adding: NeuralWeightVirtualization/fmnist_matching_ten_nets.txt (deflated 78%)\n",
            "  adding: NeuralWeightVirtualization/fmnist_test_data.npy (deflated 88%)\n",
            "  adding: NeuralWeightVirtualization/virtual_weight_page.npy (deflated 7%)\n",
            "  adding: NeuralWeightVirtualization/svhn_accuracy_ten_nets.txt (deflated 58%)\n",
            "  adding: NeuralWeightVirtualization/mnist_matching_ten_nets.txt (deflated 77%)\n",
            "  adding: NeuralWeightVirtualization/obstacle_train_data.npy (deflated 85%)\n",
            "  adding: NeuralWeightVirtualization/svhn_validation_label.npy (deflated 97%)\n",
            "  adding: NeuralWeightVirtualization/joint_optimization.sh (deflated 84%)\n",
            "  adding: NeuralWeightVirtualization/esc10/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/esc10/esc10.index (deflated 46%)\n",
            "  adding: NeuralWeightVirtualization/esc10/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/esc10/esc10.meta (deflated 93%)\n",
            "  adding: NeuralWeightVirtualization/esc10/esc10.data-00000-of-00001 (deflated 7%)\n",
            "  adding: NeuralWeightVirtualization/esc10/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/esc10/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/esc10/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/esc10/esc10_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/esc10/esc10_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/esc10/esc10_network_fisher.npy (deflated 21%)\n",
            "  adding: NeuralWeightVirtualization/esc10/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/svhn.vnn (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/hhar.accuracy (deflated 21%)\n",
            "  adding: NeuralWeightVirtualization/cifar10_train_label.npy (deflated 98%)\n",
            "  adding: NeuralWeightVirtualization/hhar_test_label.npy (deflated 98%)\n",
            "  adding: NeuralWeightVirtualization/svhn_test_label.npy (deflated 97%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/gtsrb_network_fisher.npy (deflated 22%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/gtsrb.meta (deflated 90%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/gtsrb.data-00000-of-00001 (deflated 7%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/gtsrb_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/gtsrb_network_weight.npy (deflated 26%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/gtsrb.index (deflated 47%)\n",
            "  adding: NeuralWeightVirtualization/GTSRB_data.py (deflated 71%)\n",
            "  adding: NeuralWeightVirtualization/GSC_v2_validation_label.npy (deflated 99%)\n",
            "  adding: NeuralWeightVirtualization/GTSRB_train_label.npy (deflated 99%)\n",
            "  adding: NeuralWeightVirtualization/weight_virtualization.py (deflated 82%)\n",
            "  adding: NeuralWeightVirtualization/obstacle_test_data.npy (deflated 85%)\n",
            "  adding: NeuralWeightVirtualization/MNIST_data/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/MNIST_data/t10k-labels-idx1-ubyte.gz (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/MNIST_data/t10k-images-idx3-ubyte.gz (deflated 0%)\n",
            "  adding: NeuralWeightVirtualization/MNIST_data/train-images-idx3-ubyte.gz (deflated 0%)\n",
            "  adding: NeuralWeightVirtualization/MNIST_data/train-labels-idx1-ubyte.gz (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/obstacle_train_label.npy (deflated 100%)\n",
            "  adding: NeuralWeightVirtualization/US8K_test_label.npy (deflated 100%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb_matching_ten_nets.txt (deflated 77%)\n",
            "  adding: NeuralWeightVirtualization/gsc.accuracy (deflated 19%)\n",
            "  adding: NeuralWeightVirtualization/baseline_execute.py (deflated 62%)\n",
            "  adding: NeuralWeightVirtualization/gsc_accuracy_ten_nets.txt (deflated 56%)\n",
            "  adding: NeuralWeightVirtualization/README.md (deflated 73%)\n",
            "  adding: NeuralWeightVirtualization/cifar10_test_label.npy (deflated 98%)\n",
            "  adding: NeuralWeightVirtualization/GSC_v2_data.py (deflated 70%)\n",
            "  adding: NeuralWeightVirtualization/hhar_test_data.npy (deflated 37%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/GSC_v2_data.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/GTSRB_data.cpython-36.pyc (deflated 47%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/obstacle_data.cpython-36.pyc (deflated 54%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/esc10_data.cpython-36.pyc (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/mnist_data.cpython-36.pyc (deflated 48%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/svhn_data.cpython-36.pyc (deflated 54%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/fmnist_data.cpython-36.pyc (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/hhar_data.cpython-36.pyc (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/cifar10_data.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/us8k_data.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/mnist_accuracy_ten_nets.txt (deflated 88%)\n",
            "  adding: NeuralWeightVirtualization/svhn_train_label.npy (deflated 97%)\n",
            "  adding: NeuralWeightVirtualization/err.log (deflated 89%)\n",
            "  adding: NeuralWeightVirtualization/hhar_data.py (deflated 71%)\n",
            "  adding: NeuralWeightVirtualization/mnist_data.py (deflated 53%)\n",
            "  adding: NeuralWeightVirtualization/hhar_accuracy_ten_nets.txt (deflated 57%)\n",
            "  adding: NeuralWeightVirtualization/us8k/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/us8k/us8k.index (deflated 45%)\n",
            "  adding: NeuralWeightVirtualization/us8k/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/us8k/us8k_network_fisher.npy (deflated 22%)\n",
            "  adding: NeuralWeightVirtualization/us8k/us8k_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/us8k/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/us8k/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/us8k/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/us8k/us8k_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/us8k/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/us8k/us8k.data-00000-of-00001 (deflated 8%)\n",
            "  adding: NeuralWeightVirtualization/us8k/us8k.meta (deflated 92%)\n",
            "  adding: NeuralWeightVirtualization/fmnist_data.py (deflated 72%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb.accuracy (deflated 18%)\n",
            "  adding: NeuralWeightVirtualization/GSC_v2_test_label.npy (deflated 99%)\n",
            "  adding: NeuralWeightVirtualization/GTSRB_test_data.npy (deflated 79%)\n",
            "  adding: NeuralWeightVirtualization/obs_matching_ten_nets.txt (deflated 80%)\n",
            "  adding: NeuralWeightVirtualization/esc10_matching_ten_nets.txt (deflated 80%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb.vnn (deflated 51%)\n",
            "  adding: NeuralWeightVirtualization/gsc/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/gsc/gsc.meta (deflated 90%)\n",
            "  adding: NeuralWeightVirtualization/gsc/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/gsc/gsc.data-00000-of-00001 (deflated 8%)\n",
            "  adding: NeuralWeightVirtualization/gsc/gsc_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/gsc/gsc_network_fisher.npy (deflated 22%)\n",
            "  adding: NeuralWeightVirtualization/gsc/gsc_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/gsc/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/gsc/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/gsc/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/gsc/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/gsc/gsc.index (deflated 44%)\n",
            "  adding: NeuralWeightVirtualization/obstacle_validation_label.npy (deflated 100%)\n",
            "  adding: NeuralWeightVirtualization/hhar_train_data.npy (deflated 38%)\n",
            "  adding: NeuralWeightVirtualization/fmnist_test_label.npy (deflated 97%)\n",
            "  adding: NeuralWeightVirtualization/esc10_accuracy_ten_nets.txt (deflated 58%)\n",
            "  adding: NeuralWeightVirtualization/obstacle_data.py (deflated 76%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/cifar10.data-00000-of-00001 (deflated 8%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/cifar10_network_fisher.npy (deflated 22%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/cifar10_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/cifar10_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/cifar10.index (deflated 47%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/cifar10.meta (deflated 91%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb_accuracy_ten_nets.txt (deflated 56%)\n",
            "  adding: NeuralWeightVirtualization/GSC_v2_train_data.npy (deflated 6%)\n",
            "  adding: NeuralWeightVirtualization/GTSRB_train_data.npy (deflated 79%)\n",
            "  adding: NeuralWeightVirtualization/esc10.vnn (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/cifar10_train_data.npy (deflated 82%)\n",
            "  adding: NeuralWeightVirtualization/svhn.accuracy (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/US8K_train_data.npy (deflated 61%)\n",
            "  adding: NeuralWeightVirtualization/us8k.accuracy (deflated 22%)\n",
            "  adding: NeuralWeightVirtualization/us8k_accuracy_ten_nets.txt (deflated 57%)\n",
            "  adding: NeuralWeightVirtualization/us8k_matching_ten_nets.txt (deflated 78%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/tf_operation.so (deflated 64%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/weight_loader.cu.o (deflated 64%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/build_tf_operation.sh (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/tf_operation.cu (deflated 82%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/weight_loader.c (deflated 59%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/weight_loader.cu (deflated 54%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/tf_operation.cu.o (deflated 64%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/build_tf_operation_nano.sh (deflated 47%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/build_weight_loader_nano.sh (deflated 43%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/tf_operation.cc (deflated 85%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/build_weight_loader.sh (deflated 43%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/weight_loader.so (deflated 68%)\n",
            "  adding: NeuralWeightVirtualization/fmnist_train_label.npy (deflated 97%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/fmnist_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/fmnist_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/fmnist.meta (deflated 93%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/fmnist_network_fisher.npy (deflated 21%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/fmnist.index (deflated 46%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/fmnist.data-00000-of-00001 (deflated 7%)\n",
            "  adding: NeuralWeightVirtualization/esc10.accuracy (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/svhn_validation_data.npy (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/cifar10_matching_ten_nets.txt (deflated 77%)\n",
            "  adding: NeuralWeightVirtualization/us8k.vnn (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/hhar/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/hhar/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/hhar/hhar.data-00000-of-00001 (deflated 6%)\n",
            "  adding: NeuralWeightVirtualization/hhar/hhar_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/hhar/hhar_network_fisher.npy (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/hhar/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/hhar/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/hhar/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/hhar/hhar.index (deflated 45%)\n",
            "  adding: NeuralWeightVirtualization/hhar/hhar_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/hhar/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/hhar/hhar.meta (deflated 92%)\n",
            "  adding: NeuralWeightVirtualization/fmnist_train_data.npy (deflated 88%)\n",
            "  adding: NeuralWeightVirtualization/out_ten_nets.log (deflated 86%)\n",
            "  adding: NeuralWeightVirtualization/obs.vnn (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/svhn_matching_ten_nets.txt (deflated 78%)\n",
            "  adding: NeuralWeightVirtualization/hhar.vnn (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/GSC_v2_validation_data.npy (deflated 6%)\n",
            "  adding: NeuralWeightVirtualization/.gitignore (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/obs/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/obs/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/obs/obs_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/obs/obs.meta (deflated 91%)\n",
            "  adding: NeuralWeightVirtualization/obs/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/obs/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/obs/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/obs/obs.index (deflated 48%)\n",
            "  adding: NeuralWeightVirtualization/obs/obs.data-00000-of-00001 (deflated 7%)\n",
            "  adding: NeuralWeightVirtualization/obs/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/obs/obs_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/obs/obs_network_fisher.npy (deflated 21%)\n",
            "  adding: NeuralWeightVirtualization/obs_accuracy_ten_nets.txt (deflated 62%)\n",
            "  adding: NeuralWeightVirtualization/fmnist_accuracy_ten_nets.txt (deflated 61%)\n",
            "  adding: NeuralWeightVirtualization/download_dataset.sh (deflated 87%)\n",
            "  adding: NeuralWeightVirtualization/GSC_v2_train_label.npy (deflated 99%)\n",
            "  adding: NeuralWeightVirtualization/obstacle_validation_data.npy (deflated 85%)\n",
            "  adding: NeuralWeightVirtualization/mnist/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/mnist/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/mnist/mnist_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/mnist/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/mnist/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/mnist/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/mnist/mnist.meta (deflated 91%)\n",
            "  adding: NeuralWeightVirtualization/mnist/mnist.data-00000-of-00001 (deflated 7%)\n",
            "  adding: NeuralWeightVirtualization/mnist/mnist.index (deflated 46%)\n",
            "  adding: NeuralWeightVirtualization/mnist/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/mnist/mnist_network_fisher.npy (deflated 21%)\n",
            "  adding: NeuralWeightVirtualization/mnist/mnist_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/in-memory_execute.py (deflated 70%)\n",
            "  adding: NeuralWeightVirtualization/.git/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/description (deflated 14%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/pre-commit.sample (deflated 43%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/fsmonitor-watchman.sample (deflated 53%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/pre-push.sample (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: NeuralWeightVirtualization/.git/info/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/info/exclude (deflated 28%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/refs/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/refs/remotes/origin/HEAD (deflated 27%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/refs/heads/master (deflated 27%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/HEAD (deflated 27%)\n",
            "  adding: NeuralWeightVirtualization/.git/objects/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/objects/info/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/objects/pack/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/objects/pack/pack-db02d4b6051da394f5f82a8f658ce6ab7f24806b.idx (deflated 11%)\n",
            "  adding: NeuralWeightVirtualization/.git/objects/pack/pack-db02d4b6051da394f5f82a8f658ce6ab7f24806b.pack (deflated 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/index (deflated 56%)\n",
            "  adding: NeuralWeightVirtualization/.git/branches/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/refs/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/refs/remotes/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/refs/heads/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/refs/heads/master (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/refs/tags/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/config (deflated 31%)\n",
            "  adding: NeuralWeightVirtualization/.git/packed-refs (deflated 10%)\n",
            "  adding: NeuralWeightVirtualization/.git/HEAD (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/US8K_test_data.npy (deflated 58%)\n",
            "  adding: NeuralWeightVirtualization/mnist.vnn (deflated 57%)\n",
            "  adding: NeuralWeightVirtualization/us8k_data.py (deflated 70%)\n",
            "  adding: NeuralWeightVirtualization/esc10_train_label.npy (deflated 97%)\n",
            "  adding: NeuralWeightVirtualization/esc10_data.py (deflated 71%)\n",
            "  adding: NeuralWeightVirtualization/svhn_data.py (deflated 67%)\n",
            "  adding: NeuralWeightVirtualization/cifar10.accuracy (deflated 17%)\n",
            "  adding: NeuralWeightVirtualization/sequential_optimization.sh (deflated 92%)\n",
            "  adding: NeuralWeightVirtualization/gsc_matching_ten_nets.txt (deflated 77%)\n",
            "  adding: NeuralWeightVirtualization/svhn_test_data.npy (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/esc10_test_label.npy (deflated 97%)\n",
            "  adding: NeuralWeightVirtualization/obstacle_test_label.npy (deflated 100%)\n",
            "  adding: NeuralWeightVirtualization/fmnist.accuracy (deflated 12%)\n",
            "  adding: NeuralWeightVirtualization/esc10_train_data.npy (deflated 8%)\n",
            "  adding: NeuralWeightVirtualization/fmnist.vnn (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/svhn_train_data.npy (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/US8K_train_label.npy (deflated 100%)\n",
            "  adding: NeuralWeightVirtualization/GTSRB_test_label.npy (deflated 99%)\n",
            "  adding: NeuralWeightVirtualization/weight_loader.so (deflated 68%)\n",
            "  adding: NeuralWeightVirtualization/obs.accuracy (deflated 26%)\n",
            "  adding: NeuralWeightVirtualization/gsc.vnn (deflated 51%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P3VbNpzn5wE",
        "outputId": "68e146b4-1b75-4818-fb9b-80bf59a2c1d9"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralWeightVirtualization  NeuralWeightVirtualization.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8xmIUjYpvYd",
        "outputId": "e3bfffa2-9c2b-473a-fd8f-2c49a237bca7"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralWeightVirtualization  NeuralWeightVirtualization.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy-SmNBguiF0",
        "outputId": "72a1f115-884f-459a-b739-695dde8da5fc"
      },
      "source": [
        "!ls -la NeuralWeightVirtualization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 9124384\n",
            "drwxr-xr-x 16 root root       4096 Jan  1 23:58 .\n",
            "drwxr-xr-x  1 root root       4096 Jan  2 01:11 ..\n",
            "-rw-r--r--  1 root root       3341 Jan  1 23:14 baseline_execute.py\n",
            "drwxr-xr-x  3 root root       4096 Jan  1 23:35 cifar10\n",
            "-rw-r--r--  1 root root         35 Jan  2 00:47 cifar10.accuracy\n",
            "-rw-r--r--  1 root root        145 Jan  2 00:47 cifar10_accuracy_ten_nets.txt\n",
            "-rw-r--r--  1 root root       2721 Jan  1 23:14 cifar10_data.py\n",
            "-rw-r--r--  1 root root       4045 Jan  1 23:35 cifar10_matching_ten_nets.txt\n",
            "-rw-r--r--  1 root root  245760128 Jan  1 23:14 cifar10_test_data.npy\n",
            "-rw-r--r--  1 root root     800128 Jan  1 23:14 cifar10_test_label.npy\n",
            "-rw-r--r--  1 root root 1228800128 Jan  1 23:14 cifar10_train_data.npy\n",
            "-rw-r--r--  1 root root    4000128 Jan  1 23:14 cifar10_train_label.npy\n",
            "-rw-r--r--  1 root root       2935 Jan  1 23:35 cifar10.vnn\n",
            "-rwxr-xr-x  1 root root      13568 Jan  1 23:14 download_dataset.sh\n",
            "-rw-r--r--  1 root root       5475 Jan  2 00:43 err.log\n",
            "drwxr-xr-x  3 root root       4096 Jan  1 23:43 esc10\n",
            "-rw-r--r--  1 root root         50 Jan  2 00:50 esc10.accuracy\n",
            "-rw-r--r--  1 root root        145 Jan  2 00:50 esc10_accuracy_ten_nets.txt\n",
            "-rw-r--r--  1 root root       1901 Jan  1 23:14 esc10_data.py\n",
            "-rw-r--r--  1 root root       4312 Jan  1 23:43 esc10_matching_ten_nets.txt\n",
            "-rw-r--r--  1 root root   27630848 Jan  1 23:17 esc10_test_data.npy\n",
            "-rw-r--r--  1 root root      28208 Jan  1 23:17 esc10_test_label.npy\n",
            "-rw-r--r--  1 root root  248715968 Jan  1 23:18 esc10_train_data.npy\n",
            "-rw-r--r--  1 root root     252888 Jan  1 23:18 esc10_train_label.npy\n",
            "-rw-r--r--  1 root root       3683 Jan  1 23:43 esc10.vnn\n",
            "drwxr-xr-x  3 root root       4096 Jan  1 23:38 fmnist\n",
            "-rw-r--r--  1 root root         34 Jan  2 00:49 fmnist.accuracy\n",
            "-rw-r--r--  1 root root        145 Jan  2 00:49 fmnist_accuracy_ten_nets.txt\n",
            "-rw-r--r--  1 root root       1880 Jan  1 23:14 fmnist_data.py\n",
            "-rw-r--r--  1 root root       4109 Jan  1 23:38 fmnist_matching_ten_nets.txt\n",
            "-rw-r--r--  1 root root   62720128 Jan  1 23:16 fmnist_test_data.npy\n",
            "-rw-r--r--  1 root root     400128 Jan  1 23:16 fmnist_test_label.npy\n",
            "-rw-r--r--  1 root root  376320128 Jan  1 23:16 fmnist_train_data.npy\n",
            "-rw-r--r--  1 root root    2400128 Jan  1 23:16 fmnist_train_label.npy\n",
            "-rw-r--r--  1 root root       3895 Jan  1 23:38 fmnist.vnn\n",
            "drwxr-xr-x  8 root root       4096 Jan  1 23:14 .git\n",
            "-rw-r--r--  1 root root         14 Jan  1 23:14 .gitignore\n",
            "drwxr-xr-x  3 root root       4096 Jan  1 23:32 gsc\n",
            "-rw-r--r--  1 root root         53 Jan  2 00:46 gsc.accuracy\n",
            "-rw-r--r--  1 root root        145 Jan  2 00:46 gsc_accuracy_ten_nets.txt\n",
            "-rw-r--r--  1 root root       3888 Jan  1 23:32 gsc_matching_ten_nets.txt\n",
            "-rw-r--r--  1 root root       2711 Jan  1 23:14 GSC_v2_data.py\n",
            "-rw-r--r--  1 root root   69815848 Jan  1 23:15 GSC_v2_test_data.npy\n",
            "-rw-r--r--  1 root root    3081528 Jan  1 23:15 GSC_v2_test_label.npy\n",
            "-rw-r--r--  1 root root  538244120 Jan  1 23:15 GSC_v2_train_data.npy\n",
            "-rw-r--r--  1 root root   23756168 Jan  1 23:15 GSC_v2_train_label.npy\n",
            "-rw-r--r--  1 root root   63319592 Jan  1 23:15 GSC_v2_validation_data.npy\n",
            "-rw-r--r--  1 root root    2794808 Jan  1 23:15 GSC_v2_validation_label.npy\n",
            "-rw-r--r--  1 root root       3659 Jan  1 23:32 gsc.vnn\n",
            "drwxr-xr-x  3 root root       4096 Jan  1 23:33 gtsrb\n",
            "-rw-r--r--  1 root root         51 Jan  2 00:46 gtsrb.accuracy\n",
            "-rw-r--r--  1 root root        145 Jan  2 00:46 gtsrb_accuracy_ten_nets.txt\n",
            "-rw-r--r--  1 root root       2913 Jan  1 23:14 GTSRB_data.py\n",
            "-rw-r--r--  1 root root       3968 Jan  1 23:33 gtsrb_matching_ten_nets.txt\n",
            "-rw-r--r--  1 root root  103465088 Jan  1 23:15 GTSRB_test_data.npy\n",
            "-rw-r--r--  1 root root    4344848 Jan  1 23:15 GTSRB_test_label.npy\n",
            "-rw-r--r--  1 root root  321200256 Jan  1 23:15 GTSRB_train_data.npy\n",
            "-rw-r--r--  1 root root   13488024 Jan  1 23:15 GTSRB_train_label.npy\n",
            "-rw-r--r--  1 root root       3735 Jan  1 23:33 gtsrb.vnn\n",
            "drwxr-xr-x  3 root root       4096 Jan  1 23:42 hhar\n",
            "-rw-r--r--  1 root root         53 Jan  2 00:50 hhar.accuracy\n",
            "-rw-r--r--  1 root root        145 Jan  2 00:50 hhar_accuracy_ten_nets.txt\n",
            "-rw-r--r--  1 root root       1836 Jan  1 23:14 hhar_data.py\n",
            "-rw-r--r--  1 root root       4247 Jan  1 23:42 hhar_matching_ten_nets.txt\n",
            "-rw-r--r--  1 root root   22905728 Jan  1 23:17 hhar_test_data.npy\n",
            "-rw-r--r--  1 root root      57392 Jan  1 23:17 hhar_test_label.npy\n",
            "-rw-r--r--  1 root root 2286336128 Jan  1 23:17 hhar_train_data.npy\n",
            "-rw-r--r--  1 root root    5715968 Jan  1 23:17 hhar_train_label.npy\n",
            "-rw-r--r--  1 root root       3663 Jan  1 23:42 hhar.vnn\n",
            "-rw-r--r--  1 root root       6038 Jan  1 23:14 in-memory_execute.py\n",
            "-rwxr-xr-x  1 root root       2266 Jan  1 23:38 joint_optimization.sh\n",
            "-rw-r--r--  1 root root     262295 Jan  1 23:14 L46_Project_Notebook.ipynb\n",
            "drwxr-xr-x  3 root root       4096 Jan  1 23:28 mnist\n",
            "-rw-r--r--  1 root root         33 Jan  2 00:46 mnist.accuracy\n",
            "-rw-r--r--  1 root root       1115 Jan  2 00:46 mnist_accuracy_ten_nets.txt\n",
            "drwxr-xr-x  2 root root       4096 Jan  1 23:27 MNIST_data\n",
            "-rw-r--r--  1 root root        542 Jan  1 23:14 mnist_data.py\n",
            "-rw-r--r--  1 root root       4068 Jan  1 23:28 mnist_matching_ten_nets.txt\n",
            "-rw-r--r--  1 root root       2907 Jan  1 23:28 mnist.vnn\n",
            "drwxr-xr-x  3 root root       4096 Jan  1 23:44 obs\n",
            "-rw-r--r--  1 root root         54 Jan  2 00:51 obs.accuracy\n",
            "-rw-r--r--  1 root root        145 Jan  2 00:51 obs_accuracy_ten_nets.txt\n",
            "-rw-r--r--  1 root root       4374 Jan  1 23:44 obs_matching_ten_nets.txt\n",
            "-rw-r--r--  1 root root       2839 Jan  1 23:14 obstacle_data.py\n",
            "-rw-r--r--  1 root root   82224128 Jan  1 23:18 obstacle_test_data.npy\n",
            "-rw-r--r--  1 root root      91488 Jan  1 23:18 obstacle_test_label.npy\n",
            "-rw-r--r--  1 root root  164620928 Jan  1 23:18 obstacle_train_data.npy\n",
            "-rw-r--r--  1 root root     183040 Jan  1 23:18 obstacle_train_label.npy\n",
            "-rw-r--r--  1 root root   82224128 Jan  1 23:18 obstacle_validation_data.npy\n",
            "-rw-r--r--  1 root root      91488 Jan  1 23:18 obstacle_validation_label.npy\n",
            "-rw-r--r--  1 root root       3919 Jan  1 23:44 obs.vnn\n",
            "-rw-r--r--  1 root root     152029 Jan  2 00:45 out_ten_nets.log\n",
            "drwxr-xr-x  2 root root       4096 Jan  1 23:43 __pycache__\n",
            "-rw-r--r--  1 root root      20691 Jan  1 23:14 README.md\n",
            "-rwxr-xr-x  1 root root       1751 Jan  1 23:14 sequential_optimization.sh\n",
            "drwxr-xr-x  3 root root       4096 Jan  1 23:36 svhn\n",
            "-rw-r--r--  1 root root         54 Jan  2 00:48 svhn.accuracy\n",
            "-rw-r--r--  1 root root        145 Jan  2 00:48 svhn_accuracy_ten_nets.txt\n",
            "-rw-r--r--  1 root root        844 Jan  1 23:14 svhn_data.py\n",
            "-rw-r--r--  1 root root       3985 Jan  1 23:37 svhn_matching_ten_nets.txt\n",
            "-rw-r--r--  1 root root  319881344 Jan  1 23:15 svhn_test_data.npy\n",
            "-rw-r--r--  1 root root    1041408 Jan  1 23:15 svhn_test_label.npy\n",
            "-rw-r--r--  1 root root  810160256 Jan  1 23:15 svhn_train_data.npy\n",
            "-rw-r--r--  1 root root    2637368 Jan  1 23:15 svhn_train_label.npy\n",
            "-rw-r--r--  1 root root   90022016 Jan  1 23:15 svhn_validation_data.npy\n",
            "-rw-r--r--  1 root root     293168 Jan  1 23:15 svhn_validation_label.npy\n",
            "-rw-r--r--  1 root root       2875 Jan  1 23:36 svhn.vnn\n",
            "drwxr-xr-x  2 root root       4096 Jan  1 23:14 tf_operation\n",
            "-rwxr-xr-x  1 root root     149584 Jan  1 23:14 tf_operation.so\n",
            "drwxr-xr-x  3 root root       4096 Jan  1 23:40 us8k\n",
            "-rw-r--r--  1 root root         55 Jan  2 00:49 us8k.accuracy\n",
            "-rw-r--r--  1 root root        145 Jan  2 00:49 us8k_accuracy_ten_nets.txt\n",
            "-rw-r--r--  1 root root       1887 Jan  1 23:14 us8k_data.py\n",
            "-rw-r--r--  1 root root       4167 Jan  1 23:40 us8k_matching_ten_nets.txt\n",
            "-rw-r--r--  1 root root  212740928 Jan  1 23:15 US8K_test_data.npy\n",
            "-rw-r--r--  1 root root     432528 Jan  1 23:15 US8K_test_label.npy\n",
            "-rw-r--r--  1 root root 1914982208 Jan  1 23:16 US8K_train_data.npy\n",
            "-rw-r--r--  1 root root    3892368 Jan  1 23:16 US8K_train_label.npy\n",
            "-rw-r--r--  1 root root       3731 Jan  1 23:40 us8k.vnn\n",
            "-rw-r--r--  1 root root     288528 Jan  2 00:45 virtual_weight_page.npy\n",
            "-rwxr-xr-x  1 root root      18728 Jan  1 23:14 weight_loader.so\n",
            "-rw-r--r--  1 root root      81901 Jan  1 23:44 weight_page_occupation.npy\n",
            "-rw-r--r--  1 root root      43046 Jan  1 23:14 weight_virtualization.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvZQgpUtvuCu",
        "outputId": "6dd70731-5174-48a0-ba3a-859ddf70489a"
      },
      "source": [
        "%cd NeuralWeightVirtualization/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NeuralWeightVirtualization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2jPVTD3ux_C"
      },
      "source": [
        "!rm hhar_train_data.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzhwDNRFvr7E"
      },
      "source": [
        "!rm hhar_test_data.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79qhCJ06v4X8"
      },
      "source": [
        "!rm *_test_data.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjgdbGdlwRus"
      },
      "source": [
        "!rm *_train_data.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0veLsJAwUQb"
      },
      "source": [
        "!rm *_train_label.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfnXRqEQwWl8"
      },
      "source": [
        "!rm *_test_label.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6NovyRNwX88"
      },
      "source": [
        "!rm *_validation_data.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QEUOMuDwa-L"
      },
      "source": [
        "!rm *_validation_label.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JxOkPcgwceR",
        "outputId": "e677281c-1c54-40ad-e1cb-fc0abd8c9815"
      },
      "source": [
        "!zip -r ./NeuralWeightVirtualization.zip ./NeuralWeightVirtualization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tzip warning: name not matched: ./NeuralWeightVirtualization\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r ./NeuralWeightVirtualization.zip . -i ./NeuralWeightVirtualization)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdYCh0FpwffZ",
        "outputId": "c0fc022e-64c6-429b-f6b6-9eddb16ecfe7"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Juri6RaiwhGM",
        "outputId": "92c28b9e-90b6-4450-9f2b-fa211d6af300"
      },
      "source": [
        "!zip -r ./NeuralWeightVirtualization.zip ./NeuralWeightVirtualization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: NeuralWeightVirtualization/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/weight_page_occupation.npy (deflated 60%)\n",
            "updating: NeuralWeightVirtualization/hhar_matching_ten_nets.txt (deflated 78%)\n",
            "updating: NeuralWeightVirtualization/tf_operation.so (deflated 64%)\n",
            "updating: NeuralWeightVirtualization/cifar10_data.py (deflated 72%)\n",
            "updating: NeuralWeightVirtualization/cifar10_accuracy_ten_nets.txt (deflated 63%)\n",
            "updating: NeuralWeightVirtualization/svhn/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/svhn/svhn_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/svhn/__init__.py (stored 0%)\n",
            "updating: NeuralWeightVirtualization/svhn/svhn_network_fisher.npy (deflated 22%)\n",
            "updating: NeuralWeightVirtualization/svhn/svhn_network_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/svhn/svhn.meta (deflated 93%)\n",
            "updating: NeuralWeightVirtualization/svhn/__pycache__/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/svhn/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "updating: NeuralWeightVirtualization/svhn/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/svhn/svhn.data-00000-of-00001 (deflated 8%)\n",
            "updating: NeuralWeightVirtualization/svhn/svhn.index (deflated 47%)\n",
            "updating: NeuralWeightVirtualization/svhn/pintle.py (deflated 69%)\n",
            "updating: NeuralWeightVirtualization/cifar10.vnn (deflated 50%)\n",
            "updating: NeuralWeightVirtualization/L46_Project_Notebook.ipynb (deflated 86%)\n",
            "updating: NeuralWeightVirtualization/mnist.accuracy (deflated 9%)\n",
            "updating: NeuralWeightVirtualization/fmnist_matching_ten_nets.txt (deflated 78%)\n",
            "updating: NeuralWeightVirtualization/virtual_weight_page.npy (deflated 7%)\n",
            "updating: NeuralWeightVirtualization/svhn_accuracy_ten_nets.txt (deflated 58%)\n",
            "updating: NeuralWeightVirtualization/mnist_matching_ten_nets.txt (deflated 77%)\n",
            "updating: NeuralWeightVirtualization/joint_optimization.sh (deflated 84%)\n",
            "updating: NeuralWeightVirtualization/esc10/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/esc10/esc10.index (deflated 46%)\n",
            "updating: NeuralWeightVirtualization/esc10/__init__.py (stored 0%)\n",
            "updating: NeuralWeightVirtualization/esc10/esc10.meta (deflated 93%)\n",
            "updating: NeuralWeightVirtualization/esc10/esc10.data-00000-of-00001 (deflated 7%)\n",
            "updating: NeuralWeightVirtualization/esc10/__pycache__/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/esc10/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "updating: NeuralWeightVirtualization/esc10/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/esc10/esc10_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/esc10/esc10_network_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/esc10/esc10_network_fisher.npy (deflated 21%)\n",
            "updating: NeuralWeightVirtualization/esc10/pintle.py (deflated 69%)\n",
            "updating: NeuralWeightVirtualization/svhn.vnn (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/hhar.accuracy (deflated 21%)\n",
            "updating: NeuralWeightVirtualization/gtsrb/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/gtsrb/__init__.py (stored 0%)\n",
            "updating: NeuralWeightVirtualization/gtsrb/gtsrb_network_fisher.npy (deflated 22%)\n",
            "updating: NeuralWeightVirtualization/gtsrb/__pycache__/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/gtsrb/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "updating: NeuralWeightVirtualization/gtsrb/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/gtsrb/gtsrb.meta (deflated 90%)\n",
            "updating: NeuralWeightVirtualization/gtsrb/gtsrb.data-00000-of-00001 (deflated 7%)\n",
            "updating: NeuralWeightVirtualization/gtsrb/gtsrb_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/gtsrb/pintle.py (deflated 69%)\n",
            "updating: NeuralWeightVirtualization/gtsrb/gtsrb_network_weight.npy (deflated 26%)\n",
            "updating: NeuralWeightVirtualization/gtsrb/gtsrb.index (deflated 47%)\n",
            "updating: NeuralWeightVirtualization/GTSRB_data.py (deflated 71%)\n",
            "updating: NeuralWeightVirtualization/weight_virtualization.py (deflated 82%)\n",
            "updating: NeuralWeightVirtualization/MNIST_data/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/MNIST_data/t10k-labels-idx1-ubyte.gz (stored 0%)\n",
            "updating: NeuralWeightVirtualization/MNIST_data/t10k-images-idx3-ubyte.gz (deflated 0%)\n",
            "updating: NeuralWeightVirtualization/MNIST_data/train-images-idx3-ubyte.gz (deflated 0%)\n",
            "updating: NeuralWeightVirtualization/MNIST_data/train-labels-idx1-ubyte.gz (stored 0%)\n",
            "updating: NeuralWeightVirtualization/gtsrb_matching_ten_nets.txt (deflated 77%)\n",
            "updating: NeuralWeightVirtualization/gsc.accuracy (deflated 19%)\n",
            "updating: NeuralWeightVirtualization/baseline_execute.py (deflated 62%)\n",
            "updating: NeuralWeightVirtualization/gsc_accuracy_ten_nets.txt (deflated 56%)\n",
            "updating: NeuralWeightVirtualization/README.md (deflated 73%)\n",
            "updating: NeuralWeightVirtualization/GSC_v2_data.py (deflated 70%)\n",
            "updating: NeuralWeightVirtualization/__pycache__/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/__pycache__/GSC_v2_data.cpython-36.pyc (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/__pycache__/GTSRB_data.cpython-36.pyc (deflated 47%)\n",
            "updating: NeuralWeightVirtualization/__pycache__/obstacle_data.cpython-36.pyc (deflated 54%)\n",
            "updating: NeuralWeightVirtualization/__pycache__/esc10_data.cpython-36.pyc (deflated 50%)\n",
            "updating: NeuralWeightVirtualization/__pycache__/mnist_data.cpython-36.pyc (deflated 48%)\n",
            "updating: NeuralWeightVirtualization/__pycache__/svhn_data.cpython-36.pyc (deflated 54%)\n",
            "updating: NeuralWeightVirtualization/__pycache__/fmnist_data.cpython-36.pyc (deflated 50%)\n",
            "updating: NeuralWeightVirtualization/__pycache__/hhar_data.cpython-36.pyc (deflated 50%)\n",
            "updating: NeuralWeightVirtualization/__pycache__/cifar10_data.cpython-36.pyc (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/__pycache__/us8k_data.cpython-36.pyc (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/mnist_accuracy_ten_nets.txt (deflated 88%)\n",
            "updating: NeuralWeightVirtualization/err.log (deflated 89%)\n",
            "updating: NeuralWeightVirtualization/hhar_data.py (deflated 71%)\n",
            "updating: NeuralWeightVirtualization/mnist_data.py (deflated 53%)\n",
            "updating: NeuralWeightVirtualization/hhar_accuracy_ten_nets.txt (deflated 57%)\n",
            "updating: NeuralWeightVirtualization/us8k/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/us8k/us8k.index (deflated 45%)\n",
            "updating: NeuralWeightVirtualization/us8k/__init__.py (stored 0%)\n",
            "updating: NeuralWeightVirtualization/us8k/us8k_network_fisher.npy (deflated 22%)\n",
            "updating: NeuralWeightVirtualization/us8k/us8k_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/us8k/__pycache__/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/us8k/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "updating: NeuralWeightVirtualization/us8k/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/us8k/us8k_network_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/us8k/pintle.py (deflated 69%)\n",
            "updating: NeuralWeightVirtualization/us8k/us8k.data-00000-of-00001 (deflated 8%)\n",
            "updating: NeuralWeightVirtualization/us8k/us8k.meta (deflated 92%)\n",
            "updating: NeuralWeightVirtualization/fmnist_data.py (deflated 72%)\n",
            "updating: NeuralWeightVirtualization/gtsrb.accuracy (deflated 18%)\n",
            "updating: NeuralWeightVirtualization/obs_matching_ten_nets.txt (deflated 80%)\n",
            "updating: NeuralWeightVirtualization/esc10_matching_ten_nets.txt (deflated 80%)\n",
            "updating: NeuralWeightVirtualization/gtsrb.vnn (deflated 51%)\n",
            "updating: NeuralWeightVirtualization/gsc/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/gsc/gsc.meta (deflated 90%)\n",
            "updating: NeuralWeightVirtualization/gsc/__init__.py (stored 0%)\n",
            "updating: NeuralWeightVirtualization/gsc/gsc.data-00000-of-00001 (deflated 8%)\n",
            "updating: NeuralWeightVirtualization/gsc/gsc_network_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/gsc/gsc_network_fisher.npy (deflated 22%)\n",
            "updating: NeuralWeightVirtualization/gsc/gsc_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/gsc/__pycache__/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/gsc/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "updating: NeuralWeightVirtualization/gsc/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/gsc/pintle.py (deflated 69%)\n",
            "updating: NeuralWeightVirtualization/gsc/gsc.index (deflated 44%)\n",
            "updating: NeuralWeightVirtualization/esc10_accuracy_ten_nets.txt (deflated 58%)\n",
            "updating: NeuralWeightVirtualization/obstacle_data.py (deflated 76%)\n",
            "updating: NeuralWeightVirtualization/cifar10/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/cifar10/cifar10.data-00000-of-00001 (deflated 8%)\n",
            "updating: NeuralWeightVirtualization/cifar10/__init__.py (stored 0%)\n",
            "updating: NeuralWeightVirtualization/cifar10/cifar10_network_fisher.npy (deflated 22%)\n",
            "updating: NeuralWeightVirtualization/cifar10/cifar10_network_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/cifar10/cifar10_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/cifar10/__pycache__/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/cifar10/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "updating: NeuralWeightVirtualization/cifar10/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/cifar10/pintle.py (deflated 69%)\n",
            "updating: NeuralWeightVirtualization/cifar10/cifar10.index (deflated 47%)\n",
            "updating: NeuralWeightVirtualization/cifar10/cifar10.meta (deflated 91%)\n",
            "updating: NeuralWeightVirtualization/gtsrb_accuracy_ten_nets.txt (deflated 56%)\n",
            "updating: NeuralWeightVirtualization/esc10.vnn (deflated 50%)\n",
            "updating: NeuralWeightVirtualization/svhn.accuracy (deflated 20%)\n",
            "updating: NeuralWeightVirtualization/us8k.accuracy (deflated 22%)\n",
            "updating: NeuralWeightVirtualization/us8k_accuracy_ten_nets.txt (deflated 57%)\n",
            "updating: NeuralWeightVirtualization/us8k_matching_ten_nets.txt (deflated 78%)\n",
            "updating: NeuralWeightVirtualization/tf_operation/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/tf_operation/tf_operation.so (deflated 64%)\n",
            "updating: NeuralWeightVirtualization/tf_operation/weight_loader.cu.o (deflated 64%)\n",
            "updating: NeuralWeightVirtualization/tf_operation/build_tf_operation.sh (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/tf_operation/tf_operation.cu (deflated 82%)\n",
            "updating: NeuralWeightVirtualization/tf_operation/weight_loader.c (deflated 59%)\n",
            "updating: NeuralWeightVirtualization/tf_operation/weight_loader.cu (deflated 54%)\n",
            "updating: NeuralWeightVirtualization/tf_operation/tf_operation.cu.o (deflated 64%)\n",
            "updating: NeuralWeightVirtualization/tf_operation/build_tf_operation_nano.sh (deflated 47%)\n",
            "updating: NeuralWeightVirtualization/tf_operation/build_weight_loader_nano.sh (deflated 43%)\n",
            "updating: NeuralWeightVirtualization/tf_operation/tf_operation.cc (deflated 85%)\n",
            "updating: NeuralWeightVirtualization/tf_operation/build_weight_loader.sh (deflated 43%)\n",
            "updating: NeuralWeightVirtualization/tf_operation/weight_loader.so (deflated 68%)\n",
            "updating: NeuralWeightVirtualization/fmnist/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/fmnist/__init__.py (stored 0%)\n",
            "updating: NeuralWeightVirtualization/fmnist/fmnist_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/fmnist/fmnist_network_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/fmnist/fmnist.meta (deflated 93%)\n",
            "updating: NeuralWeightVirtualization/fmnist/__pycache__/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/fmnist/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "updating: NeuralWeightVirtualization/fmnist/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/fmnist/fmnist_network_fisher.npy (deflated 21%)\n",
            "updating: NeuralWeightVirtualization/fmnist/fmnist.index (deflated 46%)\n",
            "updating: NeuralWeightVirtualization/fmnist/pintle.py (deflated 69%)\n",
            "updating: NeuralWeightVirtualization/fmnist/fmnist.data-00000-of-00001 (deflated 7%)\n",
            "updating: NeuralWeightVirtualization/esc10.accuracy (deflated 20%)\n",
            "updating: NeuralWeightVirtualization/cifar10_matching_ten_nets.txt (deflated 77%)\n",
            "updating: NeuralWeightVirtualization/us8k.vnn (deflated 50%)\n",
            "updating: NeuralWeightVirtualization/hhar/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/hhar/__init__.py (stored 0%)\n",
            "updating: NeuralWeightVirtualization/hhar/hhar.data-00000-of-00001 (deflated 6%)\n",
            "updating: NeuralWeightVirtualization/hhar/hhar_network_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/hhar/hhar_network_fisher.npy (deflated 20%)\n",
            "updating: NeuralWeightVirtualization/hhar/__pycache__/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/hhar/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "updating: NeuralWeightVirtualization/hhar/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/hhar/hhar.index (deflated 45%)\n",
            "updating: NeuralWeightVirtualization/hhar/hhar_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/hhar/pintle.py (deflated 69%)\n",
            "updating: NeuralWeightVirtualization/hhar/hhar.meta (deflated 92%)\n",
            "updating: NeuralWeightVirtualization/out_ten_nets.log (deflated 86%)\n",
            "updating: NeuralWeightVirtualization/obs.vnn (deflated 50%)\n",
            "updating: NeuralWeightVirtualization/svhn_matching_ten_nets.txt (deflated 78%)\n",
            "updating: NeuralWeightVirtualization/hhar.vnn (deflated 50%)\n",
            "updating: NeuralWeightVirtualization/.gitignore (stored 0%)\n",
            "updating: NeuralWeightVirtualization/obs/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/obs/__init__.py (stored 0%)\n",
            "updating: NeuralWeightVirtualization/obs/obs_network_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/obs/obs.meta (deflated 91%)\n",
            "updating: NeuralWeightVirtualization/obs/__pycache__/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/obs/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "updating: NeuralWeightVirtualization/obs/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/obs/obs.index (deflated 48%)\n",
            "updating: NeuralWeightVirtualization/obs/obs.data-00000-of-00001 (deflated 7%)\n",
            "updating: NeuralWeightVirtualization/obs/pintle.py (deflated 69%)\n",
            "updating: NeuralWeightVirtualization/obs/obs_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/obs/obs_network_fisher.npy (deflated 21%)\n",
            "updating: NeuralWeightVirtualization/obs_accuracy_ten_nets.txt (deflated 62%)\n",
            "updating: NeuralWeightVirtualization/fmnist_accuracy_ten_nets.txt (deflated 61%)\n",
            "updating: NeuralWeightVirtualization/download_dataset.sh (deflated 87%)\n",
            "updating: NeuralWeightVirtualization/mnist/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/mnist/__init__.py (stored 0%)\n",
            "updating: NeuralWeightVirtualization/mnist/mnist_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/mnist/__pycache__/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/mnist/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "updating: NeuralWeightVirtualization/mnist/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "updating: NeuralWeightVirtualization/mnist/mnist.meta (deflated 91%)\n",
            "updating: NeuralWeightVirtualization/mnist/mnist.data-00000-of-00001 (deflated 7%)\n",
            "updating: NeuralWeightVirtualization/mnist/mnist.index (deflated 46%)\n",
            "updating: NeuralWeightVirtualization/mnist/pintle.py (deflated 69%)\n",
            "updating: NeuralWeightVirtualization/mnist/mnist_network_fisher.npy (deflated 21%)\n",
            "updating: NeuralWeightVirtualization/mnist/mnist_network_weight.npy (deflated 25%)\n",
            "updating: NeuralWeightVirtualization/in-memory_execute.py (deflated 70%)\n",
            "updating: NeuralWeightVirtualization/.git/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/description (deflated 14%)\n",
            "updating: NeuralWeightVirtualization/.git/hooks/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/hooks/post-update.sample (deflated 27%)\n",
            "updating: NeuralWeightVirtualization/.git/hooks/pre-commit.sample (deflated 43%)\n",
            "updating: NeuralWeightVirtualization/.git/hooks/update.sample (deflated 68%)\n",
            "updating: NeuralWeightVirtualization/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "updating: NeuralWeightVirtualization/.git/hooks/fsmonitor-watchman.sample (deflated 53%)\n",
            "updating: NeuralWeightVirtualization/.git/hooks/pre-push.sample (deflated 50%)\n",
            "updating: NeuralWeightVirtualization/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "updating: NeuralWeightVirtualization/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "updating: NeuralWeightVirtualization/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "updating: NeuralWeightVirtualization/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "updating: NeuralWeightVirtualization/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "updating: NeuralWeightVirtualization/.git/info/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/info/exclude (deflated 28%)\n",
            "updating: NeuralWeightVirtualization/.git/logs/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/logs/refs/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/logs/refs/remotes/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/logs/refs/remotes/origin/HEAD (deflated 27%)\n",
            "updating: NeuralWeightVirtualization/.git/logs/refs/heads/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/logs/refs/heads/master (deflated 27%)\n",
            "updating: NeuralWeightVirtualization/.git/logs/HEAD (deflated 27%)\n",
            "updating: NeuralWeightVirtualization/.git/objects/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/objects/info/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/objects/pack/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/objects/pack/pack-db02d4b6051da394f5f82a8f658ce6ab7f24806b.idx (deflated 11%)\n",
            "updating: NeuralWeightVirtualization/.git/objects/pack/pack-db02d4b6051da394f5f82a8f658ce6ab7f24806b.pack (deflated 0%)\n",
            "updating: NeuralWeightVirtualization/.git/index (deflated 56%)\n",
            "updating: NeuralWeightVirtualization/.git/branches/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/refs/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/refs/remotes/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/refs/remotes/origin/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/refs/heads/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/refs/heads/master (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/refs/tags/ (stored 0%)\n",
            "updating: NeuralWeightVirtualization/.git/config (deflated 31%)\n",
            "updating: NeuralWeightVirtualization/.git/packed-refs (deflated 10%)\n",
            "updating: NeuralWeightVirtualization/.git/HEAD (stored 0%)\n",
            "updating: NeuralWeightVirtualization/mnist.vnn (deflated 57%)\n",
            "updating: NeuralWeightVirtualization/us8k_data.py (deflated 70%)\n",
            "updating: NeuralWeightVirtualization/esc10_data.py (deflated 71%)\n",
            "updating: NeuralWeightVirtualization/svhn_data.py (deflated 67%)\n",
            "updating: NeuralWeightVirtualization/cifar10.accuracy (deflated 17%)\n",
            "updating: NeuralWeightVirtualization/sequential_optimization.sh (deflated 92%)\n",
            "updating: NeuralWeightVirtualization/gsc_matching_ten_nets.txt (deflated 77%)\n",
            "updating: NeuralWeightVirtualization/fmnist.accuracy (deflated 12%)\n",
            "updating: NeuralWeightVirtualization/fmnist.vnn (deflated 50%)\n",
            "updating: NeuralWeightVirtualization/weight_loader.so (deflated 68%)\n",
            "updating: NeuralWeightVirtualization/obs.accuracy (deflated 26%)\n",
            "updating: NeuralWeightVirtualization/gsc.vnn (deflated 51%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHGQjAPpwhgh",
        "outputId": "d4f622de-291d-42ae-c808-fd4d50fae8c0"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4161444\n",
            "drwxr-xr-x  1 root root       4096 Jan  2 01:43 .\n",
            "drwxr-xr-x  1 root root       4096 Jan  1 23:06 ..\n",
            "drwxr-xr-x  1 root root       4096 Dec 21 17:29 .config\n",
            "drwxr-xr-x 16 root root       4096 Jan  2 01:41 NeuralWeightVirtualization\n",
            "-rw-r--r--  1 root root 4261291540 Jan  2 01:43 NeuralWeightVirtualization.zip\n",
            "drwxr-xr-x  1 root root       4096 Dec 21 17:29 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXDlBi7axJxy"
      },
      "source": [
        "rm NeuralWeightVirtualization.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWoCVGd8xP3P",
        "outputId": "01da5fa8-5301-476d-9268-ba16ae9d7cd4"
      },
      "source": [
        "!zip -r ./NeuralWeightVirtualization.zip ./NeuralWeightVirtualization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: NeuralWeightVirtualization/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/weight_page_occupation.npy (deflated 60%)\n",
            "  adding: NeuralWeightVirtualization/hhar_matching_ten_nets.txt (deflated 78%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation.so (deflated 64%)\n",
            "  adding: NeuralWeightVirtualization/cifar10_data.py (deflated 72%)\n",
            "  adding: NeuralWeightVirtualization/cifar10_accuracy_ten_nets.txt (deflated 63%)\n",
            "  adding: NeuralWeightVirtualization/svhn/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/svhn/svhn_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/svhn/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/svhn/svhn_network_fisher.npy (deflated 22%)\n",
            "  adding: NeuralWeightVirtualization/svhn/svhn_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/svhn/svhn.meta (deflated 93%)\n",
            "  adding: NeuralWeightVirtualization/svhn/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/svhn/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/svhn/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/svhn/svhn.data-00000-of-00001 (deflated 8%)\n",
            "  adding: NeuralWeightVirtualization/svhn/svhn.index (deflated 47%)\n",
            "  adding: NeuralWeightVirtualization/svhn/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/cifar10.vnn (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/L46_Project_Notebook.ipynb (deflated 86%)\n",
            "  adding: NeuralWeightVirtualization/mnist.accuracy (deflated 9%)\n",
            "  adding: NeuralWeightVirtualization/fmnist_matching_ten_nets.txt (deflated 78%)\n",
            "  adding: NeuralWeightVirtualization/virtual_weight_page.npy (deflated 7%)\n",
            "  adding: NeuralWeightVirtualization/svhn_accuracy_ten_nets.txt (deflated 58%)\n",
            "  adding: NeuralWeightVirtualization/mnist_matching_ten_nets.txt (deflated 77%)\n",
            "  adding: NeuralWeightVirtualization/joint_optimization.sh (deflated 84%)\n",
            "  adding: NeuralWeightVirtualization/esc10/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/esc10/esc10.index (deflated 46%)\n",
            "  adding: NeuralWeightVirtualization/esc10/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/esc10/esc10.meta (deflated 93%)\n",
            "  adding: NeuralWeightVirtualization/esc10/esc10.data-00000-of-00001 (deflated 7%)\n",
            "  adding: NeuralWeightVirtualization/esc10/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/esc10/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/esc10/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/esc10/esc10_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/esc10/esc10_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/esc10/esc10_network_fisher.npy (deflated 21%)\n",
            "  adding: NeuralWeightVirtualization/esc10/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/svhn.vnn (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/hhar.accuracy (deflated 21%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/gtsrb_network_fisher.npy (deflated 22%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/gtsrb.meta (deflated 90%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/gtsrb.data-00000-of-00001 (deflated 7%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/gtsrb_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/gtsrb_network_weight.npy (deflated 26%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb/gtsrb.index (deflated 47%)\n",
            "  adding: NeuralWeightVirtualization/GTSRB_data.py (deflated 71%)\n",
            "  adding: NeuralWeightVirtualization/weight_virtualization.py (deflated 82%)\n",
            "  adding: NeuralWeightVirtualization/MNIST_data/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/MNIST_data/t10k-labels-idx1-ubyte.gz (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/MNIST_data/t10k-images-idx3-ubyte.gz (deflated 0%)\n",
            "  adding: NeuralWeightVirtualization/MNIST_data/train-images-idx3-ubyte.gz (deflated 0%)\n",
            "  adding: NeuralWeightVirtualization/MNIST_data/train-labels-idx1-ubyte.gz (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb_matching_ten_nets.txt (deflated 77%)\n",
            "  adding: NeuralWeightVirtualization/gsc.accuracy (deflated 19%)\n",
            "  adding: NeuralWeightVirtualization/baseline_execute.py (deflated 62%)\n",
            "  adding: NeuralWeightVirtualization/gsc_accuracy_ten_nets.txt (deflated 56%)\n",
            "  adding: NeuralWeightVirtualization/README.md (deflated 73%)\n",
            "  adding: NeuralWeightVirtualization/GSC_v2_data.py (deflated 70%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/GSC_v2_data.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/GTSRB_data.cpython-36.pyc (deflated 47%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/obstacle_data.cpython-36.pyc (deflated 54%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/esc10_data.cpython-36.pyc (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/mnist_data.cpython-36.pyc (deflated 48%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/svhn_data.cpython-36.pyc (deflated 54%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/fmnist_data.cpython-36.pyc (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/hhar_data.cpython-36.pyc (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/cifar10_data.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/__pycache__/us8k_data.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/mnist_accuracy_ten_nets.txt (deflated 88%)\n",
            "  adding: NeuralWeightVirtualization/err.log (deflated 89%)\n",
            "  adding: NeuralWeightVirtualization/hhar_data.py (deflated 71%)\n",
            "  adding: NeuralWeightVirtualization/mnist_data.py (deflated 53%)\n",
            "  adding: NeuralWeightVirtualization/hhar_accuracy_ten_nets.txt (deflated 57%)\n",
            "  adding: NeuralWeightVirtualization/us8k/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/us8k/us8k.index (deflated 45%)\n",
            "  adding: NeuralWeightVirtualization/us8k/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/us8k/us8k_network_fisher.npy (deflated 22%)\n",
            "  adding: NeuralWeightVirtualization/us8k/us8k_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/us8k/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/us8k/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/us8k/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/us8k/us8k_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/us8k/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/us8k/us8k.data-00000-of-00001 (deflated 8%)\n",
            "  adding: NeuralWeightVirtualization/us8k/us8k.meta (deflated 92%)\n",
            "  adding: NeuralWeightVirtualization/fmnist_data.py (deflated 72%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb.accuracy (deflated 18%)\n",
            "  adding: NeuralWeightVirtualization/obs_matching_ten_nets.txt (deflated 80%)\n",
            "  adding: NeuralWeightVirtualization/esc10_matching_ten_nets.txt (deflated 80%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb.vnn (deflated 51%)\n",
            "  adding: NeuralWeightVirtualization/gsc/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/gsc/gsc.meta (deflated 90%)\n",
            "  adding: NeuralWeightVirtualization/gsc/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/gsc/gsc.data-00000-of-00001 (deflated 8%)\n",
            "  adding: NeuralWeightVirtualization/gsc/gsc_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/gsc/gsc_network_fisher.npy (deflated 22%)\n",
            "  adding: NeuralWeightVirtualization/gsc/gsc_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/gsc/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/gsc/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/gsc/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/gsc/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/gsc/gsc.index (deflated 44%)\n",
            "  adding: NeuralWeightVirtualization/esc10_accuracy_ten_nets.txt (deflated 58%)\n",
            "  adding: NeuralWeightVirtualization/obstacle_data.py (deflated 76%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/cifar10.data-00000-of-00001 (deflated 8%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/cifar10_network_fisher.npy (deflated 22%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/cifar10_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/cifar10_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/cifar10.index (deflated 47%)\n",
            "  adding: NeuralWeightVirtualization/cifar10/cifar10.meta (deflated 91%)\n",
            "  adding: NeuralWeightVirtualization/gtsrb_accuracy_ten_nets.txt (deflated 56%)\n",
            "  adding: NeuralWeightVirtualization/esc10.vnn (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/svhn.accuracy (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/us8k.accuracy (deflated 22%)\n",
            "  adding: NeuralWeightVirtualization/us8k_accuracy_ten_nets.txt (deflated 57%)\n",
            "  adding: NeuralWeightVirtualization/us8k_matching_ten_nets.txt (deflated 78%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/tf_operation.so (deflated 64%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/weight_loader.cu.o (deflated 64%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/build_tf_operation.sh (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/tf_operation.cu (deflated 82%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/weight_loader.c (deflated 59%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/weight_loader.cu (deflated 54%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/tf_operation.cu.o (deflated 64%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/build_tf_operation_nano.sh (deflated 47%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/build_weight_loader_nano.sh (deflated 43%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/tf_operation.cc (deflated 85%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/build_weight_loader.sh (deflated 43%)\n",
            "  adding: NeuralWeightVirtualization/tf_operation/weight_loader.so (deflated 68%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/fmnist_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/fmnist_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/fmnist.meta (deflated 93%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/fmnist_network_fisher.npy (deflated 21%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/fmnist.index (deflated 46%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/fmnist/fmnist.data-00000-of-00001 (deflated 7%)\n",
            "  adding: NeuralWeightVirtualization/esc10.accuracy (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/cifar10_matching_ten_nets.txt (deflated 77%)\n",
            "  adding: NeuralWeightVirtualization/us8k.vnn (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/hhar/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/hhar/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/hhar/hhar.data-00000-of-00001 (deflated 6%)\n",
            "  adding: NeuralWeightVirtualization/hhar/hhar_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/hhar/hhar_network_fisher.npy (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/hhar/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/hhar/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/hhar/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/hhar/hhar.index (deflated 45%)\n",
            "  adding: NeuralWeightVirtualization/hhar/hhar_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/hhar/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/hhar/hhar.meta (deflated 92%)\n",
            "  adding: NeuralWeightVirtualization/out_ten_nets.log (deflated 86%)\n",
            "  adding: NeuralWeightVirtualization/obs.vnn (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/svhn_matching_ten_nets.txt (deflated 78%)\n",
            "  adding: NeuralWeightVirtualization/hhar.vnn (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/.gitignore (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/obs/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/obs/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/obs/obs_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/obs/obs.meta (deflated 91%)\n",
            "  adding: NeuralWeightVirtualization/obs/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/obs/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/obs/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/obs/obs.index (deflated 48%)\n",
            "  adding: NeuralWeightVirtualization/obs/obs.data-00000-of-00001 (deflated 7%)\n",
            "  adding: NeuralWeightVirtualization/obs/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/obs/obs_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/obs/obs_network_fisher.npy (deflated 21%)\n",
            "  adding: NeuralWeightVirtualization/obs_accuracy_ten_nets.txt (deflated 62%)\n",
            "  adding: NeuralWeightVirtualization/fmnist_accuracy_ten_nets.txt (deflated 61%)\n",
            "  adding: NeuralWeightVirtualization/download_dataset.sh (deflated 87%)\n",
            "  adding: NeuralWeightVirtualization/mnist/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/mnist/__init__.py (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/mnist/mnist_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/mnist/__pycache__/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/mnist/__pycache__/__init__.cpython-36.pyc (deflated 20%)\n",
            "  adding: NeuralWeightVirtualization/mnist/__pycache__/pintle.cpython-36.pyc (deflated 49%)\n",
            "  adding: NeuralWeightVirtualization/mnist/mnist.meta (deflated 91%)\n",
            "  adding: NeuralWeightVirtualization/mnist/mnist.data-00000-of-00001 (deflated 7%)\n",
            "  adding: NeuralWeightVirtualization/mnist/mnist.index (deflated 46%)\n",
            "  adding: NeuralWeightVirtualization/mnist/pintle.py (deflated 69%)\n",
            "  adding: NeuralWeightVirtualization/mnist/mnist_network_fisher.npy (deflated 21%)\n",
            "  adding: NeuralWeightVirtualization/mnist/mnist_network_weight.npy (deflated 25%)\n",
            "  adding: NeuralWeightVirtualization/in-memory_execute.py (deflated 70%)\n",
            "  adding: NeuralWeightVirtualization/.git/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/description (deflated 14%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/pre-commit.sample (deflated 43%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/fsmonitor-watchman.sample (deflated 53%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/pre-push.sample (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: NeuralWeightVirtualization/.git/info/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/info/exclude (deflated 28%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/refs/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/refs/remotes/origin/HEAD (deflated 27%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/refs/heads/master (deflated 27%)\n",
            "  adding: NeuralWeightVirtualization/.git/logs/HEAD (deflated 27%)\n",
            "  adding: NeuralWeightVirtualization/.git/objects/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/objects/info/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/objects/pack/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/objects/pack/pack-db02d4b6051da394f5f82a8f658ce6ab7f24806b.idx (deflated 11%)\n",
            "  adding: NeuralWeightVirtualization/.git/objects/pack/pack-db02d4b6051da394f5f82a8f658ce6ab7f24806b.pack (deflated 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/index (deflated 56%)\n",
            "  adding: NeuralWeightVirtualization/.git/branches/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/refs/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/refs/remotes/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/refs/heads/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/refs/heads/master (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/refs/tags/ (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/.git/config (deflated 31%)\n",
            "  adding: NeuralWeightVirtualization/.git/packed-refs (deflated 10%)\n",
            "  adding: NeuralWeightVirtualization/.git/HEAD (stored 0%)\n",
            "  adding: NeuralWeightVirtualization/mnist.vnn (deflated 57%)\n",
            "  adding: NeuralWeightVirtualization/us8k_data.py (deflated 70%)\n",
            "  adding: NeuralWeightVirtualization/esc10_data.py (deflated 71%)\n",
            "  adding: NeuralWeightVirtualization/svhn_data.py (deflated 67%)\n",
            "  adding: NeuralWeightVirtualization/cifar10.accuracy (deflated 17%)\n",
            "  adding: NeuralWeightVirtualization/sequential_optimization.sh (deflated 92%)\n",
            "  adding: NeuralWeightVirtualization/gsc_matching_ten_nets.txt (deflated 77%)\n",
            "  adding: NeuralWeightVirtualization/fmnist.accuracy (deflated 12%)\n",
            "  adding: NeuralWeightVirtualization/fmnist.vnn (deflated 50%)\n",
            "  adding: NeuralWeightVirtualization/weight_loader.so (deflated 68%)\n",
            "  adding: NeuralWeightVirtualization/obs.accuracy (deflated 26%)\n",
            "  adding: NeuralWeightVirtualization/gsc.vnn (deflated 51%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhgZo2PxqQJG",
        "outputId": "d961877f-5734-47b2-92b1-24b6ee708b38"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoMXBE50xSHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1098a0-0b80-48d3-9871-1c877844f0b8"
      },
      "source": [
        "!unzip ./NeuralWeightVirtualization2.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./NeuralWeightVirtualization2.zip\n",
            "   creating: NeuralWeightVirtualization/\n",
            "  inflating: __MACOSX/._NeuralWeightVirtualization  \n",
            "   creating: NeuralWeightVirtualization/mnist/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._mnist  \n",
            "  inflating: NeuralWeightVirtualization/weight_virtualization.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._weight_virtualization.py  \n",
            "  inflating: NeuralWeightVirtualization/err.log  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._err.log  \n",
            "  inflating: NeuralWeightVirtualization/gsc_matching_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._gsc_matching_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/mnist.accuracy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._mnist.accuracy  \n",
            "   creating: NeuralWeightVirtualization/MNIST_data/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._MNIST_data  \n",
            "  inflating: NeuralWeightVirtualization/gtsrb.accuracy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._gtsrb.accuracy  \n",
            "  inflating: NeuralWeightVirtualization/us8k.accuracy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._us8k.accuracy  \n",
            "   creating: NeuralWeightVirtualization/esc10/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._esc10  \n",
            "  inflating: NeuralWeightVirtualization/sequential_optimization.sh  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._sequential_optimization.sh  \n",
            "  inflating: NeuralWeightVirtualization/in-memory_execute.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._in-memory_execute.py  \n",
            "   creating: NeuralWeightVirtualization/svhn/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._svhn  \n",
            "  inflating: NeuralWeightVirtualization/joint_optimization.sh  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._joint_optimization.sh  \n",
            "  inflating: NeuralWeightVirtualization/obs_matching_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._obs_matching_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/us8k_accuracy_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._us8k_accuracy_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/obs.vnn  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._obs.vnn  \n",
            "  inflating: NeuralWeightVirtualization/.DS_Store  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._.DS_Store  \n",
            "  inflating: NeuralWeightVirtualization/esc10_matching_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._esc10_matching_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/virtual_weight_page.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._virtual_weight_page.npy  \n",
            "  inflating: NeuralWeightVirtualization/hhar_matching_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._hhar_matching_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/svhn.vnn  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._svhn.vnn  \n",
            "  inflating: NeuralWeightVirtualization/weight_loader.so  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._weight_loader.so  \n",
            "  inflating: NeuralWeightVirtualization/fmnist.vnn  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._fmnist.vnn  \n",
            "  inflating: NeuralWeightVirtualization/cifar10.accuracy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._cifar10.accuracy  \n",
            "  inflating: NeuralWeightVirtualization/cifar10_matching_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._cifar10_matching_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/tf_operation.so  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._tf_operation.so  \n",
            "  inflating: NeuralWeightVirtualization/gtsrb_matching_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._gtsrb_matching_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/cifar10_data.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._cifar10_data.py  \n",
            "  inflating: NeuralWeightVirtualization/fmnist_accuracy_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._fmnist_accuracy_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/svhn_matching_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._svhn_matching_ten_nets.txt  \n",
            "   creating: NeuralWeightVirtualization/us8k/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._us8k  \n",
            "  inflating: NeuralWeightVirtualization/fmnist.accuracy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._fmnist.accuracy  \n",
            "  inflating: NeuralWeightVirtualization/mnist_matching_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._mnist_matching_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/us8k.vnn  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._us8k.vnn  \n",
            "   creating: NeuralWeightVirtualization/hhar/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._hhar  \n",
            "  inflating: NeuralWeightVirtualization/hhar.accuracy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._hhar.accuracy  \n",
            "  inflating: NeuralWeightVirtualization/GTSRB_data.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._GTSRB_data.py  \n",
            "  inflating: NeuralWeightVirtualization/gsc.vnn  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._gsc.vnn  \n",
            "  inflating: NeuralWeightVirtualization/mnist_data.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._mnist_data.py  \n",
            "  inflating: NeuralWeightVirtualization/hhar_data.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._hhar_data.py  \n",
            "   creating: NeuralWeightVirtualization/__pycache__/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.___pycache__  \n",
            "  inflating: NeuralWeightVirtualization/cifar10.vnn  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._cifar10.vnn  \n",
            "  inflating: NeuralWeightVirtualization/obstacle_data.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._obstacle_data.py  \n",
            "  inflating: NeuralWeightVirtualization/README.md  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._README.md  \n",
            "  inflating: NeuralWeightVirtualization/us8k_matching_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._us8k_matching_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/obs_accuracy_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._obs_accuracy_ten_nets.txt  \n",
            "   creating: NeuralWeightVirtualization/obs/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._obs  \n",
            "  inflating: NeuralWeightVirtualization/GSC_v2_data.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._GSC_v2_data.py  \n",
            "   creating: NeuralWeightVirtualization/fmnist/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._fmnist  \n",
            "  inflating: NeuralWeightVirtualization/esc10_accuracy_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._esc10_accuracy_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/hhar_accuracy_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._hhar_accuracy_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/L46_Project_Notebook.ipynb  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._L46_Project_Notebook.ipynb  \n",
            "  inflating: NeuralWeightVirtualization/download_dataset.sh  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._download_dataset.sh  \n",
            "  inflating: NeuralWeightVirtualization/fmnist_data.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._fmnist_data.py  \n",
            "  inflating: NeuralWeightVirtualization/hhar.vnn  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._hhar.vnn  \n",
            "  inflating: NeuralWeightVirtualization/.gitignore  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._.gitignore  \n",
            "  inflating: NeuralWeightVirtualization/cifar10_accuracy_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._cifar10_accuracy_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/gsc_accuracy_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._gsc_accuracy_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/obs.accuracy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._obs.accuracy  \n",
            "   creating: NeuralWeightVirtualization/gsc/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._gsc  \n",
            "  inflating: NeuralWeightVirtualization/weight_page_occupation.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._weight_page_occupation.npy  \n",
            "  inflating: NeuralWeightVirtualization/gsc.accuracy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._gsc.accuracy  \n",
            "  inflating: NeuralWeightVirtualization/baseline_execute.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._baseline_execute.py  \n",
            "   creating: NeuralWeightVirtualization/tf_operation/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._tf_operation  \n",
            "  inflating: NeuralWeightVirtualization/gtsrb.vnn  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._gtsrb.vnn  \n",
            "  inflating: NeuralWeightVirtualization/svhn_data.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._svhn_data.py  \n",
            "  inflating: NeuralWeightVirtualization/us8k_data.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._us8k_data.py  \n",
            "  inflating: NeuralWeightVirtualization/mnist.vnn  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._mnist.vnn  \n",
            "   creating: NeuralWeightVirtualization/.git/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._.git  \n",
            "   creating: NeuralWeightVirtualization/gtsrb/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._gtsrb  \n",
            "  inflating: NeuralWeightVirtualization/gtsrb_accuracy_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._gtsrb_accuracy_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/esc10.accuracy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._esc10.accuracy  \n",
            "  inflating: NeuralWeightVirtualization/fmnist_matching_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._fmnist_matching_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/svhn.accuracy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._svhn.accuracy  \n",
            "  inflating: NeuralWeightVirtualization/svhn_accuracy_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._svhn_accuracy_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/mnist_accuracy_ten_nets.txt  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._mnist_accuracy_ten_nets.txt  \n",
            "  inflating: NeuralWeightVirtualization/esc10_data.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._esc10_data.py  \n",
            "  inflating: NeuralWeightVirtualization/out_ten_nets.log  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._out_ten_nets.log  \n",
            "  inflating: NeuralWeightVirtualization/esc10.vnn  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._esc10.vnn  \n",
            "   creating: NeuralWeightVirtualization/cifar10/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/._cifar10  \n",
            "  inflating: NeuralWeightVirtualization/mnist/mnist_network_fisher.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/mnist/._mnist_network_fisher.npy  \n",
            "  inflating: NeuralWeightVirtualization/mnist/pintle.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/mnist/._pintle.py  \n",
            "  inflating: NeuralWeightVirtualization/mnist/__init__.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/mnist/.___init__.py  \n",
            "   creating: NeuralWeightVirtualization/mnist/__pycache__/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/mnist/.___pycache__  \n",
            "  inflating: NeuralWeightVirtualization/mnist/mnist.index  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/mnist/._mnist.index  \n",
            "  inflating: NeuralWeightVirtualization/mnist/mnist_network_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/mnist/._mnist_network_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/mnist/mnist_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/mnist/._mnist_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/mnist/mnist.data-00000-of-00001  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/mnist/._mnist.data-00000-of-00001  \n",
            "  inflating: NeuralWeightVirtualization/mnist/mnist.meta  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/mnist/._mnist.meta  \n",
            "  inflating: NeuralWeightVirtualization/MNIST_data/t10k-images-idx3-ubyte.gz  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/MNIST_data/._t10k-images-idx3-ubyte.gz  \n",
            "  inflating: NeuralWeightVirtualization/MNIST_data/train-images-idx3-ubyte.gz  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/MNIST_data/._train-images-idx3-ubyte.gz  \n",
            "  inflating: NeuralWeightVirtualization/MNIST_data/train-labels-idx1-ubyte.gz  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/MNIST_data/._train-labels-idx1-ubyte.gz  \n",
            "  inflating: NeuralWeightVirtualization/MNIST_data/t10k-labels-idx1-ubyte.gz  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/MNIST_data/._t10k-labels-idx1-ubyte.gz  \n",
            "  inflating: NeuralWeightVirtualization/esc10/esc10.data-00000-of-00001  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/esc10/._esc10.data-00000-of-00001  \n",
            "  inflating: NeuralWeightVirtualization/esc10/esc10_network_fisher.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/esc10/._esc10_network_fisher.npy  \n",
            "  inflating: NeuralWeightVirtualization/esc10/pintle.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/esc10/._pintle.py  \n",
            "  inflating: NeuralWeightVirtualization/esc10/__init__.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/esc10/.___init__.py  \n",
            "  inflating: NeuralWeightVirtualization/esc10/esc10.meta  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/esc10/._esc10.meta  \n",
            "   creating: NeuralWeightVirtualization/esc10/__pycache__/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/esc10/.___pycache__  \n",
            "  inflating: NeuralWeightVirtualization/esc10/esc10_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/esc10/._esc10_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/esc10/esc10_network_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/esc10/._esc10_network_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/esc10/esc10.index  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/esc10/._esc10.index  \n",
            "  inflating: NeuralWeightVirtualization/svhn/svhn.index  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/svhn/._svhn.index  \n",
            "  inflating: NeuralWeightVirtualization/svhn/svhn.meta  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/svhn/._svhn.meta  \n",
            "  inflating: NeuralWeightVirtualization/svhn/svhn_network_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/svhn/._svhn_network_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/svhn/svhn_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/svhn/._svhn_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/svhn/svhn.data-00000-of-00001  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/svhn/._svhn.data-00000-of-00001  \n",
            "  inflating: NeuralWeightVirtualization/svhn/pintle.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/svhn/._pintle.py  \n",
            "  inflating: NeuralWeightVirtualization/svhn/__init__.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/svhn/.___init__.py  \n",
            "   creating: NeuralWeightVirtualization/svhn/__pycache__/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/svhn/.___pycache__  \n",
            "  inflating: NeuralWeightVirtualization/svhn/svhn_network_fisher.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/svhn/._svhn_network_fisher.npy  \n",
            "  inflating: NeuralWeightVirtualization/us8k/us8k_network_fisher.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/us8k/._us8k_network_fisher.npy  \n",
            "  inflating: NeuralWeightVirtualization/us8k/pintle.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/us8k/._pintle.py  \n",
            "  inflating: NeuralWeightVirtualization/us8k/__init__.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/us8k/.___init__.py  \n",
            "  inflating: NeuralWeightVirtualization/us8k/us8k.meta  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/us8k/._us8k.meta  \n",
            "   creating: NeuralWeightVirtualization/us8k/__pycache__/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/us8k/.___pycache__  \n",
            "  inflating: NeuralWeightVirtualization/us8k/us8k_network_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/us8k/._us8k_network_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/us8k/us8k.data-00000-of-00001  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/us8k/._us8k.data-00000-of-00001  \n",
            "  inflating: NeuralWeightVirtualization/us8k/us8k.index  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/us8k/._us8k.index  \n",
            "  inflating: NeuralWeightVirtualization/us8k/us8k_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/us8k/._us8k_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/hhar/hhar.index  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/hhar/._hhar.index  \n",
            "  inflating: NeuralWeightVirtualization/hhar/hhar_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/hhar/._hhar_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/hhar/pintle.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/hhar/._pintle.py  \n",
            "  inflating: NeuralWeightVirtualization/hhar/__init__.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/hhar/.___init__.py  \n",
            "  inflating: NeuralWeightVirtualization/hhar/hhar_network_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/hhar/._hhar_network_weight.npy  \n",
            "   creating: NeuralWeightVirtualization/hhar/__pycache__/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/hhar/.___pycache__  \n",
            "  inflating: NeuralWeightVirtualization/hhar/hhar.meta  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/hhar/._hhar.meta  \n",
            "  inflating: NeuralWeightVirtualization/hhar/hhar.data-00000-of-00001  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/hhar/._hhar.data-00000-of-00001  \n",
            "  inflating: NeuralWeightVirtualization/hhar/hhar_network_fisher.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/hhar/._hhar_network_fisher.npy  \n",
            "  inflating: NeuralWeightVirtualization/__pycache__/cifar10_data.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/__pycache__/._cifar10_data.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/__pycache__/svhn_data.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/__pycache__/._svhn_data.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/__pycache__/GTSRB_data.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/__pycache__/._GTSRB_data.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/__pycache__/esc10_data.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/__pycache__/._esc10_data.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/__pycache__/obstacle_data.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/__pycache__/._obstacle_data.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/__pycache__/hhar_data.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/__pycache__/._hhar_data.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/__pycache__/mnist_data.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/__pycache__/._mnist_data.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/__pycache__/us8k_data.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/__pycache__/._us8k_data.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/__pycache__/fmnist_data.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/__pycache__/._fmnist_data.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/__pycache__/GSC_v2_data.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/__pycache__/._GSC_v2_data.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/obs/obs.data-00000-of-00001  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/obs/._obs.data-00000-of-00001  \n",
            "  inflating: NeuralWeightVirtualization/obs/obs.index  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/obs/._obs.index  \n",
            "  inflating: NeuralWeightVirtualization/obs/pintle.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/obs/._pintle.py  \n",
            "  inflating: NeuralWeightVirtualization/obs/__init__.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/obs/.___init__.py  \n",
            "   creating: NeuralWeightVirtualization/obs/__pycache__/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/obs/.___pycache__  \n",
            "  inflating: NeuralWeightVirtualization/obs/obs_network_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/obs/._obs_network_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/obs/obs.meta  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/obs/._obs.meta  \n",
            "  inflating: NeuralWeightVirtualization/obs/obs_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/obs/._obs_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/obs/obs_network_fisher.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/obs/._obs_network_fisher.npy  \n",
            "  inflating: NeuralWeightVirtualization/fmnist/fmnist_network_fisher.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/fmnist/._fmnist_network_fisher.npy  \n",
            "  inflating: NeuralWeightVirtualization/fmnist/fmnist.meta  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/fmnist/._fmnist.meta  \n",
            "  inflating: NeuralWeightVirtualization/fmnist/pintle.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/fmnist/._pintle.py  \n",
            "  inflating: NeuralWeightVirtualization/fmnist/__init__.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/fmnist/.___init__.py  \n",
            "   creating: NeuralWeightVirtualization/fmnist/__pycache__/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/fmnist/.___pycache__  \n",
            "  inflating: NeuralWeightVirtualization/fmnist/fmnist.index  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/fmnist/._fmnist.index  \n",
            "  inflating: NeuralWeightVirtualization/fmnist/fmnist_network_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/fmnist/._fmnist_network_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/fmnist/fmnist_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/fmnist/._fmnist_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/fmnist/fmnist.data-00000-of-00001  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/fmnist/._fmnist.data-00000-of-00001  \n",
            "  inflating: NeuralWeightVirtualization/gsc/gsc.index  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gsc/._gsc.index  \n",
            "  inflating: NeuralWeightVirtualization/gsc/gsc.data-00000-of-00001  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gsc/._gsc.data-00000-of-00001  \n",
            "  inflating: NeuralWeightVirtualization/gsc/gsc_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gsc/._gsc_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/gsc/gsc_network_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gsc/._gsc_network_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/gsc/pintle.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gsc/._pintle.py  \n",
            "  inflating: NeuralWeightVirtualization/gsc/__init__.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gsc/.___init__.py  \n",
            "   creating: NeuralWeightVirtualization/gsc/__pycache__/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gsc/.___pycache__  \n",
            "  inflating: NeuralWeightVirtualization/gsc/gsc.meta  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gsc/._gsc.meta  \n",
            "  inflating: NeuralWeightVirtualization/gsc/gsc_network_fisher.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gsc/._gsc_network_fisher.npy  \n",
            "  inflating: NeuralWeightVirtualization/tf_operation/tf_operation.cu.o  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/tf_operation/._tf_operation.cu.o  \n",
            "  inflating: NeuralWeightVirtualization/tf_operation/build_tf_operation_nano.sh  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/tf_operation/._build_tf_operation_nano.sh  \n",
            "  inflating: NeuralWeightVirtualization/tf_operation/weight_loader.so  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/tf_operation/._weight_loader.so  \n",
            "  inflating: NeuralWeightVirtualization/tf_operation/build_weight_loader.sh  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/tf_operation/._build_weight_loader.sh  \n",
            "  inflating: NeuralWeightVirtualization/tf_operation/tf_operation.so  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/tf_operation/._tf_operation.so  \n",
            "  inflating: NeuralWeightVirtualization/tf_operation/weight_loader.cu.o  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/tf_operation/._weight_loader.cu.o  \n",
            "  inflating: NeuralWeightVirtualization/tf_operation/build_weight_loader_nano.sh  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/tf_operation/._build_weight_loader_nano.sh  \n",
            "  inflating: NeuralWeightVirtualization/tf_operation/tf_operation.cc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/tf_operation/._tf_operation.cc  \n",
            "  inflating: NeuralWeightVirtualization/tf_operation/weight_loader.c  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/tf_operation/._weight_loader.c  \n",
            "  inflating: NeuralWeightVirtualization/tf_operation/build_tf_operation.sh  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/tf_operation/._build_tf_operation.sh  \n",
            "  inflating: NeuralWeightVirtualization/tf_operation/weight_loader.cu  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/tf_operation/._weight_loader.cu  \n",
            "  inflating: NeuralWeightVirtualization/tf_operation/tf_operation.cu  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/tf_operation/._tf_operation.cu  \n",
            "  inflating: NeuralWeightVirtualization/.git/config  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/._config  \n",
            "   creating: NeuralWeightVirtualization/.git/objects/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/._objects  \n",
            "  inflating: NeuralWeightVirtualization/.git/HEAD  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/._HEAD  \n",
            "   creating: NeuralWeightVirtualization/.git/info/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/._info  \n",
            "   creating: NeuralWeightVirtualization/.git/logs/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/._logs  \n",
            "  inflating: NeuralWeightVirtualization/.git/description  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/._description  \n",
            "   creating: NeuralWeightVirtualization/.git/hooks/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/._hooks  \n",
            "   creating: NeuralWeightVirtualization/.git/refs/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/._refs  \n",
            "  inflating: NeuralWeightVirtualization/.git/index  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/._index  \n",
            "   creating: NeuralWeightVirtualization/.git/branches/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/._branches  \n",
            "  inflating: NeuralWeightVirtualization/.git/packed-refs  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/._packed-refs  \n",
            "  inflating: NeuralWeightVirtualization/gtsrb/gtsrb_network_fisher.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gtsrb/._gtsrb_network_fisher.npy  \n",
            "  inflating: NeuralWeightVirtualization/gtsrb/gtsrb.data-00000-of-00001  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gtsrb/._gtsrb.data-00000-of-00001  \n",
            "  inflating: NeuralWeightVirtualization/gtsrb/gtsrb_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gtsrb/._gtsrb_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/gtsrb/pintle.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gtsrb/._pintle.py  \n",
            "  inflating: NeuralWeightVirtualization/gtsrb/__init__.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gtsrb/.___init__.py  \n",
            "   creating: NeuralWeightVirtualization/gtsrb/__pycache__/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gtsrb/.___pycache__  \n",
            "  inflating: NeuralWeightVirtualization/gtsrb/gtsrb_network_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gtsrb/._gtsrb_network_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/gtsrb/gtsrb.index  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gtsrb/._gtsrb.index  \n",
            "  inflating: NeuralWeightVirtualization/gtsrb/gtsrb.meta  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gtsrb/._gtsrb.meta  \n",
            "  inflating: NeuralWeightVirtualization/cifar10/cifar10_network_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/cifar10/._cifar10_network_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/cifar10/cifar10.index  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/cifar10/._cifar10.index  \n",
            "  inflating: NeuralWeightVirtualization/cifar10/pintle.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/cifar10/._pintle.py  \n",
            "  inflating: NeuralWeightVirtualization/cifar10/cifar10.data-00000-of-00001  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/cifar10/._cifar10.data-00000-of-00001  \n",
            "  inflating: NeuralWeightVirtualization/cifar10/__init__.py  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/cifar10/.___init__.py  \n",
            "   creating: NeuralWeightVirtualization/cifar10/__pycache__/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/cifar10/.___pycache__  \n",
            "  inflating: NeuralWeightVirtualization/cifar10/cifar10_weight.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/cifar10/._cifar10_weight.npy  \n",
            "  inflating: NeuralWeightVirtualization/cifar10/cifar10_network_fisher.npy  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/cifar10/._cifar10_network_fisher.npy  \n",
            "  inflating: NeuralWeightVirtualization/cifar10/cifar10.meta  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/cifar10/._cifar10.meta  \n",
            "  inflating: NeuralWeightVirtualization/mnist/__pycache__/pintle.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/mnist/__pycache__/._pintle.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/mnist/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/mnist/__pycache__/.___init__.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/esc10/__pycache__/pintle.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/esc10/__pycache__/._pintle.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/esc10/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/esc10/__pycache__/.___init__.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/svhn/__pycache__/pintle.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/svhn/__pycache__/._pintle.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/svhn/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/svhn/__pycache__/.___init__.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/us8k/__pycache__/pintle.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/us8k/__pycache__/._pintle.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/us8k/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/us8k/__pycache__/.___init__.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/hhar/__pycache__/pintle.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/hhar/__pycache__/._pintle.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/hhar/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/hhar/__pycache__/.___init__.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/obs/__pycache__/pintle.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/obs/__pycache__/._pintle.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/obs/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/obs/__pycache__/.___init__.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/fmnist/__pycache__/pintle.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/fmnist/__pycache__/._pintle.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/fmnist/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/fmnist/__pycache__/.___init__.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/gsc/__pycache__/pintle.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gsc/__pycache__/._pintle.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/gsc/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gsc/__pycache__/.___init__.cpython-36.pyc  \n",
            "   creating: NeuralWeightVirtualization/.git/objects/pack/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/objects/._pack  \n",
            "   creating: NeuralWeightVirtualization/.git/objects/info/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/objects/._info  \n",
            "  inflating: NeuralWeightVirtualization/.git/info/exclude  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/info/._exclude  \n",
            "  inflating: NeuralWeightVirtualization/.git/logs/HEAD  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/logs/._HEAD  \n",
            "   creating: NeuralWeightVirtualization/.git/logs/refs/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/logs/._refs  \n",
            "  inflating: NeuralWeightVirtualization/.git/hooks/commit-msg.sample  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/hooks/._commit-msg.sample  \n",
            "  inflating: NeuralWeightVirtualization/.git/hooks/pre-rebase.sample  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/hooks/._pre-rebase.sample  \n",
            "  inflating: NeuralWeightVirtualization/.git/hooks/pre-commit.sample  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/hooks/._pre-commit.sample  \n",
            "  inflating: NeuralWeightVirtualization/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/hooks/._applypatch-msg.sample  \n",
            "  inflating: NeuralWeightVirtualization/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/hooks/._fsmonitor-watchman.sample  \n",
            "  inflating: NeuralWeightVirtualization/.git/hooks/pre-receive.sample  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/hooks/._pre-receive.sample  \n",
            "  inflating: NeuralWeightVirtualization/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/hooks/._prepare-commit-msg.sample  \n",
            "  inflating: NeuralWeightVirtualization/.git/hooks/post-update.sample  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/hooks/._post-update.sample  \n",
            "  inflating: NeuralWeightVirtualization/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/hooks/._pre-applypatch.sample  \n",
            "  inflating: NeuralWeightVirtualization/.git/hooks/pre-push.sample  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/hooks/._pre-push.sample  \n",
            "  inflating: NeuralWeightVirtualization/.git/hooks/update.sample  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/hooks/._update.sample  \n",
            "   creating: NeuralWeightVirtualization/.git/refs/heads/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/refs/._heads  \n",
            "   creating: NeuralWeightVirtualization/.git/refs/tags/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/refs/._tags  \n",
            "   creating: NeuralWeightVirtualization/.git/refs/remotes/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/refs/._remotes  \n",
            "  inflating: NeuralWeightVirtualization/gtsrb/__pycache__/pintle.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gtsrb/__pycache__/._pintle.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/gtsrb/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/gtsrb/__pycache__/.___init__.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/cifar10/__pycache__/pintle.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/cifar10/__pycache__/._pintle.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/cifar10/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/cifar10/__pycache__/.___init__.cpython-36.pyc  \n",
            "  inflating: NeuralWeightVirtualization/.git/objects/pack/pack-db02d4b6051da394f5f82a8f658ce6ab7f24806b.pack  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/objects/pack/._pack-db02d4b6051da394f5f82a8f658ce6ab7f24806b.pack  \n",
            "  inflating: NeuralWeightVirtualization/.git/objects/pack/pack-db02d4b6051da394f5f82a8f658ce6ab7f24806b.idx  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/objects/pack/._pack-db02d4b6051da394f5f82a8f658ce6ab7f24806b.idx  \n",
            "   creating: NeuralWeightVirtualization/.git/logs/refs/heads/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/logs/refs/._heads  \n",
            "   creating: NeuralWeightVirtualization/.git/logs/refs/remotes/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/logs/refs/._remotes  \n",
            "  inflating: NeuralWeightVirtualization/.git/refs/heads/master  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/refs/heads/._master  \n",
            "   creating: NeuralWeightVirtualization/.git/refs/remotes/origin/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/refs/remotes/._origin  \n",
            "  inflating: NeuralWeightVirtualization/.git/logs/refs/heads/master  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/logs/refs/heads/._master  \n",
            "   creating: NeuralWeightVirtualization/.git/logs/refs/remotes/origin/\n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/logs/refs/remotes/._origin  \n",
            "  inflating: NeuralWeightVirtualization/.git/refs/remotes/origin/HEAD  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/refs/remotes/origin/._HEAD  \n",
            "  inflating: NeuralWeightVirtualization/.git/logs/refs/remotes/origin/HEAD  \n",
            "  inflating: __MACOSX/NeuralWeightVirtualization/.git/logs/refs/remotes/origin/._HEAD  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HwBWAkkqSxj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}